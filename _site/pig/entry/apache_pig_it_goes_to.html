<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Apache Pig: It goes to 0.11 | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Apache Pig: It goes to 0.11" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="After months of work, we are happy to announce the 0.11 release of Apache Pig. In this blog post, we highlight some of the major new features and performance improvements that were contributed to this release. A large chunk of the new features was created by Google Summer of Code (GSoC) students with supervision from the Apache Pig PMC, while the core Pig team focused on performance improvements, usability issues, and bug fixes. We encourage CS students to consider applying for GSOC in 2013 -- it&rsquo;s a great way to contribute to open source software. This blog post hits some of the highlights of the release. Pig users may also find a presentation by Daniel Dai, which includes code and output samples for the new operators, helpful. New Features DateTime Data Type The DateTime data type has been added to make it easier to work with timestamps. You can now do date and time arithmetic directly in a Pig script, use UDFs such as CurrentTime, AddDuration, WeeksBetween, etc. PigStorage expects timestamps to be represented in the ISO 8601 format. Much of this work was done by Zhijie Shen as part of his GSoC project. RANK Operator The new RANK operator allows one to assign an ordinal number to every tuple in a relation. A user can specify whether she wants exact rank (elements with the same sort value get the same rank) or &lsquo;DENSE&rsquo; rank (elements with the same sort value get consecutive rank values). One can also rank by a field value, in which case the relation is sorted by this field prior to ranks being assigned. Much of this work was done by Allan Avenda&ntilde;o as part of his GSoC project. A = load &#39;data&#39; AS (f1:chararray,f2:int,f3:chararray); DUMP A; (David,1,N) (Tete,2,N) (Ranjit,3,M) (Ranjit,3,P) (David,4,Q) (David,4,Q) (Jillian,8,Q) (JaePak,7,Q) (Michael,8,T) (Jillian,8,Q) (Jose,10,V) B = rank A; dump B; (1,David,1,N) (2,Tete,2,N) (3,Ranjit,3,M) (4,Ranjit,3,P) (5,David,4,Q) (6,David,4,Q) (7,Jillian,8,Q) (8,JaePak,7,Q) (9,Michael,8,T) (10,Jillian,8,Q) (11,Jose,10,V) C = rank A by f1 DESC, f2 ASC DENSE; dump C; (1,Tete,2,N) (2,Ranjit,3,M) (2,Ranjit,3,P) (3,Michael,8,T) (4,Jose,10,V) (5,Jillian,8,Q) (5,Jillian,8,Q) (6,JaePak,7,Q) (7,David,1,N) (8,David,4,Q) (8,David,4,Q) CUBE and ROLLUP Operators The new CUBE and ROLLUP operators of the equivalent SQL operators provide the ability to easily compute aggregates over multi-dimensional data. Here is an example: events = LOAD &#39;/logs/events&#39; USING EventLoader() AS (lang, country, app_id, event_id, total); eventcube = CUBE events BY CUBE(lang, country), ROLLUP(app_id, event_id); result = FOREACH eventcube GENERATE FLATTEN(group) as (lang, country, app_id, event_id), COUNT_STAR(cube), SUM(cube.total); STORE result INTO &#39;cuberesult&#39;; The CUBE operator produces all combinations of cubed dimensions. The ROLLUP operator produces all levels of a hierarchical group, meaning, ROLLUP(country, region, city) will produce aggregates by country, country and region, country, region, and city, but not country and city (without region). When used together as in the above example, the output groups will be the cross product of all groups generated by cube and rollup operation. That means that if there are m dimensions in cube operations and n dimensions in rollup operation then overall number of combinations will be (2^m) * (n+1). Detailed documentation can be seen in the CUBE Jira. This work was done by Prasanth Jayachandran as part of his GSoC project. He also did further work on optimizing the cubing computation to make it extremely scalable; this optimization will likely be added to the Pig 0.12 release. Groovy UDFs Pig has support for UDFs written in JRuby and Jython. In this release, support for UDFs in Groovy is added, providing an easy bridge for converting Groovy and Pig data types and specifying output schemas via annotations. This work was contributed by Mathias Herberts. Improvements Performance improvement of in-memory aggregation Pig 0.10 introduced in-memory aggregation for algebraic operators -- instead of relying on Hadoop combiners, which involve writing map outputs to disk and post-processing them to apply the combine function, Pig can optionally buffer up map outputs in memory and apply combiners without paying the IO cost of writing intermediate data out to platters. While the initial implementation significantly improved performance of a number of queries, we found some corner cases where it actually hurt performance; furthermore, reserving a large chunk of memory for aggregation buffers can have negative effects on memory-intensive tasks. In Pig 0.11, we completely rewrote the partial aggregation operator to be much more efficient, and integrated it with Pig&rsquo;s Spillable Memory Manager, so it no longer requires dedicated space on the task heap. This feature is still considered experimental and is off by default; you can turn it on by setting pig.exec.mapPartAgg to true. With these changes in place, Twitter was able to turn this option on by default for all Pig scripts they run on their clusters -- thousands of Map-Reduce jobs per day (they also dropped pig.exec.mapPartAgg.minReduction to 3, to be even more aggressive with this feature). Performance improvement related to Spillable management Speaking of the SpillableMemoryManager -- it also saw some significant improvements. The default collection data structure in Pig is a &ldquo;Bag&rdquo;. Bags are spillable, meaning that if there is not enough memory to hold all the tuples in a bag in RAM, Pig will spill part of the bag to disk. This allows a large job to make progress, albeit slowly, rather than crashing from &ldquo;out of memory&rdquo; errors. The way this worked before Pig 0.11 was as follows: Every time a Bag is created from the BagFactory, it is registered with the SpillableMemoryManager. The SpillableMemoryManager keeps a list of WeakReferences to Spillable objects Upon getting a notification that GC is about to happen, the SMM iterates through its list of WeakReferences, deleting ones that are no longer valid (pointing to null), and looking for the largest Spillable it can find. It then asks this Spillable to spill, and relies on the coming GC to free up spilled data. &lt;/ul&gt; Some users reported seeing large amounts of time taken up by traversing the WeakReference list kept by the SMM. A large WeakReference list affected both performance, since we had to iterate over large lists when GC was imminent, and memory, since each WeakReference adds 32 bytes of overhead on a 64-bit JVM. In Pig 0.11 we modified the Bag code so that instead of registering all bags in case they grow, we have Bags register themselves if their contents exceed 100KB, the logic being that a lot of bags will never reach this size, and would not be useful to spill anyway. This drastically reduced the amount of time and memory we spend on the SpillableMemoryManager. Improvements to AvroStorage and HBaseStorage HBase: Added the ability to set HBase scan maxTimestamp, minTimestamp and timestamp in HBaseStorage. Significant performance optimization for filters over many columns Compatibility with HBase 0.94 + secure cluster &lt;/ul&gt; Avro: AvroStorage can now optionally skip corrupt Avro files Added support for recursively defined records Added support for Avro 1.7.1 Better support for file globbing &lt;/ul&gt; Faster, leaner Schema Tuples Pig uses a generic Tuple container object to hold a &ldquo;row&rdquo; of data. Under the covers, it&rsquo;s simply a List&lt;/code&gt;, where the objects might be Strings, Longs, other Tuples, Bags, etc. Such generality comes with overhead. We found that we can achieve significant performance gains in both size and speed of Tuples if, when the schema of a tuple is known, custom classes are auto-generated on the fly for working with particular schemas. You can see the results of our benchmarks (&ldquo;Tuple&rdquo; is the default generic tuple implementation; &ldquo;Primitive&rdquo; is an earlier attempt at tuple optimization, which streamlined handling of schemas consisting only of longs, ints, etc -- this approach was abandoned after the codegen work was complete; &ldquo;Schema&rdquo; is the codegen work). To turn on schema tuples, set the pig.schematuple property to true. This feature is considered experimental and is off by default. &lt;/p&gt; New APIs for Algebraic, Accumulator UDFs, and Guava We added a number of helper classes to make creating new UDFs easier, and improved some of the APIs around UDFs. Pig Tuple object is now a Java Iterable, so you can easily loop over all of the fields if you like (&lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-2724&gt;PIG-2724 has details&lt;/a&gt;) Accumulators can now terminate early. This can be a big performance win for accumulators that can bail out upon reaching some success condition. One simply has to implement the TerminatingAccumulator interface, which has just one method: isFinished(). A new abstract class, IteratingAccumulatorEvalFunc, has been added to make it easier to write Accumulator functions. To write an Accumulator, one can simply extend this abstract class and implement a single method which takes an Iterator and returns the desired result. If you&rsquo;ve implemented Accumulators before, you&rsquo;ll see why the sample code in PIG-2651 is much cleaner and simpler to write. Instead of implementing a getOutputSchema function, UDF authors can tell Pig their output schema by annotating the UDF with an @OutputSchema annotation Before Pig 0.11 if you wanted to implement an Algebraic or Accumulative UDF, you still had to implement the regular exec() method, as well. We&rsquo;ve introduced a couple of new abstract classes, AlgebraicEvalFunc and AccumulatorEvalFunc, which give you the derivable implementations for free. So if you implement AlgebraicEvalFunc, you automatically get the regular and Accumulator implementations. Saves code, saves sanity! We&rsquo;ve found that many Pig users are interested in being able to share their UDF logic with non-Pig programs. FunctionWrapperEvalFunc allows one to easily wrap Guava functions which contain the core logic, and keep UDF-specific code minimal. We&rsquo;ve found that many UDFs work on just a single field (as opposed to a multi-field tuple), and return a single value. For those cases, extending PrimitiveEvalFunc&lt;IN, OUT&gt; allows the UDF author to skip all the tuple unwrapping business and simply implement public OUT exec(IN input), where IN and OUT are primitives. We find ourselves wrapping StoreFuncs often and have created StoreFuncWrapper and StoreFuncMetadataWrapper classes to make this easier. These classes allow one to subclass and decorate only select StoreFunc methods. mock.Storage, a helper StoreFunc to simplify writing JUnit tests for your pig scripts, was quietly added in 0.10.1 and got a couple of bug fixes in 0.11. See details in mock.Storage docs. &lt;/ul&gt; Other Changes A number of other changes were introduced -- optimizations, interface improvements, small features, etc. Here is a sampling: Penny, a debugging tool introduced as an experimental feature in Pig 0.9, has been removed due to lack of adoption and complexity of the codebase StoreFuncs can now implement a new method, cleanupOnSuccess, in addition to the previously existing cleanupOnFailure Pig Streaming can be passed values from the JobConf via environment variables. Rather than pass all the variables from the JobConf, which can cause Java&rsquo;s ProcessBuilder to croak for large enough JobConfs, Pig 11 allows the user to explicitly specify which properties should be passed in by setting the value of pig.streaming.environment property The logic used to estimate how many reducers should be used for a given Map-Reduce job is now pluggable, with the default implementation remaining as it was. Setting the pig.udf.profile property to true will turn on counters that approximately measure the number of invocations and milliseconds spent in all UDFs and Loaders. Use this with caution, as this feature can really bloat the number of counters your job uses! Useful for lightweight debugging of jobs. Local mode is now significantly faster Merge Join previously only worked immediately after loading. It is now allowed after an ORDER operator Grunt prints schemas in human-friendly JSON if you set pig.pretty.print.schema=true Better HCatalog integration Pluggable PigProgressNotificationListeners allow custom tool integration for monitoring Pig job progress -- for an example of what can be possible with this, check out Twitter Ambrose Extensive work went into making sure that Pig works with JDK 7 and Hadoop 2.0 &lt;/ul&gt; A lot of work went into this release, and we are grateful to all the contributors. We hope you like all the new stuff! Let us know what you think -- users@pig.apache.org. Dmitriy Ryaboy (@squarecog) on behalf of the Pig team." />
<meta property="og:description" content="After months of work, we are happy to announce the 0.11 release of Apache Pig. In this blog post, we highlight some of the major new features and performance improvements that were contributed to this release. A large chunk of the new features was created by Google Summer of Code (GSoC) students with supervision from the Apache Pig PMC, while the core Pig team focused on performance improvements, usability issues, and bug fixes. We encourage CS students to consider applying for GSOC in 2013 -- it&rsquo;s a great way to contribute to open source software. This blog post hits some of the highlights of the release. Pig users may also find a presentation by Daniel Dai, which includes code and output samples for the new operators, helpful. New Features DateTime Data Type The DateTime data type has been added to make it easier to work with timestamps. You can now do date and time arithmetic directly in a Pig script, use UDFs such as CurrentTime, AddDuration, WeeksBetween, etc. PigStorage expects timestamps to be represented in the ISO 8601 format. Much of this work was done by Zhijie Shen as part of his GSoC project. RANK Operator The new RANK operator allows one to assign an ordinal number to every tuple in a relation. A user can specify whether she wants exact rank (elements with the same sort value get the same rank) or &lsquo;DENSE&rsquo; rank (elements with the same sort value get consecutive rank values). One can also rank by a field value, in which case the relation is sorted by this field prior to ranks being assigned. Much of this work was done by Allan Avenda&ntilde;o as part of his GSoC project. A = load &#39;data&#39; AS (f1:chararray,f2:int,f3:chararray); DUMP A; (David,1,N) (Tete,2,N) (Ranjit,3,M) (Ranjit,3,P) (David,4,Q) (David,4,Q) (Jillian,8,Q) (JaePak,7,Q) (Michael,8,T) (Jillian,8,Q) (Jose,10,V) B = rank A; dump B; (1,David,1,N) (2,Tete,2,N) (3,Ranjit,3,M) (4,Ranjit,3,P) (5,David,4,Q) (6,David,4,Q) (7,Jillian,8,Q) (8,JaePak,7,Q) (9,Michael,8,T) (10,Jillian,8,Q) (11,Jose,10,V) C = rank A by f1 DESC, f2 ASC DENSE; dump C; (1,Tete,2,N) (2,Ranjit,3,M) (2,Ranjit,3,P) (3,Michael,8,T) (4,Jose,10,V) (5,Jillian,8,Q) (5,Jillian,8,Q) (6,JaePak,7,Q) (7,David,1,N) (8,David,4,Q) (8,David,4,Q) CUBE and ROLLUP Operators The new CUBE and ROLLUP operators of the equivalent SQL operators provide the ability to easily compute aggregates over multi-dimensional data. Here is an example: events = LOAD &#39;/logs/events&#39; USING EventLoader() AS (lang, country, app_id, event_id, total); eventcube = CUBE events BY CUBE(lang, country), ROLLUP(app_id, event_id); result = FOREACH eventcube GENERATE FLATTEN(group) as (lang, country, app_id, event_id), COUNT_STAR(cube), SUM(cube.total); STORE result INTO &#39;cuberesult&#39;; The CUBE operator produces all combinations of cubed dimensions. The ROLLUP operator produces all levels of a hierarchical group, meaning, ROLLUP(country, region, city) will produce aggregates by country, country and region, country, region, and city, but not country and city (without region). When used together as in the above example, the output groups will be the cross product of all groups generated by cube and rollup operation. That means that if there are m dimensions in cube operations and n dimensions in rollup operation then overall number of combinations will be (2^m) * (n+1). Detailed documentation can be seen in the CUBE Jira. This work was done by Prasanth Jayachandran as part of his GSoC project. He also did further work on optimizing the cubing computation to make it extremely scalable; this optimization will likely be added to the Pig 0.12 release. Groovy UDFs Pig has support for UDFs written in JRuby and Jython. In this release, support for UDFs in Groovy is added, providing an easy bridge for converting Groovy and Pig data types and specifying output schemas via annotations. This work was contributed by Mathias Herberts. Improvements Performance improvement of in-memory aggregation Pig 0.10 introduced in-memory aggregation for algebraic operators -- instead of relying on Hadoop combiners, which involve writing map outputs to disk and post-processing them to apply the combine function, Pig can optionally buffer up map outputs in memory and apply combiners without paying the IO cost of writing intermediate data out to platters. While the initial implementation significantly improved performance of a number of queries, we found some corner cases where it actually hurt performance; furthermore, reserving a large chunk of memory for aggregation buffers can have negative effects on memory-intensive tasks. In Pig 0.11, we completely rewrote the partial aggregation operator to be much more efficient, and integrated it with Pig&rsquo;s Spillable Memory Manager, so it no longer requires dedicated space on the task heap. This feature is still considered experimental and is off by default; you can turn it on by setting pig.exec.mapPartAgg to true. With these changes in place, Twitter was able to turn this option on by default for all Pig scripts they run on their clusters -- thousands of Map-Reduce jobs per day (they also dropped pig.exec.mapPartAgg.minReduction to 3, to be even more aggressive with this feature). Performance improvement related to Spillable management Speaking of the SpillableMemoryManager -- it also saw some significant improvements. The default collection data structure in Pig is a &ldquo;Bag&rdquo;. Bags are spillable, meaning that if there is not enough memory to hold all the tuples in a bag in RAM, Pig will spill part of the bag to disk. This allows a large job to make progress, albeit slowly, rather than crashing from &ldquo;out of memory&rdquo; errors. The way this worked before Pig 0.11 was as follows: Every time a Bag is created from the BagFactory, it is registered with the SpillableMemoryManager. The SpillableMemoryManager keeps a list of WeakReferences to Spillable objects Upon getting a notification that GC is about to happen, the SMM iterates through its list of WeakReferences, deleting ones that are no longer valid (pointing to null), and looking for the largest Spillable it can find. It then asks this Spillable to spill, and relies on the coming GC to free up spilled data. &lt;/ul&gt; Some users reported seeing large amounts of time taken up by traversing the WeakReference list kept by the SMM. A large WeakReference list affected both performance, since we had to iterate over large lists when GC was imminent, and memory, since each WeakReference adds 32 bytes of overhead on a 64-bit JVM. In Pig 0.11 we modified the Bag code so that instead of registering all bags in case they grow, we have Bags register themselves if their contents exceed 100KB, the logic being that a lot of bags will never reach this size, and would not be useful to spill anyway. This drastically reduced the amount of time and memory we spend on the SpillableMemoryManager. Improvements to AvroStorage and HBaseStorage HBase: Added the ability to set HBase scan maxTimestamp, minTimestamp and timestamp in HBaseStorage. Significant performance optimization for filters over many columns Compatibility with HBase 0.94 + secure cluster &lt;/ul&gt; Avro: AvroStorage can now optionally skip corrupt Avro files Added support for recursively defined records Added support for Avro 1.7.1 Better support for file globbing &lt;/ul&gt; Faster, leaner Schema Tuples Pig uses a generic Tuple container object to hold a &ldquo;row&rdquo; of data. Under the covers, it&rsquo;s simply a List&lt;/code&gt;, where the objects might be Strings, Longs, other Tuples, Bags, etc. Such generality comes with overhead. We found that we can achieve significant performance gains in both size and speed of Tuples if, when the schema of a tuple is known, custom classes are auto-generated on the fly for working with particular schemas. You can see the results of our benchmarks (&ldquo;Tuple&rdquo; is the default generic tuple implementation; &ldquo;Primitive&rdquo; is an earlier attempt at tuple optimization, which streamlined handling of schemas consisting only of longs, ints, etc -- this approach was abandoned after the codegen work was complete; &ldquo;Schema&rdquo; is the codegen work). To turn on schema tuples, set the pig.schematuple property to true. This feature is considered experimental and is off by default. &lt;/p&gt; New APIs for Algebraic, Accumulator UDFs, and Guava We added a number of helper classes to make creating new UDFs easier, and improved some of the APIs around UDFs. Pig Tuple object is now a Java Iterable, so you can easily loop over all of the fields if you like (&lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-2724&gt;PIG-2724 has details&lt;/a&gt;) Accumulators can now terminate early. This can be a big performance win for accumulators that can bail out upon reaching some success condition. One simply has to implement the TerminatingAccumulator interface, which has just one method: isFinished(). A new abstract class, IteratingAccumulatorEvalFunc, has been added to make it easier to write Accumulator functions. To write an Accumulator, one can simply extend this abstract class and implement a single method which takes an Iterator and returns the desired result. If you&rsquo;ve implemented Accumulators before, you&rsquo;ll see why the sample code in PIG-2651 is much cleaner and simpler to write. Instead of implementing a getOutputSchema function, UDF authors can tell Pig their output schema by annotating the UDF with an @OutputSchema annotation Before Pig 0.11 if you wanted to implement an Algebraic or Accumulative UDF, you still had to implement the regular exec() method, as well. We&rsquo;ve introduced a couple of new abstract classes, AlgebraicEvalFunc and AccumulatorEvalFunc, which give you the derivable implementations for free. So if you implement AlgebraicEvalFunc, you automatically get the regular and Accumulator implementations. Saves code, saves sanity! We&rsquo;ve found that many Pig users are interested in being able to share their UDF logic with non-Pig programs. FunctionWrapperEvalFunc allows one to easily wrap Guava functions which contain the core logic, and keep UDF-specific code minimal. We&rsquo;ve found that many UDFs work on just a single field (as opposed to a multi-field tuple), and return a single value. For those cases, extending PrimitiveEvalFunc&lt;IN, OUT&gt; allows the UDF author to skip all the tuple unwrapping business and simply implement public OUT exec(IN input), where IN and OUT are primitives. We find ourselves wrapping StoreFuncs often and have created StoreFuncWrapper and StoreFuncMetadataWrapper classes to make this easier. These classes allow one to subclass and decorate only select StoreFunc methods. mock.Storage, a helper StoreFunc to simplify writing JUnit tests for your pig scripts, was quietly added in 0.10.1 and got a couple of bug fixes in 0.11. See details in mock.Storage docs. &lt;/ul&gt; Other Changes A number of other changes were introduced -- optimizations, interface improvements, small features, etc. Here is a sampling: Penny, a debugging tool introduced as an experimental feature in Pig 0.9, has been removed due to lack of adoption and complexity of the codebase StoreFuncs can now implement a new method, cleanupOnSuccess, in addition to the previously existing cleanupOnFailure Pig Streaming can be passed values from the JobConf via environment variables. Rather than pass all the variables from the JobConf, which can cause Java&rsquo;s ProcessBuilder to croak for large enough JobConfs, Pig 11 allows the user to explicitly specify which properties should be passed in by setting the value of pig.streaming.environment property The logic used to estimate how many reducers should be used for a given Map-Reduce job is now pluggable, with the default implementation remaining as it was. Setting the pig.udf.profile property to true will turn on counters that approximately measure the number of invocations and milliseconds spent in all UDFs and Loaders. Use this with caution, as this feature can really bloat the number of counters your job uses! Useful for lightweight debugging of jobs. Local mode is now significantly faster Merge Join previously only worked immediately after loading. It is now allowed after an ORDER operator Grunt prints schemas in human-friendly JSON if you set pig.pretty.print.schema=true Better HCatalog integration Pluggable PigProgressNotificationListeners allow custom tool integration for monitoring Pig job progress -- for an example of what can be possible with this, check out Twitter Ambrose Extensive work went into making sure that Pig works with JDK 7 and Hadoop 2.0 &lt;/ul&gt; A lot of work went into this release, and we are grateful to all the contributors. We hope you like all the new stuff! Let us know what you think -- users@pig.apache.org. Dmitriy Ryaboy (@squarecog) on behalf of the Pig team." />
<link rel="canonical" href="http://localhost:4000/pig/entry/apache_pig_it_goes_to" />
<meta property="og:url" content="http://localhost:4000/pig/entry/apache_pig_it_goes_to" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2013-02-23T06:52:46-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Apache Pig: It goes to 0.11" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2013-02-23T06:52:46-05:00","datePublished":"2013-02-23T06:52:46-05:00","description":"After months of work, we are happy to announce the 0.11 release of Apache Pig. In this blog post, we highlight some of the major new features and performance improvements that were contributed to this release. A large chunk of the new features was created by Google Summer of Code (GSoC) students with supervision from the Apache Pig PMC, while the core Pig team focused on performance improvements, usability issues, and bug fixes. We encourage CS students to consider applying for GSOC in 2013 -- it&rsquo;s a great way to contribute to open source software. This blog post hits some of the highlights of the release. Pig users may also find a presentation by Daniel Dai, which includes code and output samples for the new operators, helpful. New Features DateTime Data Type The DateTime data type has been added to make it easier to work with timestamps. You can now do date and time arithmetic directly in a Pig script, use UDFs such as CurrentTime, AddDuration, WeeksBetween, etc. PigStorage expects timestamps to be represented in the ISO 8601 format. Much of this work was done by Zhijie Shen as part of his GSoC project. RANK Operator The new RANK operator allows one to assign an ordinal number to every tuple in a relation. A user can specify whether she wants exact rank (elements with the same sort value get the same rank) or &lsquo;DENSE&rsquo; rank (elements with the same sort value get consecutive rank values). One can also rank by a field value, in which case the relation is sorted by this field prior to ranks being assigned. Much of this work was done by Allan Avenda&ntilde;o as part of his GSoC project. A = load &#39;data&#39; AS (f1:chararray,f2:int,f3:chararray); DUMP A; (David,1,N) (Tete,2,N) (Ranjit,3,M) (Ranjit,3,P) (David,4,Q) (David,4,Q) (Jillian,8,Q) (JaePak,7,Q) (Michael,8,T) (Jillian,8,Q) (Jose,10,V) B = rank A; dump B; (1,David,1,N) (2,Tete,2,N) (3,Ranjit,3,M) (4,Ranjit,3,P) (5,David,4,Q) (6,David,4,Q) (7,Jillian,8,Q) (8,JaePak,7,Q) (9,Michael,8,T) (10,Jillian,8,Q) (11,Jose,10,V) C = rank A by f1 DESC, f2 ASC DENSE; dump C; (1,Tete,2,N) (2,Ranjit,3,M) (2,Ranjit,3,P) (3,Michael,8,T) (4,Jose,10,V) (5,Jillian,8,Q) (5,Jillian,8,Q) (6,JaePak,7,Q) (7,David,1,N) (8,David,4,Q) (8,David,4,Q) CUBE and ROLLUP Operators The new CUBE and ROLLUP operators of the equivalent SQL operators provide the ability to easily compute aggregates over multi-dimensional data. Here is an example: events = LOAD &#39;/logs/events&#39; USING EventLoader() AS (lang, country, app_id, event_id, total); eventcube = CUBE events BY CUBE(lang, country), ROLLUP(app_id, event_id); result = FOREACH eventcube GENERATE FLATTEN(group) as (lang, country, app_id, event_id), COUNT_STAR(cube), SUM(cube.total); STORE result INTO &#39;cuberesult&#39;; The CUBE operator produces all combinations of cubed dimensions. The ROLLUP operator produces all levels of a hierarchical group, meaning, ROLLUP(country, region, city) will produce aggregates by country, country and region, country, region, and city, but not country and city (without region). When used together as in the above example, the output groups will be the cross product of all groups generated by cube and rollup operation. That means that if there are m dimensions in cube operations and n dimensions in rollup operation then overall number of combinations will be (2^m) * (n+1). Detailed documentation can be seen in the CUBE Jira. This work was done by Prasanth Jayachandran as part of his GSoC project. He also did further work on optimizing the cubing computation to make it extremely scalable; this optimization will likely be added to the Pig 0.12 release. Groovy UDFs Pig has support for UDFs written in JRuby and Jython. In this release, support for UDFs in Groovy is added, providing an easy bridge for converting Groovy and Pig data types and specifying output schemas via annotations. This work was contributed by Mathias Herberts. Improvements Performance improvement of in-memory aggregation Pig 0.10 introduced in-memory aggregation for algebraic operators -- instead of relying on Hadoop combiners, which involve writing map outputs to disk and post-processing them to apply the combine function, Pig can optionally buffer up map outputs in memory and apply combiners without paying the IO cost of writing intermediate data out to platters. While the initial implementation significantly improved performance of a number of queries, we found some corner cases where it actually hurt performance; furthermore, reserving a large chunk of memory for aggregation buffers can have negative effects on memory-intensive tasks. In Pig 0.11, we completely rewrote the partial aggregation operator to be much more efficient, and integrated it with Pig&rsquo;s Spillable Memory Manager, so it no longer requires dedicated space on the task heap. This feature is still considered experimental and is off by default; you can turn it on by setting pig.exec.mapPartAgg to true. With these changes in place, Twitter was able to turn this option on by default for all Pig scripts they run on their clusters -- thousands of Map-Reduce jobs per day (they also dropped pig.exec.mapPartAgg.minReduction to 3, to be even more aggressive with this feature). Performance improvement related to Spillable management Speaking of the SpillableMemoryManager -- it also saw some significant improvements. The default collection data structure in Pig is a &ldquo;Bag&rdquo;. Bags are spillable, meaning that if there is not enough memory to hold all the tuples in a bag in RAM, Pig will spill part of the bag to disk. This allows a large job to make progress, albeit slowly, rather than crashing from &ldquo;out of memory&rdquo; errors. The way this worked before Pig 0.11 was as follows: Every time a Bag is created from the BagFactory, it is registered with the SpillableMemoryManager. The SpillableMemoryManager keeps a list of WeakReferences to Spillable objects Upon getting a notification that GC is about to happen, the SMM iterates through its list of WeakReferences, deleting ones that are no longer valid (pointing to null), and looking for the largest Spillable it can find. It then asks this Spillable to spill, and relies on the coming GC to free up spilled data. &lt;/ul&gt; Some users reported seeing large amounts of time taken up by traversing the WeakReference list kept by the SMM. A large WeakReference list affected both performance, since we had to iterate over large lists when GC was imminent, and memory, since each WeakReference adds 32 bytes of overhead on a 64-bit JVM. In Pig 0.11 we modified the Bag code so that instead of registering all bags in case they grow, we have Bags register themselves if their contents exceed 100KB, the logic being that a lot of bags will never reach this size, and would not be useful to spill anyway. This drastically reduced the amount of time and memory we spend on the SpillableMemoryManager. Improvements to AvroStorage and HBaseStorage HBase: Added the ability to set HBase scan maxTimestamp, minTimestamp and timestamp in HBaseStorage. Significant performance optimization for filters over many columns Compatibility with HBase 0.94 + secure cluster &lt;/ul&gt; Avro: AvroStorage can now optionally skip corrupt Avro files Added support for recursively defined records Added support for Avro 1.7.1 Better support for file globbing &lt;/ul&gt; Faster, leaner Schema Tuples Pig uses a generic Tuple container object to hold a &ldquo;row&rdquo; of data. Under the covers, it&rsquo;s simply a List&lt;/code&gt;, where the objects might be Strings, Longs, other Tuples, Bags, etc. Such generality comes with overhead. We found that we can achieve significant performance gains in both size and speed of Tuples if, when the schema of a tuple is known, custom classes are auto-generated on the fly for working with particular schemas. You can see the results of our benchmarks (&ldquo;Tuple&rdquo; is the default generic tuple implementation; &ldquo;Primitive&rdquo; is an earlier attempt at tuple optimization, which streamlined handling of schemas consisting only of longs, ints, etc -- this approach was abandoned after the codegen work was complete; &ldquo;Schema&rdquo; is the codegen work). To turn on schema tuples, set the pig.schematuple property to true. This feature is considered experimental and is off by default. &lt;/p&gt; New APIs for Algebraic, Accumulator UDFs, and Guava We added a number of helper classes to make creating new UDFs easier, and improved some of the APIs around UDFs. Pig Tuple object is now a Java Iterable, so you can easily loop over all of the fields if you like (&lt;a href=&quot;https://issues.apache.org/jira/browse/PIG-2724&gt;PIG-2724 has details&lt;/a&gt;) Accumulators can now terminate early. This can be a big performance win for accumulators that can bail out upon reaching some success condition. One simply has to implement the TerminatingAccumulator interface, which has just one method: isFinished(). A new abstract class, IteratingAccumulatorEvalFunc, has been added to make it easier to write Accumulator functions. To write an Accumulator, one can simply extend this abstract class and implement a single method which takes an Iterator and returns the desired result. If you&rsquo;ve implemented Accumulators before, you&rsquo;ll see why the sample code in PIG-2651 is much cleaner and simpler to write. Instead of implementing a getOutputSchema function, UDF authors can tell Pig their output schema by annotating the UDF with an @OutputSchema annotation Before Pig 0.11 if you wanted to implement an Algebraic or Accumulative UDF, you still had to implement the regular exec() method, as well. We&rsquo;ve introduced a couple of new abstract classes, AlgebraicEvalFunc and AccumulatorEvalFunc, which give you the derivable implementations for free. So if you implement AlgebraicEvalFunc, you automatically get the regular and Accumulator implementations. Saves code, saves sanity! We&rsquo;ve found that many Pig users are interested in being able to share their UDF logic with non-Pig programs. FunctionWrapperEvalFunc allows one to easily wrap Guava functions which contain the core logic, and keep UDF-specific code minimal. We&rsquo;ve found that many UDFs work on just a single field (as opposed to a multi-field tuple), and return a single value. For those cases, extending PrimitiveEvalFunc&lt;IN, OUT&gt; allows the UDF author to skip all the tuple unwrapping business and simply implement public OUT exec(IN input), where IN and OUT are primitives. We find ourselves wrapping StoreFuncs often and have created StoreFuncWrapper and StoreFuncMetadataWrapper classes to make this easier. These classes allow one to subclass and decorate only select StoreFunc methods. mock.Storage, a helper StoreFunc to simplify writing JUnit tests for your pig scripts, was quietly added in 0.10.1 and got a couple of bug fixes in 0.11. See details in mock.Storage docs. &lt;/ul&gt; Other Changes A number of other changes were introduced -- optimizations, interface improvements, small features, etc. Here is a sampling: Penny, a debugging tool introduced as an experimental feature in Pig 0.9, has been removed due to lack of adoption and complexity of the codebase StoreFuncs can now implement a new method, cleanupOnSuccess, in addition to the previously existing cleanupOnFailure Pig Streaming can be passed values from the JobConf via environment variables. Rather than pass all the variables from the JobConf, which can cause Java&rsquo;s ProcessBuilder to croak for large enough JobConfs, Pig 11 allows the user to explicitly specify which properties should be passed in by setting the value of pig.streaming.environment property The logic used to estimate how many reducers should be used for a given Map-Reduce job is now pluggable, with the default implementation remaining as it was. Setting the pig.udf.profile property to true will turn on counters that approximately measure the number of invocations and milliseconds spent in all UDFs and Loaders. Use this with caution, as this feature can really bloat the number of counters your job uses! Useful for lightweight debugging of jobs. Local mode is now significantly faster Merge Join previously only worked immediately after loading. It is now allowed after an ORDER operator Grunt prints schemas in human-friendly JSON if you set pig.pretty.print.schema=true Better HCatalog integration Pluggable PigProgressNotificationListeners allow custom tool integration for monitoring Pig job progress -- for an example of what can be possible with this, check out Twitter Ambrose Extensive work went into making sure that Pig works with JDK 7 and Hadoop 2.0 &lt;/ul&gt; A lot of work went into this release, and we are grateful to all the contributors. We hope you like all the new stuff! Let us know what you think -- users@pig.apache.org. Dmitriy Ryaboy (@squarecog) on behalf of the Pig team.","headline":"Apache Pig: It goes to 0.11","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/pig/entry/apache_pig_it_goes_to"},"url":"http://localhost:4000/pig/entry/apache_pig_it_goes_to"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Apache Pig: It goes to 0.11</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2013-02-23T06:52:46-05:00" itemprop="datePublished">Feb 23, 2013
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Dmitriy V Ryaboy", "login"=>"dvryaboy", "email"=>"dvryaboy@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>After months of work, we are happy to announce the 0.11 release of Apache Pig. In this blog post, we highlight some of the major new features and performance improvements that were contributed to this release. A large chunk of the new features was created by Google Summer of Code (GSoC) students with supervision from the Apache Pig PMC, while the core Pig team focused on performance improvements, usability issues, and bug fixes. We encourage CS students to consider <a href="http://www.google-melange.com/gsoc/homepage/google/gsoc2013">applying for GSOC in 2013</a> -- it&rsquo;s a great way to contribute to open source software.</p>
<p>
This blog post hits some of the highlights of the release. Pig users may also find a <a href="http://www.slideshare.net/hortonworks/new-features-in-pig-011">presentation by Daniel Dai</a>, which includes code and output samples for the new operators, helpful.</p>
<h2>New Features</h2>
<h3>DateTime Data Type</h3>
<p>The <code>DateTime</code> data type has been added to make it easier to work with timestamps. You can now do date and time arithmetic directly in a Pig script, use UDFs such as <code>CurrentTime</code>, <code>AddDuration</code>, <code>WeeksBetween</code>, etc. PigStorage expects timestamps to be represented in the <a href="http://www.w3.org/TR/NOTE-datetime">ISO 8601 format</a>. Much of this work was done by Zhijie Shen as part of his GSoC project.</p>
<h3>RANK Operator</h3>
<p>The new <a href="http://pig.apache.org/docs/r0.11.0/basic.html#rank">RANK operator</a> allows one to assign an ordinal number to every tuple in a relation. A user can specify whether she wants exact rank (elements with the same sort value get the same rank) or &lsquo;<code>DENSE</code>&rsquo; rank (elements with the same sort value get consecutive rank values). One can also rank by a field value, in which case the relation is sorted by this field prior to ranks being assigned. Much of this work was done by Allan Avenda&ntilde;o as part of his GSoC project.</p>
<pre>
A = load 'data' AS (f1:chararray,f2:int,f3:chararray);
   
DUMP A;
(David,1,N)
(Tete,2,N)
(Ranjit,3,M)
(Ranjit,3,P)
(David,4,Q)
(David,4,Q)
(Jillian,8,Q)
(JaePak,7,Q)
(Michael,8,T)
(Jillian,8,Q)
(Jose,10,V)

B = rank A;

dump B;
(1,David,1,N)
(2,Tete,2,N)
(3,Ranjit,3,M)
(4,Ranjit,3,P)
(5,David,4,Q)
(6,David,4,Q)
(7,Jillian,8,Q)
(8,JaePak,7,Q)
(9,Michael,8,T)
(10,Jillian,8,Q)
(11,Jose,10,V)

C = rank A by f1 DESC, f2 ASC DENSE;

dump C;
(1,Tete,2,N)
(2,Ranjit,3,M)
(2,Ranjit,3,P)
(3,Michael,8,T)
(4,Jose,10,V)
(5,Jillian,8,Q)
(5,Jillian,8,Q)
(6,JaePak,7,Q)
(7,David,1,N)
(8,David,4,Q)
(8,David,4,Q)
</pre>
<h3>CUBE and ROLLUP Operators</h3>
<p>The new <code>CUBE</code> and <code>ROLLUP</code> operators of the equivalent SQL operators provide the ability to easily compute aggregates over multi-dimensional data. Here is an example:</p>
<pre>
events = LOAD '/logs/events' USING EventLoader() AS (lang, country, app_id, event_id, total);
eventcube = CUBE events BY
 CUBE(lang, country), ROLLUP(app_id, event_id);
result = FOREACH eventcube GENERATE
  FLATTEN(group) as (lang, country, app_id, event_id),
  COUNT_STAR(cube), SUM(cube.total);
 STORE result INTO 'cuberesult';
</pre>
<p>The <code>CUBE</code> operator produces all combinations of cubed dimensions. The <code>ROLLUP</code> operator produces all levels of a hierarchical group, meaning, <code>ROLLUP(country, region, city)</code> will produce aggregates by country, country and region, country, region, and city, but not country and city (without region). When used together as in the above example, the output groups will be the cross product of all groups generated by cube and rollup operation. That means that if there are <code>m</code> dimensions in cube operations and <code>n</code> dimensions in rollup operation then overall number of combinations will be <code>(2^m) * (n+1)</code>. Detailed documentation can be seen in <a href="https://issues.apache.org/jira/browse/PIG-2765?focusedCommentId=13427021&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13427021">the CUBE Jira</a>. This work was done by Prasanth Jayachandran as part of his GSoC project. He also did further work on optimizing the cubing computation to make it extremely scalable; this optimization will likely be added to the Pig 0.12 release.</p>
<h3>Groovy UDFs</h3>
<p>Pig has support for UDFs written in JRuby and Jython. In this release, support for <a href="http://pig.apache.org/docs/r0.11.0/udf.html#groovy-udfs">UDFs in Groovy</a> is added, providing an easy bridge for converting Groovy and Pig data types and specifying output schemas via annotations. This work was contributed by Mathias Herberts.</p>
<h2>Improvements</h2>
<h3>Performance improvement of in-memory aggregation</h3>
<p>Pig 0.10 introduced in-memory aggregation for algebraic operators -- instead of relying on Hadoop combiners, which involve writing map outputs to disk and post-processing them to apply the combine function, Pig can optionally buffer up map outputs in memory and apply combiners without paying the IO cost of writing intermediate data out to platters.</p>
<p>While the initial implementation significantly improved performance of a number of queries, we found some corner cases where it actually hurt performance; furthermore, reserving a large chunk of memory for aggregation buffers can have negative effects on memory-intensive tasks. In Pig 0.11, we completely <a href="https://issues.apache.org/jira/browse/PIG-2888">rewrote the partial aggregation operator</a> to be much more efficient, and <a href="https://issues.apache.org/jira/browse/PIG-3044">integrated it with Pig&rsquo;s Spillable Memory Manager</a>, so it no longer requires dedicated space on the task heap. This feature is still considered experimental and is off by default; you can turn it on by setting <code>pig.exec.mapPartAgg</code> to <code>true</code>. With these changes in place, Twitter was able to turn this option on by default for all Pig scripts they run on their clusters -- thousands of Map-Reduce jobs per day (they also dropped <code>pig.exec.mapPartAgg.minReduction</code> to 3, to be even more aggressive with this feature). </p>
<h3>Performance improvement related to Spillable management</h3>
<p>Speaking of the <code>SpillableMemoryManager</code> -- it also saw some significant improvements. The default collection data structure in Pig is a &ldquo;Bag&rdquo;. Bags are spillable, meaning that if there is not enough memory to hold all the tuples in a bag in RAM, Pig will spill part of the bag to disk. This allows a large job to make progress, albeit slowly, rather than crashing from &ldquo;out of memory&rdquo; errors. The way this worked before Pig 0.11 was as follows:</p>
<ul>
<li>Every time a Bag is created from the <code>BagFactory</code>, it is registered with the <code>SpillableMemoryManager</code>.
<li>The <code>SpillableMemoryManager</code> keeps a list of <code>WeakReferences</code> to <code>Spillable</code> objects
<li>Upon getting a notification that GC is about to happen, the <code>SMM</code> iterates through its list of <code>WeakReferences</code>, deleting ones that are no longer valid (pointing to null), and looking for the largest <code>Spillable</code> it can find. It then asks this <code>Spillable</code> to spill, and relies on the coming GC to free up spilled data.
</ul>
<p>Some users reported seeing large amounts of time taken up by traversing the <code>WeakReference</code> list kept by the <code>SMM</code>. A large <code>WeakReference</code> list affected both performance, since we had to iterate over large lists when GC was imminent, and memory, since each <code>WeakReference</code> adds 32 bytes of overhead on a 64-bit JVM. In Pig 0.11 we modified the Bag code so that instead of registering all bags in case they grow, we have Bags register themselves if their contents exceed 100KB, the logic being that a lot of bags will never reach this size, and would not be useful to spill anyway. This drastically reduced the amount of time and memory we spend on the <code>SpillableMemoryManager</code>.</p>
<h3>Improvements to AvroStorage and HBaseStorage</h3>
<p><b>HBase:</b></p>
<ul>
<li>Added the ability to set HBase scan maxTimestamp, minTimestamp and timestamp in HBaseStorage.
<li>Significant performance optimization for filters over many columns
<li>Compatibility with HBase 0.94 + secure cluster
</ul>
<p><b>Avro:</b></p>
<ul>
<li>AvroStorage can now optionally skip corrupt Avro files
<li>Added support for recursively defined records
<li>Added support for Avro 1.7.1
<li>Better support for file globbing
</ul>
<h3>Faster, leaner Schema Tuples</h3>
<p>Pig uses a generic <code>Tuple</code> container object to hold a &ldquo;row&rdquo; of data. Under the covers, it&rsquo;s simply a <code>List<Object></code>, where the objects might be <code>String</code>s, <code>Long</code>s, other <code>Tuple</code>s, <code>Bag</code>s, etc. Such generality comes with overhead. We found that we can achieve significant performance gains in both size and speed of Tuples if, when the schema of a tuple is known, custom classes are auto-generated on the fly for working with particular schemas. You can see the <a href="https://issues.apache.org/jira/secure/attachment/12532601/schematuple%20benchmarking.pdf">results of our benchmarks</a><br />
  (&ldquo;Tuple&rdquo; is the default generic tuple implementation; &ldquo;Primitive&rdquo; is an earlier attempt at tuple optimization, which streamlined handling of schemas consisting only of longs, ints, etc -- this approach was abandoned after the codegen work was complete; &ldquo;Schema&rdquo; is the codegen work). To turn on schema tuples, set the <code>pig.schematuple</code> property to <code>true</code>. This feature is considered experimental and is off by default. </p>
<h3>New APIs for Algebraic, Accumulator UDFs, and Guava</h3>
<p>We added a number of helper classes to make creating new UDFs easier, and improved some of the APIs around UDFs.</p>
<ul>
<li>Pig Tuple object is now a Java Iterable, so you can easily loop over all of the fields if you like (<a href="https://issues.apache.org/jira/browse/PIG-2724>PIG-2724 has details</a>)
<li>Accumulators can now terminate early. This can be a big performance win for accumulators that can bail out upon reaching some success condition. One simply has to implement the <a href="http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/TerminatingAccumulator.html">TerminatingAccumulator interface</a>, which has just one method: <code>isFinished()</code>.
<li>A new abstract class, <a href="http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/IteratingAccumulatorEvalFunc.html">IteratingAccumulatorEvalFunc</a>, has been added to make it easier to write Accumulator functions. To write an Accumulator, one can simply extend this abstract class and implement a single method which takes an Iterator and returns the desired result. If you&rsquo;ve implemented Accumulators before, you&rsquo;ll see why the sample code in <a href="https://issues.apache.org/jira/browse/PIG-2651">PIG-2651</a> is much cleaner and simpler to write.
<li>Instead of implementing a getOutputSchema function, UDF authors can tell Pig their output schema by annotating the UDF with an <a href="http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/builtin/OutputSchema.html">@OutputSchema</a> annotation
<li>Before Pig 0.11 if you wanted to implement an Algebraic or Accumulative UDF, you still had to implement the regular exec() method, as well. We&rsquo;ve introduced a couple of new abstract classes, <a href="http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/AlgebraicEvalFunc.html">AlgebraicEvalFunc</a> and <a href="http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/AccumulatorEvalFunc.html">AccumulatorEvalFunc</a>, which give you the derivable implementations for free. So if you implement <code>AlgebraicEvalFunc</code>, you automatically get the regular and <code>Accumulator</code> implementations. Saves code, saves sanity!
<li>We&rsquo;ve found that many Pig users are interested in being able to share their UDF logic with non-Pig programs. <a href="http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/builtin/FunctionWrapperEvalFunc.html">FunctionWrapperEvalFunc</a> allows one to easily wrap Guava functions which contain the core logic, and keep UDF-specific code minimal.
<li>We&rsquo;ve found that many UDFs work on just a single field (as opposed to a multi-field tuple), and return a single value. For those cases, extending <a href="http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/PrimitiveEvalFunc.html">PrimitiveEvalFunc<IN, OUT></a> allows the UDF author to skip all the tuple unwrapping business and simply implement <code>public OUT exec(IN input)</code>, where IN and OUT are primitives.
<li>We find ourselves wrapping <code>StoreFuncs</code> often and have created <a href="http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/StoreFuncWrapper.html">StoreFuncWrapper</a> and <a href="http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/StoreFuncMetadataWrapper.html">StoreFuncMetadataWrapper</a> classes to make this easier. These classes allow one to subclass and decorate only select <code>StoreFunc</code> methods.
<li><code>mock.Storage</code>, a helper <code>StoreFunc</code> to simplify writing JUnit tests for your pig scripts, was quietly added in 0.10.1 and got a couple of bug fixes in 0.11. See details in <a href="http://pig.apache.org/docs/r0.11.0/api/org/apache/pig/builtin/mock/Storage.html">mock.Storage docs</a>.
</ul>
<h2>Other Changes</h2>
<p>A number of other changes were introduced -- optimizations, interface improvements, small features, etc. Here is a sampling:</p>
<ul>
<li>Penny, a debugging tool introduced as an experimental feature in Pig 0.9, has been removed due to lack of adoption and complexity of the codebase
<li>StoreFuncs can now implement a new method, <code>cleanupOnSuccess</code>, in addition to the previously existing <code>cleanupOnFailure</code>
<li>Pig Streaming can be passed values from the JobConf via environment variables. Rather than pass all the variables from the JobConf, which can cause Java&rsquo;s <code>ProcessBuilder</code> to croak for large enough JobConfs, Pig 11 allows the user to explicitly specify which properties should be passed in by setting the value of <code>pig.streaming.environment</code> property
<li>The logic used to estimate how many reducers should be used for a given Map-Reduce job is now pluggable, with the default implementation remaining as it was.
<li>Setting the <code>pig.udf.profile</code> property to true will turn on counters that approximately measure the number of invocations and milliseconds spent in all UDFs and Loaders. Use this with caution, as this feature can really bloat the number of counters your job uses! Useful for lightweight debugging of jobs.
<li>Local mode is now significantly faster
<li>Merge Join previously only worked immediately after loading. It is now allowed after an <code>ORDER</code> operator
<li>Grunt prints schemas in human-friendly JSON if you set <code>pig.pretty.print.schema=true</code>
<li>Better HCatalog integration
<li>Pluggable <code>PigProgressNotificationListeners</code> allow custom tool integration for monitoring Pig job progress -- for an example of what can be possible with this, check out <a href="https://github.com/twitter/ambrose">Twitter Ambrose</a>
<li>Extensive work went into making sure that Pig works with JDK 7 and Hadoop 2.0
</ul>
<p>
A lot of work went into this release, and we are grateful to all the contributors.</p>
<p>
We hope you like all the new stuff! Let us know what you think -- <a href="mailto:users@pig.apache.org">users@pig.apache.org</a>.</p>
<p>Dmitriy Ryaboy (<a href="http://twitter.com/squarecog">@squarecog</a>) on behalf of the Pig team.</p>

  </div><a class="u-url" href="/pig/entry/apache_pig_it_goes_to" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
