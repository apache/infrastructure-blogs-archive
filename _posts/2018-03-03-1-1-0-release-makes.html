---
layout: post
status: PUBLISHED
published: true
title: 1.1.0 Release Makes Apache MXNet Faster and More Scalable
id: abbdccaf-fcc2-48c5-ad80-04f37e65b2b1
date: '2018-03-03 05:34:40 -0500'
categories: mxnet
tags: []
permalink: mxnet/entry/1-1-0-release-makes
---
<p>We are excited about the availability of the 1.1.0 release of Apache MXNet. Deep learning is a technique used to understand patterns in large datasets using algorithms inspired by biological neurons, and it has driven recent advances in artificial intelligence. MXNet is a fast and scalable deep learning framework for training and prediction with easy-to-use, concise APIs across multiple programming languages, including Python, R, Scala, and C++. Developers can use the Python or R APIs to develop and train neural network models to make accurate predictions. When it is time to integrate a trained model into an application, they can use any of the MXNet APIs to load the model and make predictions. This includes the high-level Scala inference API released today, which maintains Scala idiomatic conventions and supports multi-threaded architectures. In addition, it supports all MXNet operators and has comprehensive documentation and examples. </p>
<p>With 1.1.0 release, MXNet makes it easier for developers to build vocabulary and load pre-trained word embeddings by adding experimental API. We also added 'sparse.dot' operator to enhance the sparse tensor support. We also made some changes to our APIs to improve user experience. For example, we added `lazy_update` option for standard `SGD` & `Adam` optimizer with `row_sparse` gradients. </p>
<p>MXNet is now faster and more scalable. We improved GPU inference speed by 20% when batch size is 1. Improved batching for GEMM/TRSM operators with large matrices on GPU makes it faster for you to train models. We also added multi-threading for the class of broadcast_reduce operators on CPU. </p>
