---
layout: post
status: PUBLISHED
published: true
title: 'HDFS HSM and HBase: Experiment (continued) (Part 5 of 7)'
author:
  display_name: Michael Stack
  login: stack
  email: stack@apache.org
author_login: stack
author_email: stack@apache.org
id: 6d1ffc55-98ce-4506-bcce-0db8eb0a55d6
date: '2019-03-02 21:42:29 -0500'
categories:
- General
tags:
- performance
- evaluation
- part5of7
comments: []
permalink: hbase/entry/hdfs_hsm_and_hbase_cluster2
---
<p>This is part 5 of a 7 part report by HBase Contributor, Jingcheng Du and HDFS contributor, Wei Zhou (Jingcheng and Wei are both Software Engineers at Intel) </p>
<ol>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_part"><font size="1">Introduction</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster"><font size="1">Cluster Setup</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_tuning"><font size="1">Tuning</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster1"><font size="1">Experiment</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster2"><font size="1">Experiment (continued)</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster3"><font size="1">Issues</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster4"><font size="1">Conclusions</font></a></li>
</ol>
<h3 dir="ltr" style="line-height: 1.38; margin-top: 14pt; margin-bottom: 4pt;" id="docs-internal-guid-894f636c-4053-f7bb-3e5c-3cf5c6a69c67"><span style="font-size: 16px; font-family: Calibri; color: #0563c1; font-weight: 400; vertical-align: baseline; background-color: transparent;">1TB Dataset in a Single Storage</span></h3>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The performance for 1TB dataset in HDD and SSD is shown in Figure 6 and Figure 7. Due to the limitation of memory capability, 1TB dataset in RAMDISK is not tested.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh4.googleusercontent.com/MYR8t15XS-6cOZ0G942EGYVk8HQ2AQFZqfwI2JM6wC08_7IkYWz7CziHcQC8GBJ84NI-ppUvAhX75wBmHZ9PKeKKK9rlZ8GHEDp34BOQ0Fb6a8TULpfUFDlxQhe29qiTO0JADodl" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 6. YCSB throughput of a single storage type with 1TB dataset</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;"><img src="https://lh6.googleusercontent.com/DYg8Fa-uIHfNMtHzLk6UpPmcU2-O4vi80AieUGgJ4jZ-Dtk9mBRE2ZHYP0tVUjZofVC1A3NCFGK_QP07jQ-vFolG1QyfmDFCyp99Se3pKR6RlSFoW1NehGAcFYSOh7MEx0ZMFxvi" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 7. YCSB latency of a single storage type with 1TB dataset</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The throughput and latency on SSD are both better than HDD (134% throughput and 35% latency). This is consistent with 50GB data test.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The benefits gained for throughput by using SSD are different between 50GB and 1TB (from 128% to 134%), SSD gains more benefits in the 1TB test. This is because much more I/O intensive events such as compactions occur in 1TB dataset test than 50GB, and this shows the superiority of SSD in huge data scenarios. Figure 8 shows the changes of the network throughput during the tests.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh5.googleusercontent.com/EoJ1a-x51qiMbic5D-VYKXlJk51BWmVHftTz85Q7CQLC2C_Wwl95Py9XOnTW8YLmLsuCRIr4VUt2WTxqtPCdDFrQKVWeREpr-BhX73ZimfYsGHKHpeASdcKSPvwRJ2sbPCoDmzdI" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 8. Network throughput measured for case 1T_HDD and 1T_SSD</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">In 1T_HDD case the network throughput is lower than 10Gbps, and in 1T_SSD case the network throughput can be much larger than 10Gbps. This means if we use a 10Gbps switch in 1T_SSD case, the network should be the bottleneck.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh5.googleusercontent.com/XCo4nOOIHpFOk-f8UHkwp1MRIefbJEU44YBCc9tDMnP_estMgEFblT51DA-f64Y-A-xRJTLmH0bA-wODvImtI8gO7EmKMzA9AgBcDGxZhCcqog3nPwGkDdnamSOw2D4TXpCeTBtm" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 9. Disk throughput measured for case 1T_HDD and 1T_SSD</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">In Figure 9, we can see the bottleneck for these two cases is disk bandwidth.</span></p>
<ul style="margin-top: 0pt; margin-bottom: 0pt;">
<li dir="ltr" style="list-style-type: disc; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">In 1T_HDD, at the beginning of the test the throughput is almost 1000 MB/s, but after a while the throughput drops down due to memstore limitation of regions caused by slow flush.</span></p>
</li>
<li dir="ltr" style="list-style-type: disc; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">In 1T_SSD case, the throughput seems to be limited by a ceiling of around 1300 MB/s, nearly the same with the bandwidth limitation of SATA controllers. To further improve the throughput, more SATA controllers are needed (e.g. using HBA card) instead of more SSDs are needed.</span></p></p>
</li>
</ul>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">During 1T_SSD test, we observe that the operation latencies on eight SSDs per node are very different as shown in the following chart. In Figure 10, we only include latency of two disks, sdb represents disks with a high latency and sdf represents disks with a low latency.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh6.googleusercontent.com/3oF43nq68BJ_GNnAhQ8UKytRoyQn5WYa0A7luL35V6Vdrd8uqss3fAg8sss3KQJr4fMFU9T6-2wpwwhzUltPrujtf9UnSO-89z7gMF0hE5HCTOYQ9iPBcf-BSrNsrVR6FGQauVs6" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 10. I/O await time measured for different disks</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Four of them have a better latency than the other ones. This is caused by the hardware design issue. You can find the details in </span><a href="https://docs.google.com/document/d/1CEpvMfrh5HxbUl7DfkrRXZHI4wCzOIehFlvDlfYr4LA/edit#heading=h.q7x244fcmnlo"><span style="font-size: 14.6667px; font-family: Calibri; color: #1155cc; text-decoration: underline; vertical-align: baseline; background-color: transparent;">Disk I/O Bandwidth and Latency Varies for Ports</span></a><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">. The disk with higher latency might take the same workload as the disks with lower latency in the existing VolumeChoosingPolicy, this would slow down the performance. We suggest to implement a latency-aware VolumeChoosingPolicy in HDFS.</span></p>
<h3 dir="ltr" style="line-height: 1.38; margin-top: 14pt; margin-bottom: 4pt;"><span style="font-size: 16px; font-family: Calibri; color: #0563c1; font-weight: 400; vertical-align: baseline; background-color: transparent;">Performance Estimation for RAMDISK with 1TB Dataset</span></h3>
<p dir="ltr" style="line-height: 1.272; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">We cannot measure the performance of RAMDISK with 1T dataset due to RAMDISK limited capacity. Instead we have to evaluate its performance by analyzing the results of cases HDD and SSD.</span></p></p>
<p dir="ltr" style="line-height: 1.272; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The performance between 1TB and 50GB dataset are pretty close in HDD and SSD.</span></p>
<p dir="ltr" style="line-height: 1.272; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The throughput difference between 50GB and 1TB dataset for HDD is</span></p>
<p dir="ltr" style="line-height: 1.284; margin-top: 0pt; margin-bottom: 2pt; text-align: justify;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">|</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">242801</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">250034</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">-1|&times;100%=2.89%</span></p></p>
<p dir="ltr" style="line-height: 1.272; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">While for SSD the value is</span></p>
<p dir="ltr" style="line-height: 1.284; margin-top: 0pt; margin-bottom: 2pt; text-align: justify;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">|</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">325148</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">320616</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">-1|&times;100%=1.41%</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">If we make an average of the above values as the throughput decrease in RAMDISK between 50GB and 1TB dataset, it is around 2.15% ((2.89%+1.41%)/2=2.15%), thus the throughput for RAMDISK with 1T dataset should be</span></p>
<p dir="ltr" style="line-height: 1.284; margin-top: 0pt; margin-bottom: 2pt; text-align: justify;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">406577&times;(1+2.15%)=415318 (ops/sec)</span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh5.googleusercontent.com/jYwZJ5VSd1cleLRiw9kDPCJdd0W8l-93VGb4R3avpL2qmJge2vGF5xhnCcjktgUen9T7F-97_jzCA4tPoFDHRX5hAeoUGoHapacm5zAUE6bFtcFPcFzc4VgtG3yV_SVMU7_fXNvl" style="transform: rotate(0rad);" /></span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 11. &nbsp;YCSB throughput estimation for RAMDISK with 1TB dataset</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Please note: the throughput doesn&rsquo;t drop much in 1 TB dataset cases compared to 50 GB dataset cases because they do not use the same number of pre-split regions. The table is pre-split to 18 regions in 50 GB dataset cases, and it is pre-split to 210 regions in the 1 TB dataset.</span></p>
<h2 dir="ltr" style="line-height: 1.38; margin-top: 18pt; margin-bottom: 4pt;" id="docs-internal-guid-894f636c-4054-b155-6bbf-2a9c2a2c6f24"><span style="font-size: 17.3333px; font-family: Calibri; color: #1e4d78; font-weight: 400; vertical-align: baseline; background-color: transparent;">Performance for Tiered Storage</span></h2>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">In this section, we will study the HBase write performance on tiered storage (i.e. different storage mixed together in one test). This would show what performance it can achieve by mixing fast and slow storage together, and help us to conclude the best balance of storage between performance and cost.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Figure 12 and Figure 13 show the performance for tiered storage. You can find the description of each case in Table 1.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Most of the cases that introduce fast storage have better throughput and latency. With no surprise, 1T_RAM_SSD has the best performance among them. The real surprise is that the throughput of 1T_RAM_HDD is worse than 1T_HDD (-11%) and 1T_RAM_SSD_All_HDD is worse than 1T_SSD_All_HDD (-2%) after introducing RAMDISK, and 1T_SSD is worse than 1T_SSD_HDD (-2%).</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh6.googleusercontent.com/zK8dt71iE6k2dJSz_z4fiZIoqSbDVpE1AOlPu5kJKAVuCqXpehNYFYRPYD-qzYtK0DDImyPIdVck4BDqRxJVUL2EvK_ZRjPVQroz0sgi_C1MK33OaO-_MAMEyEQtfW2NEHA8jd0I" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 12. &nbsp;YCSB throughput data for tiered storage</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh3.googleusercontent.com/cniaqnU2o2mNQU2fUJvYXBpqKXtZ3ZNqsQ7ymeFl55MFQVzuCOIKqR7f75Ia_k1-Zvm1jWibXkqqL2csh6nzOo89g0FFPB8PT7_kzB58pkbCaErt6tCCf-ASDgbOQ3RbdH_nqtlZ" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 13. &nbsp;YCSB latency data for tiered storage</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">We also investigate how much data is written to different storage types by collecting information from one DataNode.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh6.googleusercontent.com/Lj4D1xeN-7_lIFp7louMD1-VguZUqdI6nveLHn_eeC3PzrN24AAL8ognrg4VZ8Vi6qTo4Scma529r8yrBsXNGize19_fWfXlKTtdc0RRUAXYHvMSjWQx2YlesXiVohlWKvkXqmKv" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 14. Distribution of data blocks on each storage of HDFS in one DataNode</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">As shown in Figure 14, generally, more data are written to disks for test cases with higher throughput. Fast storage can accelerate the flush and compaction, which lead to more flushes and compactions. &nbsp;Thus, more data are written to disks. In some RAMDISK-related cases, only WAL can be written to RAMDISK, and there are 1216 GB WALs written to one DataNode.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">For tests without SSD (1T_HDD and 1T_RAM_HDD), we by purpose limiting the number of flush and compaction actions by using fewer flushers and compactors. This is due to limited IOPs capability of HDD, which lead to fewer flush &amp; compactions. Too many concurrent reads and writes can hurt HDD performance which eventually slows down the performance.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Many BLOCKED DataNode threads can be blocked up to tens of seconds in 1T_RAM_HDD. We observe this in other cases as well, but it happens most often in 1T_RAM_HDD. This is because each DataNode holds one big lock when creating/finalizing HDFS blocks, these methods might take tens of seconds sometimes (see </span><a href="https://docs.google.com/document/d/1CEpvMfrh5HxbUl7DfkrRXZHI4wCzOIehFlvDlfYr4LA/edit#heading=h.956plr9mrft9"><span style="font-size: 14.6667px; font-family: Calibri; color: #1155cc; text-decoration: underline; vertical-align: baseline; background-color: transparent;">Long-time BLOCKED threads in DataNode</span></a><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">), the more these methods are used (in HBase they are used in flusher, compactor, and WAL), the more often the BLOCKED occurs. Writing WAL in HBase needs to create/finalize blocks which can be blocked, and consequently users&rsquo; inputs are blocked. Multiple WAL with a large number of groups or WAL per region might also encounter this problem, especially in HDD.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">With the written data distribution in mind, let&rsquo;s look back at the performance result in Figure 12 and Figure 13. According to them, we have following observations:</span></p>
<ol style="margin-top: 0pt; margin-bottom: 0pt;">
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Mixing SSD and HDD can greatly improve the performance (136% throughput and 35% latency) compared to pure HDD. But fully replacing HDD with SSD doesn&rsquo;t show an improvement (98% throughput and 99% latency) over mixing SSD/HDD. This is because the hardware design cannot evenly split the I/O bandwidth to all eight disks, and 94% data are written in SSD while only 6% data are written to HDD in SSD/HDD mixing case. This strongly hints a mix use of SSD/HDD can achieve the best balance between performance and cost. More information is in </span><a href="https://docs.google.com/document/d/1CEpvMfrh5HxbUl7DfkrRXZHI4wCzOIehFlvDlfYr4LA/edit#heading=h.uf53tqx1vmld"><span style="font-size: 14.6667px; color: #1155cc; text-decoration: underline; vertical-align: baseline; background-color: transparent;">Disk Bandwidth Limitation</span></a><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;"> and </span><a href="https://docs.google.com/document/d/1CEpvMfrh5HxbUl7DfkrRXZHI4wCzOIehFlvDlfYr4LA/edit#heading=h.q7x244fcmnlo"><span style="font-size: 14.6667px; color: #1155cc; text-decoration: underline; vertical-align: baseline; background-color: transparent;">Disk I/O Bandwidth and Latency Varies for Ports</span></a><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">.</span></p>
</li>
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Including RAMDISK in SSD/HDD tiered storage has different results with 1T_RAM_SSD_All_HDD and 1T_RAM_SSD_HDD. The case 1T_RAM_SSD_HDD shows a result when there are only a few data written to HDD, which improves the performance over SSD/HDD mixing cases. The results of 1T_RAM_SSD_All_HDD when there are a large number of data written to HDD is worse than SSD/HDD mixing cases. This means if we distribute the data appropriately to SSD and HDD in HBase, we can gain a good performance when mixing RAMDISK/SSD/HDD.</span></p>
</li>
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">The RAMDISK/SSD tiered storage is the winner of both throughput and latency (109% throughput and 67% latency of pure SSD case). So, if cost is not an issue and maximum performance is needed, RAMDISK/SSD should be chosen.</span></p></p>
</li>
</ol>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The throughput decreases by 11% by comparing 1T_RAM_HDD to 1T_HDD. This is initially because 1T_RAM_HDD uses RAMDISK which consumes part of the RAM, which results in the OS buffer having less memory to cache the data.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Further, with 1T_RAM_HDD, the YCSB client can push data at very high speed, cells are accumulated very fast in memstore while the flush and compaction in HDD are slow, the RegionTooBusyException occurs more often (the figure below shows a much larger memstore in 1T_RAM_HDD than 1T_HDD), and we observe much longer GC pause in 1T_RAM_HDD than 1T_HDD, it can be up to 20 seconds in a minute.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh6.googleusercontent.com/G_iccez14CXyzQfKQ_74zJnOZKfqJm2O7s1J8XwzcMs4w8v9q9MxxH4jjaCCzbWJUS73ffnJ9ODPhvB9X-8QUGHTAdJ6L6wGqgsALDGvb9RAt7dtkQYXi-it-zAqp5VFaKCHBk6X" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 15. Memstore size in 1T_RAM_HDD and 1T_HDD</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Finally, as we try to increase the number of flushers and compactors, the performance even goes worse because of the reasons mentioned when explaining why we use less flusher and compactors in HDD-related tests (see </span><a href="https://docs.google.com/document/d/1CEpvMfrh5HxbUl7DfkrRXZHI4wCzOIehFlvDlfYr4LA/edit#heading=h.956plr9mrft9"><span style="font-size: 14.6667px; font-family: Calibri; color: #1155cc; text-decoration: underline; vertical-align: baseline; background-color: transparent;">Long-time BLOCKED threads in DataNode</span></a><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">).</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The performance reduction in 1T_RAM_SSD_All_HDD than 1T_SSD_All_HDD (-2%) is due to the same reasons mentioned above.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">We suggest:</span></p></p>
<p dir="ltr" style="font-family: Calibri; font-size: 14.6667px; line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; display: inline !important; background-color: transparent;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Implement a finer grained lock mechanism in DataNode.</span></p></p>
<ol style="margin-top: 0pt; margin-bottom: 0pt;">
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Use reasonable configurations for flusher and compactor, especially in HDD-related cases.</span></p>
</li>
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Don&rsquo;t use the storage that has large performance gaps, such as directly mixing RAMDISK and HDD together.</span></p>
</li>
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">In many cases, we can observe the long GC pause around 10 seconds per minute. We need to implement an off-heap memstore in HBase to solve long GC pause issues.</span></p>
</li>
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Implement a finer grained lock mechanism in DataNode.</span><span style="font-size: 14.6667px; line-height: normal; background-color: transparent;"> </span></p>
<p></p>
</li>
</ol>
<div><font face="Calibri"><span style="font-size: 14.6667px;">Go to part 6,&nbsp;</span></font><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster3">Issues</a></div>
<p><span style="font-size: 14.6667px; font-family: Arial; vertical-align: baseline; background-color: transparent;"></span> </p>
