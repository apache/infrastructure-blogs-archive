---
layout: post
title: Apache MXNet 0.12 Release Adds Support for New NVIDIA Volta GPUs and Sparse
  Tensor
date: '2017-11-01T18:01:50+00:00'
categories: mxnet
---
<p>
We are excited about the availability of Apache MXNet version 0.12. With this release, MXNet adds two new important features: support for NVIDIA Volta GPUs and support for Sparse Tensors
<br/>
</p>

<b> Support for NVIDIA Volta GPU Architecture </b>

<p>
The MXNet v0.12 release adds support for NVIDIA Volta V100 GPUs, enabling users to train convolutional neural networks up to 3.5 times faster than on the Pascal GPUs. Trillions of floating-point (FP) multiplications and additions for training a neural network have typically been done using single precision (FP32) to achieve high accuracy. However, recent research has shown that the same accuracy can be achieved using half-precision (FP16) data types.
</p>

<p>
The Volta GPU architecture introduces Tensor Cores. Each Tensor Core can execute 64 fuse-multiply-add ops per clock, which roughly quadruples the CUDA core FLOPS per clock per core. Each Tensor Core performs D = A x B + C, where A and B are half-precision matrices, while C and D can be either half or single-precision matrices, thereby performing mixed precision training. The new mixed-precision training allows users to achieve optimal training performance without sacrificing accuracy by using FP16 for most of the layers of a network, and higher precision data types only when necessary.
</p>

<p>
You can take advantage of Volta Tensor Cores to enable FP16 training in MXNet by passing a simple command, "--dtype float16" to the MXNet training script. For example, you can invoke <a href="https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/train_imagenet.py"> imagenet training script </a> with command:
 train_imagenet.py --dtype float16
<br/> 
</p>

<b> Sparse Tensor Support </b>

<p>
MXNet v0.12 adds support for sparse tensors to efficiently store and compute tensors allowing developers to perform sparse matrix operations in a storage and compute-efficient manner and train deep learning models faster. MXNet v0.12 supports two major sparse data formats: Compressed Sparse Row (CSR) and Row Sparse (RSP). The CSR format is optimized to represent matrices with a large number of columns where each row has only a few non-zero elements. The RSP format is optimized to represent matrices with a huge number of rows where most of the row slices are complete zeros. For example, the CSR format can be used to encode the feature vectors of input data for a recommendation engine, whereas the RSP format can be used to perform the sparse gradient updates during training. This release enables sparse support on CPU for most commonly used operators such as matrix dot product and element-wise operators. Sparse support for more operators will be added in future releases.
</p>

<p>
Follow <a href="https://mxnet.incubator.apache.org/versions/master/tutorials/#sparse-ndarray">these tutorials</a> to learn how to use the new sparse operators in MXNet.  
</p>

<p>
Get Apache MXNet 0.12 from <a href="http://www.apache.org/dist/incubator/mxnet/"> downloads page </a>. Read more about this release in <a href="https://github.com/apache/incubator-mxnet/releases/tag/0.12.0"> Release Notes </a>.
</p>
 
<p>
Or, You can download and play with MXNet easily using one of the options below:
<ul>
<li>
The Pip package can be found here: <a href="https://pypi.python.org/pypi/mxnet">https://pypi.python.org/pypi/mxnet</a>
</li>
<li>
The Docker Images can be found here: <a href="https://hub.docker.com/u/mxnet/">https://hub.docker.com/u/mxnet/ </a>
</li>
</p>

<p> 
If you want to learn more about MXNet visit <a href="https://mxnet.incubator.apache.org/">https://mxnet.incubator.apache.org/</a>. 
Finally, you are welcome to join and also invite your friends to the dynamic and growing MXNet community by subscribing to dev@mxnet.incubator.apache.org
</p> 
 
