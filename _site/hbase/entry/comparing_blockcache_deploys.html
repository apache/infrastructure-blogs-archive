<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Comparing BlockCache Deploys | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Comparing BlockCache Deploys" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Comparing BlockCache Deploys St.Ack on August 7th, 2014 A report comparing BlockCache deploys in Apache HBase for issue HBASE-11323 BucketCache all the time! Attempts to roughly equate five different deploys and compare how well they do under four different loading types that vary from no cache-misses through to missing cache most of the time. Makes recommendation on when to use which deploy. Prerequisite In Nick Dimiduk&#39;s BlockCache 101 blog post, he details the different options available in HBase. We test what remains after purging SlabCache and the caching policy implementation DoubleBlockCache which have been deprecated in HBase 0.98 and removed in trunk because of Nick&#39;s findings and that of others. Nick varies the JVM Heap+BlockCache sizes AND dataset size.&nbsp; This report keeps JVM Heap+BlockCache size constant and varies the dataset size only. Nick looks at the 99th percentile only.&nbsp; This article looks at that, as well as GC, throughput and loadings. Cell sizes in the following tests also vary between 1 byte and 256k in size. Findings If the dataset fits completely in cache, the default configuration, which uses the onheap LruBlockCache, performs best.&nbsp; GC is half that of the next most performant deploy type, CombinedBlockCache:Offheap with at least 20% more throughput.Otherwise, if your cache is experiencing churn running a steady stream of evictions, move your block cache offheap using CombinedBlockCache in the offheap mode. See the BlockCache section in the HBase Reference Guide for how to enable this deploy. Offheap mode requires only one third to one half of the GC of LruBlockCache when evictions are happening. There is also less variance as the cache misses climb when offheap is deployed. CombinedBlockCache:file mode has better GC profile but less throughput than CombinedBlockCache:Offheap. Latency is slightly higher on CBC:Offheap -- heap objects to cache have to be serialized in and out of the offheap memory -- than with the default LruBlockCache but the 95/99th percentiles are slightly better.&nbsp; CBC:Offheap uses a bit more CPU than LruBlockCache. CombinedBlockCache:Offheap is limited only by the amount of RAM available.&nbsp; It is not subject to GC. Test &lt;/p&gt; The graphs to follow show results from five different deploys each run through four different loadings. Five Deploy Variants LruBlockCache The default BlockCache in place when you start up an unconfigured HBase install. With LruBlockCache, all blocks are loaded into the java heap. See BlockCache 101 and LruBlockCache for detail on the caching policy of LruBlockCache. CombinedBlockCache:Offheap CombinedBlockCache deploys two tiers; an L1 which is an LruBlockCache instance to hold META blocks only (i.e. INDEX and BLOOM blocks), and an L2 tier which is an instance of BucketCache. In this offheap mode deploy, the BucketCache uses DirectByteBuffers to host a BlockCache outside of the JVM heap to host cached DATA blocks. CombinedBlockCache:Onheap In this onheap (&#39;heap&#39; mode), the L2 cache is hosted inside the JVM heap, and appears to the JVM to be a single large allocation. Internally it is managed by an instance of BucketCache The L1 cache is an instance of LruBlockCache. CombinedBlockCache:file In this mode, an L2 BucketCache instance puts DATA blocks into a file (hence &#39;file&#39; mode) on a mounted tmpfs in this case. CombinedBlockCache:METAonly No caching of DATA blocks (no L2 instance). DATA blocks are fetched every time. The INDEX blocks are loaded into an L1 LruBlockCache instance. Memory is fixed for each deploy. The java heap is a small 8G to bring on distress earlier. For deploy type 1., the LruBlockCache is given 4G of the 8G JVM heap. For 2.-5. deploy types., the L1 LruHeapCache is 0.1 * 8G (~800MB), which is more than enough to host the dataset META blocks. This was confirmed by inspecting the Block Cache vitals displayed in the RegionServer UI. For 2., 3., and 4. deploy types, the L2 bucket cache is 4G. Deploy type 5.&#39;s L2 is 0G (Used HBASE-11581 Add option so CombinedBlockCache L2 can be null (fscache)). Four Loading Types All cache hits all the time. A small percentage of cache misses, with all misses inside the fscache soon after the test starts. Lots of cache misses, all still inside the fscache soon after the test starts. Mostly cache misses where many misses by-pass the fscache. For each loading, we ran 25 clients reading randomly over 10M cells of zipfian varied cell sizes from 1 byte to 256k bytes over 21 regions hosted by a single RegionServer for 20 minutes. Clients would stay inside the cache size for loading 1., miss the cache at just under ~50% for loading 2., and so on. font-family: Times; font-size: medium; margin-bottom: 0in; line-height: 18.239999771118164px;&quot;&gt;The dataset was hosted on a single RegionServer on small HDFS cluster of 5 nodes.&nbsp; See below for detail on the setup. Graphs For each attribute -- GC, Throughput, i/o -- we have five charts across; one for each deploy.&nbsp; Each graph can be divided into four parts; the first quarter has the all-in-cache loading running for 20minutes, followed by the loading that has some cache misses (for twenty minutes), through to loading with mostly cache misses in the final quarter.To find out what each color represents, read the legend.&nbsp; The legend is small in the below. To see a larger version, browse to this non-apache-roller version of this document and click on the images over there.Concentrate on the first four graph types.&nbsp; The BlockCache Stats and I/O graphs add little other than confirming that the loadings ran the same across the different BlockCache deploys with fscache cutting in to soak up the seeks soon after start for all but the last mostly-cache-miss loading cases. GC Be sure to check the y-axis in the graphs below (you can click on chart to get a larger version). All profiles look basically the same but a check of the y-axis will show that for all but the no-cache-misses case, CombinedBlockCache:Offheap, the second deploy type, has the best GC profile (less GC is better!).The GC for the CombinedBLockCache:Offheap deploy mode looks to be climbing as the test runs.&nbsp; See the Notes section on the end for further comment. &lt;/p&gt; Throughput CombinedBlockCache:Offheap is better unless there are few to no cache misses.&nbsp; In that case, LruBlockCache shines (I missed why there is the step in the LruBlockCache all-in-cache section of the graph) Latency Latency when in-cache Same as above except that we do not show the last loading -- we show the first three only -- since the last loading of mostly cache misses skews the diagrams such that it is hard to compare latency when we are inside cache (BlockCache and fscache). Load Try aggregating the system and user CPUs when comparing. BlockCache Stats Curves are mostly the same except for the cache-no-DATA-blocks case. It has no L2 deployed. I/O Read profiles are about the same across all deploys and tests with a spike at first until the fscache comes around and covers. The exception is the final mostly-cache-misses case. Setup Master branch. 2.0.0-SNAPSHOT, r69039f8620f51444d9c62cfca9922baffe093610.&nbsp; Hadoop 2.4.1-SNAPSHOT.5 nodes all the same with 48G and six disks.&nbsp; One master and one regionserver, each on distinct nodes, with 21 regions of 10M rows of zipf varied size -- 0 to 256k -- created as follows: $ export HADOOP_CLASSPATH=/home/stack/conf_hbase:`./hbase-2.0.0-SNAPSHOT/bin/hbase classpath` $ nohup&nbsp; ./hadoop-2.4.1-SNAPSHOT/bin/hadoop --config /home/stack/conf_hadoop org.apache.hadoop.hbase.PerformanceEvaluation --valueSize=262144 --valueZipf --rows=100000 sequentialWrite 100 &amp; Here is my loading script.&nbsp; It performs 4 loadings: All in cache, just out of cache, a good bit out of cache and then mostly out of cache: [stack@c2020 ~]$ more bin/bc_in_mem.sh #!/bin/sh HOME=/home/stack testtype=$1 date=`date -u +&quot;%Y-%m-%dT%H:%M:%SZ&quot; echo testtype=$testtype $date` &gt;&gt; nohup.out HBASE_HOME=$HOME/hbase-2.0.0-SNAPSHOT runtime=1200 clients=25 cycles=1000000 #for i in 38 76 304 1000; do for i in 32 72 144 1000; do &nbsp; echo &quot;`date` run size=${i}, clients=$clients ; $testtype time=$runtime size=$i&quot; &gt;&gt; nohup.out &nbsp; timeout $runtime nohup ${HBASE_HOME}/bin/hbase --config /home/stack/conf_hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --valueSize=110000 --size=$i --cycles=$cycles randomRead $clients done Java version: [stack@c2020 ~]$ java -version java version &quot;1.7.0_45&quot; Java(TM) SE Runtime Environment (build 1.7.0_45-b18) Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode) I enabled caching options by uncommenting these configuration properties in hbase-site.xml . The hfile.block.cache.size was set to .5 to keep math simple. hfile.block.cache.size 0.5 &nbsp; hbase.bucketcache.ioengine &nbsp; heap &nbsp; hbase.bucketcache.size &nbsp; 4196 Notes The CombinedBlockCache has some significant overhead (to be better quantified -- 10%?). There will be slop left over because buckets will not fill to the brim especially when block sizes vary.&nbsp; Observed running loadings.I tried to bring on a FullGC by running for more than a day with LruBlockCache fielding mostly BlockCache misses and I failed. Thinking on it, the BlockCache only occupied 4G of an 8G heap. There was always elbow room available (Free block count and maxium block size allocatable settled down to a nice number and held constant). TODO: Retry but with less elbow room available. Longer running CBC offheap test Some of the tests above show disturbing GC tendencies -- ever rising GC -- so I ran tests for a longer period .&nbsp; It turns out that BucketCache throws an OOME in long-running tests. You need HBASE-11678 BucketCache ramCache fills heap after running a few hours (fixed in hbase-0.98.6+)&lt;/p&gt; After the fix, the below test ran until the cluster was pul led out from under the loading: Here are some long running tests with bucket cache onheap (ioengine=heap). The throughput is less and GC is higher. .&lt;/span&gt; Future Does LruBlockCache get more erratic as heap size grows?&nbsp; Nick&#39;s post implies that it does.Auto-sizing of L1, onheap cache.HBASE-11331 [blockcache] lazy block decompression has a report attachedthat evaluates the current state of attached patch to keep blocks compressed while in the BlockCache.&nbsp; It finds that with the patch enabled, &quot;More GC. More CPU. Slower. Less throughput. But less i/o.&quot;, but there is an issue with compression block pooling that likely impinges on performance.Review serialization in and out of L2 cache. No serialization?&nbsp; Because we can take blocks from HDFS without copying onheap?" />
<meta property="og:description" content="Comparing BlockCache Deploys St.Ack on August 7th, 2014 A report comparing BlockCache deploys in Apache HBase for issue HBASE-11323 BucketCache all the time! Attempts to roughly equate five different deploys and compare how well they do under four different loading types that vary from no cache-misses through to missing cache most of the time. Makes recommendation on when to use which deploy. Prerequisite In Nick Dimiduk&#39;s BlockCache 101 blog post, he details the different options available in HBase. We test what remains after purging SlabCache and the caching policy implementation DoubleBlockCache which have been deprecated in HBase 0.98 and removed in trunk because of Nick&#39;s findings and that of others. Nick varies the JVM Heap+BlockCache sizes AND dataset size.&nbsp; This report keeps JVM Heap+BlockCache size constant and varies the dataset size only. Nick looks at the 99th percentile only.&nbsp; This article looks at that, as well as GC, throughput and loadings. Cell sizes in the following tests also vary between 1 byte and 256k in size. Findings If the dataset fits completely in cache, the default configuration, which uses the onheap LruBlockCache, performs best.&nbsp; GC is half that of the next most performant deploy type, CombinedBlockCache:Offheap with at least 20% more throughput.Otherwise, if your cache is experiencing churn running a steady stream of evictions, move your block cache offheap using CombinedBlockCache in the offheap mode. See the BlockCache section in the HBase Reference Guide for how to enable this deploy. Offheap mode requires only one third to one half of the GC of LruBlockCache when evictions are happening. There is also less variance as the cache misses climb when offheap is deployed. CombinedBlockCache:file mode has better GC profile but less throughput than CombinedBlockCache:Offheap. Latency is slightly higher on CBC:Offheap -- heap objects to cache have to be serialized in and out of the offheap memory -- than with the default LruBlockCache but the 95/99th percentiles are slightly better.&nbsp; CBC:Offheap uses a bit more CPU than LruBlockCache. CombinedBlockCache:Offheap is limited only by the amount of RAM available.&nbsp; It is not subject to GC. Test &lt;/p&gt; The graphs to follow show results from five different deploys each run through four different loadings. Five Deploy Variants LruBlockCache The default BlockCache in place when you start up an unconfigured HBase install. With LruBlockCache, all blocks are loaded into the java heap. See BlockCache 101 and LruBlockCache for detail on the caching policy of LruBlockCache. CombinedBlockCache:Offheap CombinedBlockCache deploys two tiers; an L1 which is an LruBlockCache instance to hold META blocks only (i.e. INDEX and BLOOM blocks), and an L2 tier which is an instance of BucketCache. In this offheap mode deploy, the BucketCache uses DirectByteBuffers to host a BlockCache outside of the JVM heap to host cached DATA blocks. CombinedBlockCache:Onheap In this onheap (&#39;heap&#39; mode), the L2 cache is hosted inside the JVM heap, and appears to the JVM to be a single large allocation. Internally it is managed by an instance of BucketCache The L1 cache is an instance of LruBlockCache. CombinedBlockCache:file In this mode, an L2 BucketCache instance puts DATA blocks into a file (hence &#39;file&#39; mode) on a mounted tmpfs in this case. CombinedBlockCache:METAonly No caching of DATA blocks (no L2 instance). DATA blocks are fetched every time. The INDEX blocks are loaded into an L1 LruBlockCache instance. Memory is fixed for each deploy. The java heap is a small 8G to bring on distress earlier. For deploy type 1., the LruBlockCache is given 4G of the 8G JVM heap. For 2.-5. deploy types., the L1 LruHeapCache is 0.1 * 8G (~800MB), which is more than enough to host the dataset META blocks. This was confirmed by inspecting the Block Cache vitals displayed in the RegionServer UI. For 2., 3., and 4. deploy types, the L2 bucket cache is 4G. Deploy type 5.&#39;s L2 is 0G (Used HBASE-11581 Add option so CombinedBlockCache L2 can be null (fscache)). Four Loading Types All cache hits all the time. A small percentage of cache misses, with all misses inside the fscache soon after the test starts. Lots of cache misses, all still inside the fscache soon after the test starts. Mostly cache misses where many misses by-pass the fscache. For each loading, we ran 25 clients reading randomly over 10M cells of zipfian varied cell sizes from 1 byte to 256k bytes over 21 regions hosted by a single RegionServer for 20 minutes. Clients would stay inside the cache size for loading 1., miss the cache at just under ~50% for loading 2., and so on. font-family: Times; font-size: medium; margin-bottom: 0in; line-height: 18.239999771118164px;&quot;&gt;The dataset was hosted on a single RegionServer on small HDFS cluster of 5 nodes.&nbsp; See below for detail on the setup. Graphs For each attribute -- GC, Throughput, i/o -- we have five charts across; one for each deploy.&nbsp; Each graph can be divided into four parts; the first quarter has the all-in-cache loading running for 20minutes, followed by the loading that has some cache misses (for twenty minutes), through to loading with mostly cache misses in the final quarter.To find out what each color represents, read the legend.&nbsp; The legend is small in the below. To see a larger version, browse to this non-apache-roller version of this document and click on the images over there.Concentrate on the first four graph types.&nbsp; The BlockCache Stats and I/O graphs add little other than confirming that the loadings ran the same across the different BlockCache deploys with fscache cutting in to soak up the seeks soon after start for all but the last mostly-cache-miss loading cases. GC Be sure to check the y-axis in the graphs below (you can click on chart to get a larger version). All profiles look basically the same but a check of the y-axis will show that for all but the no-cache-misses case, CombinedBlockCache:Offheap, the second deploy type, has the best GC profile (less GC is better!).The GC for the CombinedBLockCache:Offheap deploy mode looks to be climbing as the test runs.&nbsp; See the Notes section on the end for further comment. &lt;/p&gt; Throughput CombinedBlockCache:Offheap is better unless there are few to no cache misses.&nbsp; In that case, LruBlockCache shines (I missed why there is the step in the LruBlockCache all-in-cache section of the graph) Latency Latency when in-cache Same as above except that we do not show the last loading -- we show the first three only -- since the last loading of mostly cache misses skews the diagrams such that it is hard to compare latency when we are inside cache (BlockCache and fscache). Load Try aggregating the system and user CPUs when comparing. BlockCache Stats Curves are mostly the same except for the cache-no-DATA-blocks case. It has no L2 deployed. I/O Read profiles are about the same across all deploys and tests with a spike at first until the fscache comes around and covers. The exception is the final mostly-cache-misses case. Setup Master branch. 2.0.0-SNAPSHOT, r69039f8620f51444d9c62cfca9922baffe093610.&nbsp; Hadoop 2.4.1-SNAPSHOT.5 nodes all the same with 48G and six disks.&nbsp; One master and one regionserver, each on distinct nodes, with 21 regions of 10M rows of zipf varied size -- 0 to 256k -- created as follows: $ export HADOOP_CLASSPATH=/home/stack/conf_hbase:`./hbase-2.0.0-SNAPSHOT/bin/hbase classpath` $ nohup&nbsp; ./hadoop-2.4.1-SNAPSHOT/bin/hadoop --config /home/stack/conf_hadoop org.apache.hadoop.hbase.PerformanceEvaluation --valueSize=262144 --valueZipf --rows=100000 sequentialWrite 100 &amp; Here is my loading script.&nbsp; It performs 4 loadings: All in cache, just out of cache, a good bit out of cache and then mostly out of cache: [stack@c2020 ~]$ more bin/bc_in_mem.sh #!/bin/sh HOME=/home/stack testtype=$1 date=`date -u +&quot;%Y-%m-%dT%H:%M:%SZ&quot; echo testtype=$testtype $date` &gt;&gt; nohup.out HBASE_HOME=$HOME/hbase-2.0.0-SNAPSHOT runtime=1200 clients=25 cycles=1000000 #for i in 38 76 304 1000; do for i in 32 72 144 1000; do &nbsp; echo &quot;`date` run size=${i}, clients=$clients ; $testtype time=$runtime size=$i&quot; &gt;&gt; nohup.out &nbsp; timeout $runtime nohup ${HBASE_HOME}/bin/hbase --config /home/stack/conf_hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --valueSize=110000 --size=$i --cycles=$cycles randomRead $clients done Java version: [stack@c2020 ~]$ java -version java version &quot;1.7.0_45&quot; Java(TM) SE Runtime Environment (build 1.7.0_45-b18) Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode) I enabled caching options by uncommenting these configuration properties in hbase-site.xml . The hfile.block.cache.size was set to .5 to keep math simple. hfile.block.cache.size 0.5 &nbsp; hbase.bucketcache.ioengine &nbsp; heap &nbsp; hbase.bucketcache.size &nbsp; 4196 Notes The CombinedBlockCache has some significant overhead (to be better quantified -- 10%?). There will be slop left over because buckets will not fill to the brim especially when block sizes vary.&nbsp; Observed running loadings.I tried to bring on a FullGC by running for more than a day with LruBlockCache fielding mostly BlockCache misses and I failed. Thinking on it, the BlockCache only occupied 4G of an 8G heap. There was always elbow room available (Free block count and maxium block size allocatable settled down to a nice number and held constant). TODO: Retry but with less elbow room available. Longer running CBC offheap test Some of the tests above show disturbing GC tendencies -- ever rising GC -- so I ran tests for a longer period .&nbsp; It turns out that BucketCache throws an OOME in long-running tests. You need HBASE-11678 BucketCache ramCache fills heap after running a few hours (fixed in hbase-0.98.6+)&lt;/p&gt; After the fix, the below test ran until the cluster was pul led out from under the loading: Here are some long running tests with bucket cache onheap (ioengine=heap). The throughput is less and GC is higher. .&lt;/span&gt; Future Does LruBlockCache get more erratic as heap size grows?&nbsp; Nick&#39;s post implies that it does.Auto-sizing of L1, onheap cache.HBASE-11331 [blockcache] lazy block decompression has a report attachedthat evaluates the current state of attached patch to keep blocks compressed while in the BlockCache.&nbsp; It finds that with the patch enabled, &quot;More GC. More CPU. Slower. Less throughput. But less i/o.&quot;, but there is an issue with compression block pooling that likely impinges on performance.Review serialization in and out of L2 cache. No serialization?&nbsp; Because we can take blocks from HDFS without copying onheap?" />
<link rel="canonical" href="http://localhost:4000/hbase/entry/comparing_blockcache_deploys" />
<meta property="og:url" content="http://localhost:4000/hbase/entry/comparing_blockcache_deploys" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2014-08-20T20:58:50-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Comparing BlockCache Deploys" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2014-08-20T20:58:50-04:00","datePublished":"2014-08-20T20:58:50-04:00","description":"Comparing BlockCache Deploys St.Ack on August 7th, 2014 A report comparing BlockCache deploys in Apache HBase for issue HBASE-11323 BucketCache all the time! Attempts to roughly equate five different deploys and compare how well they do under four different loading types that vary from no cache-misses through to missing cache most of the time. Makes recommendation on when to use which deploy. Prerequisite In Nick Dimiduk&#39;s BlockCache 101 blog post, he details the different options available in HBase. We test what remains after purging SlabCache and the caching policy implementation DoubleBlockCache which have been deprecated in HBase 0.98 and removed in trunk because of Nick&#39;s findings and that of others. Nick varies the JVM Heap+BlockCache sizes AND dataset size.&nbsp; This report keeps JVM Heap+BlockCache size constant and varies the dataset size only. Nick looks at the 99th percentile only.&nbsp; This article looks at that, as well as GC, throughput and loadings. Cell sizes in the following tests also vary between 1 byte and 256k in size. Findings If the dataset fits completely in cache, the default configuration, which uses the onheap LruBlockCache, performs best.&nbsp; GC is half that of the next most performant deploy type, CombinedBlockCache:Offheap with at least 20% more throughput.Otherwise, if your cache is experiencing churn running a steady stream of evictions, move your block cache offheap using CombinedBlockCache in the offheap mode. See the BlockCache section in the HBase Reference Guide for how to enable this deploy. Offheap mode requires only one third to one half of the GC of LruBlockCache when evictions are happening. There is also less variance as the cache misses climb when offheap is deployed. CombinedBlockCache:file mode has better GC profile but less throughput than CombinedBlockCache:Offheap. Latency is slightly higher on CBC:Offheap -- heap objects to cache have to be serialized in and out of the offheap memory -- than with the default LruBlockCache but the 95/99th percentiles are slightly better.&nbsp; CBC:Offheap uses a bit more CPU than LruBlockCache. CombinedBlockCache:Offheap is limited only by the amount of RAM available.&nbsp; It is not subject to GC. Test &lt;/p&gt; The graphs to follow show results from five different deploys each run through four different loadings. Five Deploy Variants LruBlockCache The default BlockCache in place when you start up an unconfigured HBase install. With LruBlockCache, all blocks are loaded into the java heap. See BlockCache 101 and LruBlockCache for detail on the caching policy of LruBlockCache. CombinedBlockCache:Offheap CombinedBlockCache deploys two tiers; an L1 which is an LruBlockCache instance to hold META blocks only (i.e. INDEX and BLOOM blocks), and an L2 tier which is an instance of BucketCache. In this offheap mode deploy, the BucketCache uses DirectByteBuffers to host a BlockCache outside of the JVM heap to host cached DATA blocks. CombinedBlockCache:Onheap In this onheap (&#39;heap&#39; mode), the L2 cache is hosted inside the JVM heap, and appears to the JVM to be a single large allocation. Internally it is managed by an instance of BucketCache The L1 cache is an instance of LruBlockCache. CombinedBlockCache:file In this mode, an L2 BucketCache instance puts DATA blocks into a file (hence &#39;file&#39; mode) on a mounted tmpfs in this case. CombinedBlockCache:METAonly No caching of DATA blocks (no L2 instance). DATA blocks are fetched every time. The INDEX blocks are loaded into an L1 LruBlockCache instance. Memory is fixed for each deploy. The java heap is a small 8G to bring on distress earlier. For deploy type 1., the LruBlockCache is given 4G of the 8G JVM heap. For 2.-5. deploy types., the L1 LruHeapCache is 0.1 * 8G (~800MB), which is more than enough to host the dataset META blocks. This was confirmed by inspecting the Block Cache vitals displayed in the RegionServer UI. For 2., 3., and 4. deploy types, the L2 bucket cache is 4G. Deploy type 5.&#39;s L2 is 0G (Used HBASE-11581 Add option so CombinedBlockCache L2 can be null (fscache)). Four Loading Types All cache hits all the time. A small percentage of cache misses, with all misses inside the fscache soon after the test starts. Lots of cache misses, all still inside the fscache soon after the test starts. Mostly cache misses where many misses by-pass the fscache. For each loading, we ran 25 clients reading randomly over 10M cells of zipfian varied cell sizes from 1 byte to 256k bytes over 21 regions hosted by a single RegionServer for 20 minutes. Clients would stay inside the cache size for loading 1., miss the cache at just under ~50% for loading 2., and so on. font-family: Times; font-size: medium; margin-bottom: 0in; line-height: 18.239999771118164px;&quot;&gt;The dataset was hosted on a single RegionServer on small HDFS cluster of 5 nodes.&nbsp; See below for detail on the setup. Graphs For each attribute -- GC, Throughput, i/o -- we have five charts across; one for each deploy.&nbsp; Each graph can be divided into four parts; the first quarter has the all-in-cache loading running for 20minutes, followed by the loading that has some cache misses (for twenty minutes), through to loading with mostly cache misses in the final quarter.To find out what each color represents, read the legend.&nbsp; The legend is small in the below. To see a larger version, browse to this non-apache-roller version of this document and click on the images over there.Concentrate on the first four graph types.&nbsp; The BlockCache Stats and I/O graphs add little other than confirming that the loadings ran the same across the different BlockCache deploys with fscache cutting in to soak up the seeks soon after start for all but the last mostly-cache-miss loading cases. GC Be sure to check the y-axis in the graphs below (you can click on chart to get a larger version). All profiles look basically the same but a check of the y-axis will show that for all but the no-cache-misses case, CombinedBlockCache:Offheap, the second deploy type, has the best GC profile (less GC is better!).The GC for the CombinedBLockCache:Offheap deploy mode looks to be climbing as the test runs.&nbsp; See the Notes section on the end for further comment. &lt;/p&gt; Throughput CombinedBlockCache:Offheap is better unless there are few to no cache misses.&nbsp; In that case, LruBlockCache shines (I missed why there is the step in the LruBlockCache all-in-cache section of the graph) Latency Latency when in-cache Same as above except that we do not show the last loading -- we show the first three only -- since the last loading of mostly cache misses skews the diagrams such that it is hard to compare latency when we are inside cache (BlockCache and fscache). Load Try aggregating the system and user CPUs when comparing. BlockCache Stats Curves are mostly the same except for the cache-no-DATA-blocks case. It has no L2 deployed. I/O Read profiles are about the same across all deploys and tests with a spike at first until the fscache comes around and covers. The exception is the final mostly-cache-misses case. Setup Master branch. 2.0.0-SNAPSHOT, r69039f8620f51444d9c62cfca9922baffe093610.&nbsp; Hadoop 2.4.1-SNAPSHOT.5 nodes all the same with 48G and six disks.&nbsp; One master and one regionserver, each on distinct nodes, with 21 regions of 10M rows of zipf varied size -- 0 to 256k -- created as follows: $ export HADOOP_CLASSPATH=/home/stack/conf_hbase:`./hbase-2.0.0-SNAPSHOT/bin/hbase classpath` $ nohup&nbsp; ./hadoop-2.4.1-SNAPSHOT/bin/hadoop --config /home/stack/conf_hadoop org.apache.hadoop.hbase.PerformanceEvaluation --valueSize=262144 --valueZipf --rows=100000 sequentialWrite 100 &amp; Here is my loading script.&nbsp; It performs 4 loadings: All in cache, just out of cache, a good bit out of cache and then mostly out of cache: [stack@c2020 ~]$ more bin/bc_in_mem.sh #!/bin/sh HOME=/home/stack testtype=$1 date=`date -u +&quot;%Y-%m-%dT%H:%M:%SZ&quot; echo testtype=$testtype $date` &gt;&gt; nohup.out HBASE_HOME=$HOME/hbase-2.0.0-SNAPSHOT runtime=1200 clients=25 cycles=1000000 #for i in 38 76 304 1000; do for i in 32 72 144 1000; do &nbsp; echo &quot;`date` run size=${i}, clients=$clients ; $testtype time=$runtime size=$i&quot; &gt;&gt; nohup.out &nbsp; timeout $runtime nohup ${HBASE_HOME}/bin/hbase --config /home/stack/conf_hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --valueSize=110000 --size=$i --cycles=$cycles randomRead $clients done Java version: [stack@c2020 ~]$ java -version java version &quot;1.7.0_45&quot; Java(TM) SE Runtime Environment (build 1.7.0_45-b18) Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode) I enabled caching options by uncommenting these configuration properties in hbase-site.xml . The hfile.block.cache.size was set to .5 to keep math simple. hfile.block.cache.size 0.5 &nbsp; hbase.bucketcache.ioengine &nbsp; heap &nbsp; hbase.bucketcache.size &nbsp; 4196 Notes The CombinedBlockCache has some significant overhead (to be better quantified -- 10%?). There will be slop left over because buckets will not fill to the brim especially when block sizes vary.&nbsp; Observed running loadings.I tried to bring on a FullGC by running for more than a day with LruBlockCache fielding mostly BlockCache misses and I failed. Thinking on it, the BlockCache only occupied 4G of an 8G heap. There was always elbow room available (Free block count and maxium block size allocatable settled down to a nice number and held constant). TODO: Retry but with less elbow room available. Longer running CBC offheap test Some of the tests above show disturbing GC tendencies -- ever rising GC -- so I ran tests for a longer period .&nbsp; It turns out that BucketCache throws an OOME in long-running tests. You need HBASE-11678 BucketCache ramCache fills heap after running a few hours (fixed in hbase-0.98.6+)&lt;/p&gt; After the fix, the below test ran until the cluster was pul led out from under the loading: Here are some long running tests with bucket cache onheap (ioengine=heap). The throughput is less and GC is higher. .&lt;/span&gt; Future Does LruBlockCache get more erratic as heap size grows?&nbsp; Nick&#39;s post implies that it does.Auto-sizing of L1, onheap cache.HBASE-11331 [blockcache] lazy block decompression has a report attachedthat evaluates the current state of attached patch to keep blocks compressed while in the BlockCache.&nbsp; It finds that with the patch enabled, &quot;More GC. More CPU. Slower. Less throughput. But less i/o.&quot;, but there is an issue with compression block pooling that likely impinges on performance.Review serialization in and out of L2 cache. No serialization?&nbsp; Because we can take blocks from HDFS without copying onheap?","headline":"Comparing BlockCache Deploys","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/hbase/entry/comparing_blockcache_deploys"},"url":"http://localhost:4000/hbase/entry/comparing_blockcache_deploys"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Comparing BlockCache Deploys</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2014-08-20T20:58:50-04:00" itemprop="datePublished">Aug 20, 2014
      </time>â€¢ <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Michael Stack", "login"=>"stack", "email"=>"stack@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 style="text-align: left; font-family: Times; letter-spacing: normal;">Comparing BlockCache Deploys</h1>
<p style="text-align: left; font-family: Times; font-size: medium;">St.Ack on August 7th, 2014</p>
<div style="font-family: Times; font-size: medium;"></div>
<p style="text-align: left; font-family: Times; font-size: medium; margin-bottom: 0in;">A report comparing BlockCache deploys in Apache HBase for issue <a href="https://issues.apache.org/jira/browse/HBASE-11323">HBASE-11323<span style="font-style: italic;"> BucketCache all the time!</span></a><span style="font-style: italic;"> </span>Attempts to roughly equate five different deploys and compare how well they do under four different loading types that vary from no cache-misses through to missing cache most of the time. Makes recommendation on when to use which deploy.</p>
<div style="font-family: Times; font-size: medium;"></div>
<h2 style="font-family: Times; letter-spacing: normal;">Prerequisite</h2>
<div style="font-family: Times; font-size: medium;"></div>
<p style="text-align: left; font-family: Times; font-size: medium; margin-bottom: 0in;">In Nick Dimiduk's <a href="http://www.n10k.com/blog/blockcache-101/">BlockCache 101</a> blog post, he details the different options available in HBase. We test what remains after purging SlabCache and the caching policy implementation DoubleBlockCache which have been deprecated in HBase 0.98 and removed in trunk because of Nick's findings and that of others.</p>
<p style="text-align: left; font-family: Times; font-size: medium; margin-bottom: 0in;">Nick varies the JVM Heap+BlockCache sizes AND dataset size.&nbsp; This report keeps JVM Heap+BlockCache size constant and varies the dataset size only. Nick looks at the 99th percentile only.&nbsp; This article looks at that, as well as GC, throughput and loadings. Cell sizes in the following tests also vary between 1 byte and 256k in size.</p>
<h1 style="text-align: left; font-family: Times; letter-spacing: normal;">Findings</h1>
<p><span style="font-family: Times; font-size: medium;">If the dataset fits completely in cache, the default configuration, which uses the onheap LruBlockCache, performs best.&nbsp; GC is half that of the next most performant deploy type, CombinedBlockCache:Offheap with at least 20% more throughput.</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /><span style="font-family: Times; font-size: medium;">Otherwise, if your cache is experiencing churn running a steady stream of evictions, move your block cache offheap using CombinedBlockCache in the </span><span style="font-family: Times; font-size: medium; font-style: italic;">offheap</span><span style="font-family: Times; font-size: medium;"> mode. See the </span><a href="http://hbase.apache.org/book.html#block.cache" style="font-family: Times; font-size: medium;">BlockCache</a><span style="font-family: Times; font-size: medium;"> section in the HBase Reference Guide for how to enable this deploy. Offheap mode requires only one third to one half of the GC of LruBlockCache when evictions are happening. There is also less variance as the cache misses climb when offheap is deployed. CombinedBlockCache:</span><span style="font-family: Times; font-size: medium; font-style: italic;">file</span><span style="font-family: Times; font-size: medium;"> mode has better GC profile but less throughput than CombinedBlockCache:</span><span style="font-family: Times; font-size: medium; font-style: italic;">Offheap</span><span style="font-family: Times; font-size: medium;">. Latency is slightly higher on CBC:</span><span style="font-family: Times; font-size: medium; font-style: italic;">Offheap</span><span style="font-family: Times; font-size: medium;"> -- heap objects to cache have to be serialized in and out of the offheap memory -- than with the default LruBlockCache but the 95/99th percentiles are slightly better.&nbsp; CBC:</span><span style="font-family: Times; font-size: medium; font-style: italic;">Offheap</span><span style="font-family: Times; font-size: medium;"> uses a bit more CPU than LruBlockCache. CombinedBlockCache:</span><span style="font-family: Times; font-size: medium; font-style: italic;">Offheap</span><span style="font-family: Times; font-size: medium;"> is limited only by the amount of RAM available.&nbsp; It is not subject to GC.</span></p>
<p><font face="times new roman, times, serif"> </font></p>
<div style="font-size: medium;">
<h1 style="text-align: left;"><font face="times new roman, times, serif">Test</font></h1>
</p></div>
<div style="font-size: medium;"></div>
<p style="text-align: left; font-size: medium; margin-bottom: 0in;">
<div style="font-size: medium;"></div>
<p><span style="font-family: Times; font-size: medium;">The graphs to follow show results from five different deploys each run through four different loadings.</span> </p>
<h2 style="font-family: Times; letter-spacing: normal;">Five Deploy Variants</h2>
<div style="font-family: Times; font-size: medium;"></div>
<div style="font-family: Times; font-size: medium;"></div>
<ol style="font-family: Times; font-size: medium;">
<li><a href="http://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/LruBlockCache.html" style="font-style: italic;">LruBlockCache</a><span style="font-style: italic;"> </span>The default BlockCache in place when you start up an unconfigured HBase install. With LruBlockCache, all blocks are loaded into the java heap. See <a href="http://www.n10k.com/blog/blockcache-101/">BlockCache 101</a> and <a href="http://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/LruBlockCache.html">LruBlockCache</a> for detail on the caching policy of LruBlockCache.</li>
<li><span style="font-style: italic;">CombinedBlockCache:Offheap</span> <a href="http://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CombinedBlockCache.html">CombinedBlockCache</a> deploys two tiers; an L1 which is an LruBlockCache instance to hold META blocks only (i.e. INDEX and BLOOM blocks), and an L2 tier which is an instance of <a href="http://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/bucket/BucketCache.html">BucketCache</a>. In this offheap mode deploy, the BucketCache uses DirectByteBuffers to host a BlockCache outside of the JVM heap to host cached DATA blocks. </li>
<li><span style="font-style: italic;">CombinedBlockCache:Onheap</span> In this onheap ('heap' mode), the L2 cache is hosted inside the JVM heap, and appears to the JVM to be a single large allocation. Internally it is managed by an instance of BucketCache The L1 cache is an instance of LruBlockCache.</li>
<li><span style="font-style: italic;">CombinedBlockCache:file</span> In this mode, an L2 BucketCache instance puts DATA blocks into a file (hence 'file' mode) on a mounted tmpfs in this case.</li>
<li><span style="font-style: italic;">CombinedBlockCache:METAonly</span> No caching of DATA blocks (no L2 instance). DATA blocks are fetched every time. The INDEX blocks are loaded into an L1 LruBlockCache instance.</li>
</ol>
<div style="font-family: Times; font-size: medium;"></div>
<p style="text-align: left; font-family: Times; font-size: medium; margin-bottom: 0in;">
<div style="font-family: Times; font-size: medium;"></div>
<p style="text-align: left; font-family: Times; font-size: medium; margin-bottom: 0in;">Memory is fixed for each deploy. The java heap is a small 8G to bring on distress earlier. For deploy type 1., the LruBlockCache is given 4G of the 8G JVM heap. For 2.-5. deploy types., the L1 LruHeapCache is 0.1 * 8G (~800MB), which is more than enough to host the dataset META blocks. This was confirmed by inspecting the Block Cache vitals displayed in the RegionServer UI. For 2., 3., and 4. deploy types, the L2 bucket cache is 4G. Deploy type 5.'s L2 is 0G (Used <a href="https://issues.apache.org/jira/browse/HBASE-11581">HBASE-11581</a> <span style="font-style: italic;">Add option so CombinedBlockCache L2 can be null (fscache)</span>).</p>
<div style="font-family: Times; font-size: medium;"></div>
<p style="text-align: left; font-family: Times; font-size: medium; margin-bottom: 0in;">
<div style="font-family: Times; font-size: medium;"></div>
<h2 style="font-family: Times; letter-spacing: normal;">Four Loading Types</h2>
<div style="font-family: Times; font-size: medium;"></div>
<div style="font-family: Times; font-size: medium;"></div>
<ol style="font-family: Times; font-size: medium;">
<li>All cache hits all the time.</li>
<li>A small percentage of cache misses, with all misses inside the fscache soon after the test starts.</li>
<li>Lots of cache misses, all still inside the fscache soon after the test starts.</li>
<li>Mostly cache misses where many misses by-pass the fscache.</li>
</ol>
<p style="text-align: left; font-family: Times; font-size: medium; margin-bottom: 0in; line-height: 18.239999771118164px;">For each loading, we ran 25 clients reading randomly over 10M cells of zipfian varied cell sizes from 1 byte to 256k bytes over 21 regions hosted by a single RegionServer for 20 minutes. Clients would stay inside the cache size for loading 1., miss the cache at just under ~50% for loading 2., and so on.</p>
<p style="text-align: left;"> font-family: Times; font-size: medium; margin-bottom: 0in; line-height: 18.239999771118164px;">The dataset was hosted on a single RegionServer on small HDFS cluster of 5 nodes.&nbsp; See below for detail on the <a href="http://people.apache.org/~stack/bc/#setup">setup</a>.</p>
<h1 style="text-align: left; font-family: Times; letter-spacing: normal;">Graphs</h1>
<p><span style="font-family: Times; font-size: medium;">For each attribute -- GC, Throughput, i/o -- we have five charts across; one for each deploy.&nbsp; Each graph can be divided into four parts; the first quarter has the all-in-cache loading running for 20minutes, followed by the loading that has some cache misses (for twenty minutes), through to loading with mostly cache misses in the final quarter.</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /><span style="font-family: Times; font-size: medium;">To find out what each color represents, read the legend.&nbsp; The legend is small in the below. To see a larger version, browse to this <a href="http://people.apache.org/~stack/bc/">non-apache-roller version of this document</a> and click on the images over there.</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /><span style="font-family: Times; font-size: medium;">Concentrate on the first four graph types.&nbsp; The BlockCache Stats and I/O graphs add little other than confirming that the loadings ran the same across the different BlockCache deploys with fscache cutting in to soak up the seeks soon after start for all but the last mostly-cache-miss loading cases.</span><br style="font-family: Times; font-size: medium;" /> </p>
<h2 style="font-family: Times; letter-spacing: normal;">GC</h2>
<p><span style="font-family: Times; font-size: medium;">Be sure to check the y-axis in the graphs below (you can click on chart to get a larger version). All profiles look basically the same but a check of the y-axis will show that for all but the no-cache-misses case, CombinedBlockCache:</span><span style="font-family: Times; font-size: medium; font-style: italic;">Offheap</span><span style="font-family: Times; font-size: medium;">, the second deploy type, has the best GC profile (less GC is better!).</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /><span style="font-family: Times; font-size: medium;">The GC for the CombinedBLockCache:</span><span style="font-family: Times; font-size: medium; font-style: italic;">Offheap</span><span style="font-family: Times; font-size: medium;"> deploy mode looks to be climbing as the test runs.&nbsp; See the </span><a href="http://people.apache.org/~stack/bc/#Notes" style="font-family: Times; font-size: medium;">Notes</a><span style="font-family: Times; font-size: medium;"> section on the end for further comment.</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /> </p>
<div style="font-family: Times; font-size: medium;"> <img usemap="#gc" src="http://people.apache.org/~stack/bc/all_files/gc.png" style="border: 0pt none;" alt="Image map" /><br />
<map id="gc" name="gc">
<area href="http://people.apache.org/~stack/bc/compare_lrubc_only/gc.png" shape="rect" coords="0,0,239,259" />
<area href="http://people.apache.org/~stack/bc/compare_cbc_offheap_4g_only/gc.png" shape="rect" coords="240,0,479,259" />
<area href="http://people.apache.org/~stack/bc/compare_cbc_onheap/gc.png" shape="rect" coords="480,0,719,259" />
<area href="http://people.apache.org/~stack/bc/compare_cbc_file/gc.png" shape="rect" coords="720,0,959,259" />
<area href="http://people.apache.org/~stack/bc/compare_nocache_data_blocks/gc.png" shape="rect" coords="960,0,1199,259" /> </map>
</p></div>
<h2 style="font-family: Times; letter-spacing: normal;">Throughput</h2>
<p><span style="font-family: Times; font-size: medium;">CombinedBlockCache:</span><span style="font-family: Times; font-size: medium; font-style: italic;">Offheap</span><span style="font-family: Times; font-size: medium;"> is better unless there are few to no cache misses.&nbsp; In that case, LruBlockCache shines (I missed why there is the step in the LruBlockCache all-in-cache section of the graph)</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /> </p>
<div style="font-family: Times; font-size: medium;"> <img usemap="#throughput" src="http://people.apache.org/~stack/bc/all_files/gets.png" alt="Image map" style="border-width: 0pt; border-bottom-style: none;" /><br />
<map id="throughput" name="throughput"></map>
</div>
<h2 style="font-family: Times; letter-spacing: normal;">Latency</h2>
<div style="font-family: Times; font-size: medium;"><img usemap="#latency" src="http://people.apache.org/~stack/bc/all_files/latency.png" alt="Image map" style="border-width: 0pt; border-bottom-style: none;" /><br />
<map id="latency" name="latency"></map>
</div>
<h3 style="font-family: Times; letter-spacing: normal;">Latency when in-cache</h3>
<p><span style="font-family: Times; font-size: medium;">Same as above except that we do not show the last loading -- we show the first three only -- since the last loading of mostly cache misses skews the diagrams such that it is hard to compare latency when we are inside cache (BlockCache and fscache).</span> </p>
<div style="font-family: Times; font-size: medium;"><img usemap="#latency_upclose" src="http://people.apache.org/~stack/bc/all_files/latency_upclose.png" alt="Image map" style="border-width: 0pt; border-bottom-style: none;" /><br />
<map id="latency_upclose" name="latency_upclose"></map>
</div>
<h2 style="font-family: Times; letter-spacing: normal;">Load</h2>
<p><span style="font-family: Times; font-size: medium;">Try aggregating the system and user CPUs when comparing.</span><br style="font-family: Times; font-size: medium;" /> </p>
<div style="font-family: Times; font-size: medium;"><img usemap="#load" src="http://people.apache.org/~stack/bc/all_files/load.png" alt="Image map" style="border-width: 0pt; border-bottom-style: none;" /><br />
<map id="load" name="load"></map>
</div>
<h2 style="font-family: Times; letter-spacing: normal;">BlockCache Stats</h2>
<p><span style="font-family: Times; font-size: medium;">Curves are mostly the same except for the cache-no-DATA-blocks case. It has no L2 deployed.</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /> </p>
<div style="font-family: Times; font-size: medium;"><img usemap="#bc" src="http://people.apache.org/~stack/bc/all_files/bc.png" alt="Image map" style="border-width: 0pt; border-bottom-style: none;" /><br />
<map id="bc" name="bc"></map>
</div>
<h2 style="font-family: Times; letter-spacing: normal;">I/O</h2>
<p><span style="font-family: Times; font-size: medium;">Read profiles are about the same across all deploys and tests with a spike at first until the fscache comes around and covers. The exception is the final mostly-cache-misses case.</span><br style="font-family: Times; font-size: medium;" /> </p>
<div style="font-family: Times; font-size: medium;"><img usemap="#io" src="http://people.apache.org/~stack/bc/all_files/io.png" alt="Image map" style="border-width: 0pt; border-bottom-style: none;" /><br />
<map id="io" name="io"></map>
</div>
<h1 style="text-align: left; font-family: Times; letter-spacing: normal;"><a name="setup"></a>Setup</h1>
<p><span style="font-family: Times; font-size: medium;">Master branch. 2.0.0-SNAPSHOT, r69039f8620f51444d9c62cfca9922baffe093610.&nbsp; Hadoop 2.4.1-SNAPSHOT.</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /><span style="font-family: Times; font-size: medium;">5 nodes all the same with 48G and six disks.&nbsp; One master and one regionserver, each on distinct nodes, with 21 regions of 10M rows of zipf varied size -- 0 to 256k -- created as follows:</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /> </p>
<pre style="margin-left: 40px;">$ export HADOOP_CLASSPATH=/home/stack/conf_hbase:`./hbase-2.0.0-SNAPSHOT/bin/hbase classpath`
$ nohup&nbsp; ./hadoop-2.4.1-SNAPSHOT/bin/hadoop --config /home/stack/conf_hadoop org.apache.hadoop.hbase.PerformanceEvaluation --valueSize=262144 --valueZipf --rows=100000 sequentialWrite 100 &amp;</pre>
<p><br style="font-family: Times; font-size: medium;" /><span style="font-family: Times; font-size: medium;">Here is my loading script.&nbsp; It performs 4 loadings: All in cache, just out of cache, a good bit out of cache and then mostly out of cache:</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /> </p>
<pre style="margin-left: 40px;">[stack@c2020 ~]$ more bin/bc_in_mem.sh
#!/bin/sh
HOME=/home/stack
testtype=$1
date=`date -u +"%Y-%m-%dT%H:%M:%SZ"
echo testtype=$testtype $date` >> nohup.out
HBASE_HOME=$HOME/hbase-2.0.0-SNAPSHOT
runtime=1200
clients=25
cycles=1000000
#for i in 38 76 304 1000; do
for i in 32 72 144 1000; do
&nbsp; echo "`date` run size=${i}, clients=$clients ; $testtype time=$runtime size=$i" >> nohup.out
&nbsp; timeout $runtime nohup ${HBASE_HOME}/bin/hbase --config /home/stack/conf_hbase org.apache.hadoop.hbase.PerformanceEvaluation --nomapred --valueSize=110000 --size=$i --cycles=$cycles randomRead $clients
done
</pre>
<map id="io" name="io" style="font-family: Times; font-size: medium;"></map>
<p><span style="font-family: Times; font-size: medium;">Java version:</span><br style="font-family: Times; font-size: medium;" /> </p>
<pre style="margin-left: 40px;">[stack@c2020 ~]$ java -version
java version "1.7.0_45"
Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)</pre>
<p><br style="font-family: Times; font-size: medium;" /><span style="font-family: Times; font-size: medium;">I enabled caching options by uncommenting these configuration properties in hbase-site.xml . The hfile.block.cache.size was set to .5 to keep math simple.</span><br style="font-family: Times; font-size: medium;" /> </p>
<pre style="margin-left: 40px;"><!--LRU Cache-->
<property>
<name>hfile.block.cache.size</name>
<value>0.5</value>
</property>

<!--Bucket cache-->
<property>
&nbsp; <name>hbase.bucketcache.ioengine</name>
&nbsp; <value>heap</value>
</property>
<property>
&nbsp; <name>hbase.bucketcache.size</name>
&nbsp; <value>4196</value>
</property>

</pre>
<h1 style="text-align: left; font-family: Times; letter-spacing: normal;"><a name="Notes"></a>Notes</h1>
<p><span style="font-family: Times; font-size: medium;">The CombinedBlockCache has some significant overhead (to be better quantified -- 10%?). There will be slop left over because buckets will not fill to the brim especially when block sizes vary.&nbsp; Observed running loadings.</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /><span style="font-family: Times; font-size: medium;">I tried to bring on a FullGC by running for more than a day with LruBlockCache fielding mostly BlockCache misses and I failed. Thinking on it, the BlockCache only occupied 4G of an 8G heap. There was always elbow room available (Free block count and maxium block size allocatable settled down to a nice number and held constant). TODO: Retry but with less elbow room available.</span><br style="font-family: Times; font-size: medium;" /> </p>
<h3 style="font-family: Times; letter-spacing: normal;">Longer running CBC offheap test</h3>
<p><span style="font-family: Times; font-size: medium;">Some of the tests above show disturbing GC tendencies -- ever rising GC -- so I ran tests for a longer period .&nbsp; It turns out that BucketCache throws an OOME in long-running tests. You need </span><a href="https://issues.apache.org/jira/browse/HBASE-11678" style="font-family: Times; font-size: medium;">HBASE-11678 BucketCache ramCache fills heap after running a few hours</a><span style="font-family: Times; font-size: medium;"> (fixed in hbase-0.98.6+)</p>
<p>After the fix, the below test ran until the cluster was pul    led out from under the loading:<br /> <img style="width: 512px; height: 384px;" alt="gc over three days" src="http://people.apache.org/~stack/bc/all_files/gc_3days.png" /><img style="width: 512px; height: 384px;" alt="Thro    ughput over three days" src="http://people.apache.org/~stack/bc/all_files/throughput_3days.png" /><br />
Here are some long running tests with bucket cache onheap (ioengine=heap). The throughput is less and GC is higher.<br />
<br /> <img style="width: 512px; height: 384px;" alt="gc over three days onheap" src="http://people.apache.org/~stack/bc/all_files/gc_3days_onheap.png" /><img style="width: 512px;" alt="Throughput over three days onheap" src="http://people.apache.org/~stack/bc/all_files/throughput_3days_onheap.png" /></p>
<p>.</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /> </p>
<h1 style="text-align: left; font-family: Times; letter-spacing: normal;">Future</h1>
<p><span style="font-family: Times; font-size: medium;">Does LruBlockCache get more erratic as heap size grows?&nbsp; Nick's post implies that it does.</span><br style="font-family: Times; font-size: medium;" /><span style="font-family: Times; font-size: medium;">Auto-sizing of L1, onheap cache.</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /><a href="https://issues.apache.org/jira/browse/HBASE-11331" style="font-family: Times; font-size: medium;">HBASE-11331 [blockcache] lazy block decompression</a><span style="font-family: Times; font-size: medium;"> has a report </span><a href="https://issues.apache.org/jira/secure/attachment/12654142/HBASE-11331LazyBlockDecompressperfcompare.pdf" style="font-family: Times; font-size: medium;">attached</a><span style="font-family: Times; font-size: medium;">that evaluates the current state of attached patch to keep blocks compressed while in the BlockCache.&nbsp; It finds that with the patch enabled, "More GC. More CPU. Slower. Less throughput. But less i/o.", but there is an issue with compression block pooling that likely impinges on performance.</span><br style="font-family: Times; font-size: medium;" /><br style="font-family: Times; font-size: medium;" /><span style="font-family: Times; font-size: medium;">Review serialization in and out of L2 cache. No serialization?&nbsp; Because we can take blocks from HDFS without copying onheap?</span><br />
<style type="text/css">
        <!--<br />
                h1 { color: #000000; text-align: center }<br />
                p { color: #000000; text-align: center }<br />
        --><br />
        </style>

  </div><a class="u-url" href="/hbase/entry/comparing_blockcache_deploys" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
