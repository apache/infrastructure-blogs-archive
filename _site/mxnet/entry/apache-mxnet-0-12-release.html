<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Apache MXNet 0.12 Release Adds Support for New NVIDIA Volta GPUs and Sparse Tensor | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Apache MXNet 0.12 Release Adds Support for New NVIDIA Volta GPUs and Sparse Tensor" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We are excited about the availability of Apache MXNet version 0.12. With this release, MXNet adds two new important features: support for NVIDIA Volta GPUs and support for Sparse Tensors Support for NVIDIA Volta GPU Architecture The MXNet v0.12 release adds support for NVIDIA Volta V100 GPUs, enabling users to train convolutional neural networks up to 3.5 times faster than on the Pascal GPUs. Trillions of floating-point (FP) multiplications and additions for training a neural network have typically been done using single precision (FP32) to achieve high accuracy. However, recent research has shown that the same accuracy can be achieved using half-precision (FP16) data types. The Volta GPU architecture introduces Tensor Cores. Each Tensor Core can execute 64 fuse-multiply-add ops per clock, which roughly quadruples the CUDA core FLOPS per clock per core. Each Tensor Core performs D = A x B + C, where A and B are half-precision matrices, while C and D can be either half or single-precision matrices, thereby performing mixed precision training. The new mixed-precision training allows users to achieve optimal training performance without sacrificing accuracy by using FP16 for most of the layers of a network, and higher precision data types only when necessary. You can take advantage of Volta Tensor Cores to enable FP16 training in MXNet by passing a simple command, &quot;--dtype float16&quot; to the MXNet training script. For example, you can invoke imagenet training script with command: train_imagenet.py --dtype float16 Sparse Tensor Support MXNet v0.12 adds support for sparse tensors to efficiently store and compute tensors allowing developers to perform sparse matrix operations in a storage and compute-efficient manner and train deep learning models faster. MXNet v0.12 supports two major sparse data formats: Compressed Sparse Row (CSR) and Row Sparse (RSP). The CSR format is optimized to represent matrices with a large number of columns where each row has only a few non-zero elements. The RSP format is optimized to represent matrices with a huge number of rows where most of the row slices are complete zeros. For example, the CSR format can be used to encode the feature vectors of input data for a recommendation engine, whereas the RSP format can be used to perform the sparse gradient updates during training. This release enables sparse support on CPU for most commonly used operators such as matrix dot product and element-wise operators. Sparse support for more operators will be added in future releases. Follow these tutorials to learn how to use the new sparse operators in MXNet. Get Apache MXNet 0.12 from downloads page . Read more about this release in Release Notes . Or, You can download and play with MXNet easily using one of the options below: The Pip package can be found here: https://pypi.python.org/pypi/mxnet The Docker Images can be found here: https://hub.docker.com/u/mxnet/ &lt;/p&gt; If you want to learn more about MXNet visit https://mxnet.incubator.apache.org/. Finally, you are welcome to join and also invite your friends to the dynamic and growing MXNet community by subscribing to dev@mxnet.incubator.apache.org" />
<meta property="og:description" content="We are excited about the availability of Apache MXNet version 0.12. With this release, MXNet adds two new important features: support for NVIDIA Volta GPUs and support for Sparse Tensors Support for NVIDIA Volta GPU Architecture The MXNet v0.12 release adds support for NVIDIA Volta V100 GPUs, enabling users to train convolutional neural networks up to 3.5 times faster than on the Pascal GPUs. Trillions of floating-point (FP) multiplications and additions for training a neural network have typically been done using single precision (FP32) to achieve high accuracy. However, recent research has shown that the same accuracy can be achieved using half-precision (FP16) data types. The Volta GPU architecture introduces Tensor Cores. Each Tensor Core can execute 64 fuse-multiply-add ops per clock, which roughly quadruples the CUDA core FLOPS per clock per core. Each Tensor Core performs D = A x B + C, where A and B are half-precision matrices, while C and D can be either half or single-precision matrices, thereby performing mixed precision training. The new mixed-precision training allows users to achieve optimal training performance without sacrificing accuracy by using FP16 for most of the layers of a network, and higher precision data types only when necessary. You can take advantage of Volta Tensor Cores to enable FP16 training in MXNet by passing a simple command, &quot;--dtype float16&quot; to the MXNet training script. For example, you can invoke imagenet training script with command: train_imagenet.py --dtype float16 Sparse Tensor Support MXNet v0.12 adds support for sparse tensors to efficiently store and compute tensors allowing developers to perform sparse matrix operations in a storage and compute-efficient manner and train deep learning models faster. MXNet v0.12 supports two major sparse data formats: Compressed Sparse Row (CSR) and Row Sparse (RSP). The CSR format is optimized to represent matrices with a large number of columns where each row has only a few non-zero elements. The RSP format is optimized to represent matrices with a huge number of rows where most of the row slices are complete zeros. For example, the CSR format can be used to encode the feature vectors of input data for a recommendation engine, whereas the RSP format can be used to perform the sparse gradient updates during training. This release enables sparse support on CPU for most commonly used operators such as matrix dot product and element-wise operators. Sparse support for more operators will be added in future releases. Follow these tutorials to learn how to use the new sparse operators in MXNet. Get Apache MXNet 0.12 from downloads page . Read more about this release in Release Notes . Or, You can download and play with MXNet easily using one of the options below: The Pip package can be found here: https://pypi.python.org/pypi/mxnet The Docker Images can be found here: https://hub.docker.com/u/mxnet/ &lt;/p&gt; If you want to learn more about MXNet visit https://mxnet.incubator.apache.org/. Finally, you are welcome to join and also invite your friends to the dynamic and growing MXNet community by subscribing to dev@mxnet.incubator.apache.org" />
<link rel="canonical" href="http://localhost:4000/mxnet/entry/apache-mxnet-0-12-release" />
<meta property="og:url" content="http://localhost:4000/mxnet/entry/apache-mxnet-0-12-release" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-11-01T20:11:04-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Apache MXNet 0.12 Release Adds Support for New NVIDIA Volta GPUs and Sparse Tensor" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2017-11-01T20:11:04-04:00","datePublished":"2017-11-01T20:11:04-04:00","description":"We are excited about the availability of Apache MXNet version 0.12. With this release, MXNet adds two new important features: support for NVIDIA Volta GPUs and support for Sparse Tensors Support for NVIDIA Volta GPU Architecture The MXNet v0.12 release adds support for NVIDIA Volta V100 GPUs, enabling users to train convolutional neural networks up to 3.5 times faster than on the Pascal GPUs. Trillions of floating-point (FP) multiplications and additions for training a neural network have typically been done using single precision (FP32) to achieve high accuracy. However, recent research has shown that the same accuracy can be achieved using half-precision (FP16) data types. The Volta GPU architecture introduces Tensor Cores. Each Tensor Core can execute 64 fuse-multiply-add ops per clock, which roughly quadruples the CUDA core FLOPS per clock per core. Each Tensor Core performs D = A x B + C, where A and B are half-precision matrices, while C and D can be either half or single-precision matrices, thereby performing mixed precision training. The new mixed-precision training allows users to achieve optimal training performance without sacrificing accuracy by using FP16 for most of the layers of a network, and higher precision data types only when necessary. You can take advantage of Volta Tensor Cores to enable FP16 training in MXNet by passing a simple command, &quot;--dtype float16&quot; to the MXNet training script. For example, you can invoke imagenet training script with command: train_imagenet.py --dtype float16 Sparse Tensor Support MXNet v0.12 adds support for sparse tensors to efficiently store and compute tensors allowing developers to perform sparse matrix operations in a storage and compute-efficient manner and train deep learning models faster. MXNet v0.12 supports two major sparse data formats: Compressed Sparse Row (CSR) and Row Sparse (RSP). The CSR format is optimized to represent matrices with a large number of columns where each row has only a few non-zero elements. The RSP format is optimized to represent matrices with a huge number of rows where most of the row slices are complete zeros. For example, the CSR format can be used to encode the feature vectors of input data for a recommendation engine, whereas the RSP format can be used to perform the sparse gradient updates during training. This release enables sparse support on CPU for most commonly used operators such as matrix dot product and element-wise operators. Sparse support for more operators will be added in future releases. Follow these tutorials to learn how to use the new sparse operators in MXNet. Get Apache MXNet 0.12 from downloads page . Read more about this release in Release Notes . Or, You can download and play with MXNet easily using one of the options below: The Pip package can be found here: https://pypi.python.org/pypi/mxnet The Docker Images can be found here: https://hub.docker.com/u/mxnet/ &lt;/p&gt; If you want to learn more about MXNet visit https://mxnet.incubator.apache.org/. Finally, you are welcome to join and also invite your friends to the dynamic and growing MXNet community by subscribing to dev@mxnet.incubator.apache.org","headline":"Apache MXNet 0.12 Release Adds Support for New NVIDIA Volta GPUs and Sparse Tensor","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/mxnet/entry/apache-mxnet-0-12-release"},"url":"http://localhost:4000/mxnet/entry/apache-mxnet-0-12-release"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Apache MXNet 0.12 Release Adds Support for New NVIDIA Volta GPUs and Sparse Tensor</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2017-11-01T20:11:04-04:00" itemprop="datePublished">Nov 1, 2017
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Sandeep Krishnamurthy", "login"=>"skm", "email"=>"sandeep.krishna98@gmail.com"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>
We are excited about the availability of Apache MXNet version 0.12. With this release, MXNet adds two new important features: support for NVIDIA Volta GPUs and support for Sparse Tensors<br />
<br/></p>
<p><b> Support for NVIDIA Volta GPU Architecture </b></p>
<p>
The MXNet v0.12 release adds support for NVIDIA Volta V100 GPUs, enabling users to train convolutional neural networks up to 3.5 times faster than on the Pascal GPUs. Trillions of floating-point (FP) multiplications and additions for training a neural network have typically been done using single precision (FP32) to achieve high accuracy. However, recent research has shown that the same accuracy can be achieved using half-precision (FP16) data types.</p>
<p>
The Volta GPU architecture introduces Tensor Cores. Each Tensor Core can execute 64 fuse-multiply-add ops per clock, which roughly quadruples the CUDA core FLOPS per clock per core. Each Tensor Core performs D = A x B + C, where A and B are half-precision matrices, while C and D can be either half or single-precision matrices, thereby performing mixed precision training. The new mixed-precision training allows users to achieve optimal training performance without sacrificing accuracy by using FP16 for most of the layers of a network, and higher precision data types only when necessary.</p>
<p>
You can take advantage of Volta Tensor Cores to enable FP16 training in MXNet by passing a simple command, "--dtype float16" to the MXNet training script. For example, you can invoke <a href="https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/train_imagenet.py"> imagenet training script </a> with command:<br />
 train_imagenet.py --dtype float16<br />
<br/></p>
<p><b> Sparse Tensor Support </b></p>
<p>
MXNet v0.12 adds support for sparse tensors to efficiently store and compute tensors allowing developers to perform sparse matrix operations in a storage and compute-efficient manner and train deep learning models faster. MXNet v0.12 supports two major sparse data formats: Compressed Sparse Row (CSR) and Row Sparse (RSP). The CSR format is optimized to represent matrices with a large number of columns where each row has only a few non-zero elements. The RSP format is optimized to represent matrices with a huge number of rows where most of the row slices are complete zeros. For example, the CSR format can be used to encode the feature vectors of input data for a recommendation engine, whereas the RSP format can be used to perform the sparse gradient updates during training. This release enables sparse support on CPU for most commonly used operators such as matrix dot product and element-wise operators. Sparse support for more operators will be added in future releases.</p>
<p>
Follow <a href="https://mxnet.incubator.apache.org/versions/master/tutorials/#sparse-ndarray">these tutorials</a> to learn how to use the new sparse operators in MXNet.</p>
<p>
Get Apache MXNet 0.12 from <a href="http://www.apache.org/dist/incubator/mxnet/"> downloads page </a>. Read more about this release in <a href="https://github.com/apache/incubator-mxnet/releases/tag/0.12.0"> Release Notes </a>.</p>
<p>
Or, You can download and play with MXNet easily using one of the options below:</p>
<ul>
<li>
The Pip package can be found here: <a href="https://pypi.python.org/pypi/mxnet">https://pypi.python.org/pypi/mxnet</a>
</li>
<li>
The Docker Images can be found here: <a href="https://hub.docker.com/u/mxnet/">https://hub.docker.com/u/mxnet/ </a>
</li></p>
<p>
If you want to learn more about MXNet visit <a href="https://mxnet.incubator.apache.org/">https://mxnet.incubator.apache.org/</a>.<br />
Finally, you are welcome to join and also invite your friends to the dynamic and growing MXNet community by subscribing to dev@mxnet.incubator.apache.org</p>

  </div><a class="u-url" href="/mxnet/entry/apache-mxnet-0-12-release" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
