<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Spark Integration in Apache Phoenix | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Spark Integration in Apache Phoenix" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Today&#39;s blog is brought to you by our latest committer and the developer behind the Spark integration in Apache Phoenix,&nbsp;Josh Mahonin, a Software Architect at Interset. PageRank with Phoenix and Spark The Phoenix SQL interface provides a lot of great analytics capabilities on top of structured HBase data. Some tasks however, such as machine learning or graph analysis, are more efficiently done using other tools like Apache Spark. Since the Phoenix 4.4.0 release, the phoenix-spark module allows us to expose Phoenix tables as RDDs or DataFrames within Spark. From there, that same data can be used with other tools within Spark, such as the machine learning library MLlib, the graph engine GraphX, or Spark Streaming. This example makes use of the Enron email test set from Stanford Network Analysis Project, and executes the GraphX implementation of PageRank on it to find interesting entities. It then saves the results back to Phoenix. Note that runnable source code is also available on Github &lt;/p&gt; Prerequisites Phoenix 4.4.0+ Spark 1.3.0+ (ensure phoenix-client JAR is in the Spark driver classpath, see setup guide ) Load sample data Login to a node with the Apache Phoenix binaries available. I will use localhost to refer to the Phoenix URL, but you may need to adjust to your local environment cd /path/to/phoenix/bin ./sqlline.py localhost Once in the SQLLine console, we&#39;ll create the tables to hold the input data, and the destination table for the pagerank results CREATE TABLE EMAIL_ENRON( MAIL_FROM BIGINT NOT NULL, MAIL_TO BIGINT NOT NULL CONSTRAINT pk PRIMARY KEY(MAIL_FROM, MAIL_TO)); CREATE TABLE EMAIL_ENRON_PAGERANK( ID BIGINT NOT NULL, RANK DOUBLE CONSTRAINT pk PRIMARY KEY(ID)); Use &#39;ctrl+d&#39; to exit SQLline Download and extract the file enron.csv.gz to a local directory, such as /tmp. We&#39;ll use &#39;psql.py&#39; to load the CSV data gunzip /tmp/enron.csv.gz ./psql.py -t EMAIL_ENRON localhost /tmp/enron.csv When finished, you should see the output: CSV Upsert complete. 367662 rows upserted Interactive analysis with spark-shell Login to a node with Spark installed. Note that the phoenix-client JAR must be available in the Spark driver classpath cd /path/to/spark/bin ./spark-shell Once you&#39;re in the spark shell, you can type, or copy the code below into the interactive shell &lt;/p&gt; import org.apache.spark.graphx._ import org.apache.phoenix.spark._ // Load the phoenix table val rdd = sc.phoenixTableAsRDD(&quot;EMAIL_ENRON&quot;, Seq(&quot;MAIL_FROM&quot;, &quot;MAIL_TO&quot;), zkUrl=Some(&quot;localhost&quot;)) // Convert to an RDD of VertexId tuples val rawEdges = rdd.map{ e =&gt; (e(&quot;MAIL_FROM&quot;).asInstanceOf[VertexId], e(&quot;MAIL_TO&quot;).asInstanceOf[VertexId]) } // Create a graph with default edge weights val graph = Graph.fromEdgeTuples(rawEdges, 1.0) // Run page rank val pr = graph.pageRank(0.001) // Save to Phoenix pr.vertices.saveToPhoenix(&quot;EMAIL_ENRON_PAGERANK&quot;, Seq(&quot;ID&quot;, &quot;RANK&quot;), zkUrl = Some(&quot;localhost&quot;)) Once finished, you can exit spark-shell with &#39;ctrl+d&#39; Results On your Phoenix node, open sqlline again &lt;/p&gt; cd /path/to/phoenix/bin ./sqlline.py localhost Let&#39;s run a query that will give us the top-ranked entities from the PageRank results SELECT * FROM EMAIL_ENRON_PAGERANK ORDER BY RANK DESC LIMIT 5; +------------------------------------------+------------------------------------------+ | ID | RANK | +------------------------------------------+------------------------------------------+ | 5038 | 497.2989872977676 | | 273 | 117.18141799210386 | | 140 | 108.63091596789913 | | 458 | 107.2728800448782 | | 588 | 106.11840798585399 | +------------------------------------------+------------------------------------------+ Although this data-set has the email addresses removed, if you&#39;re curious, you can find results of a similar analysis here. If you&#39;re familiar with the Enron case, some of those names will ring a bell. &lt;/p&gt; Conclusion Although this example is fairly trivial, it shows the capabilities, as well as succinctness, of using Phoenix and Spark together to run complex algorithms across arbitrarily large datasets. In my experience, the methods shown here extend quite well to other &quot;big data&quot; problems such as community detection and clustering, as well as anomaly detection. There are likely many other problem domains which are applicable as well Thanks for reading!" />
<meta property="og:description" content="Today&#39;s blog is brought to you by our latest committer and the developer behind the Spark integration in Apache Phoenix,&nbsp;Josh Mahonin, a Software Architect at Interset. PageRank with Phoenix and Spark The Phoenix SQL interface provides a lot of great analytics capabilities on top of structured HBase data. Some tasks however, such as machine learning or graph analysis, are more efficiently done using other tools like Apache Spark. Since the Phoenix 4.4.0 release, the phoenix-spark module allows us to expose Phoenix tables as RDDs or DataFrames within Spark. From there, that same data can be used with other tools within Spark, such as the machine learning library MLlib, the graph engine GraphX, or Spark Streaming. This example makes use of the Enron email test set from Stanford Network Analysis Project, and executes the GraphX implementation of PageRank on it to find interesting entities. It then saves the results back to Phoenix. Note that runnable source code is also available on Github &lt;/p&gt; Prerequisites Phoenix 4.4.0+ Spark 1.3.0+ (ensure phoenix-client JAR is in the Spark driver classpath, see setup guide ) Load sample data Login to a node with the Apache Phoenix binaries available. I will use localhost to refer to the Phoenix URL, but you may need to adjust to your local environment cd /path/to/phoenix/bin ./sqlline.py localhost Once in the SQLLine console, we&#39;ll create the tables to hold the input data, and the destination table for the pagerank results CREATE TABLE EMAIL_ENRON( MAIL_FROM BIGINT NOT NULL, MAIL_TO BIGINT NOT NULL CONSTRAINT pk PRIMARY KEY(MAIL_FROM, MAIL_TO)); CREATE TABLE EMAIL_ENRON_PAGERANK( ID BIGINT NOT NULL, RANK DOUBLE CONSTRAINT pk PRIMARY KEY(ID)); Use &#39;ctrl+d&#39; to exit SQLline Download and extract the file enron.csv.gz to a local directory, such as /tmp. We&#39;ll use &#39;psql.py&#39; to load the CSV data gunzip /tmp/enron.csv.gz ./psql.py -t EMAIL_ENRON localhost /tmp/enron.csv When finished, you should see the output: CSV Upsert complete. 367662 rows upserted Interactive analysis with spark-shell Login to a node with Spark installed. Note that the phoenix-client JAR must be available in the Spark driver classpath cd /path/to/spark/bin ./spark-shell Once you&#39;re in the spark shell, you can type, or copy the code below into the interactive shell &lt;/p&gt; import org.apache.spark.graphx._ import org.apache.phoenix.spark._ // Load the phoenix table val rdd = sc.phoenixTableAsRDD(&quot;EMAIL_ENRON&quot;, Seq(&quot;MAIL_FROM&quot;, &quot;MAIL_TO&quot;), zkUrl=Some(&quot;localhost&quot;)) // Convert to an RDD of VertexId tuples val rawEdges = rdd.map{ e =&gt; (e(&quot;MAIL_FROM&quot;).asInstanceOf[VertexId], e(&quot;MAIL_TO&quot;).asInstanceOf[VertexId]) } // Create a graph with default edge weights val graph = Graph.fromEdgeTuples(rawEdges, 1.0) // Run page rank val pr = graph.pageRank(0.001) // Save to Phoenix pr.vertices.saveToPhoenix(&quot;EMAIL_ENRON_PAGERANK&quot;, Seq(&quot;ID&quot;, &quot;RANK&quot;), zkUrl = Some(&quot;localhost&quot;)) Once finished, you can exit spark-shell with &#39;ctrl+d&#39; Results On your Phoenix node, open sqlline again &lt;/p&gt; cd /path/to/phoenix/bin ./sqlline.py localhost Let&#39;s run a query that will give us the top-ranked entities from the PageRank results SELECT * FROM EMAIL_ENRON_PAGERANK ORDER BY RANK DESC LIMIT 5; +------------------------------------------+------------------------------------------+ | ID | RANK | +------------------------------------------+------------------------------------------+ | 5038 | 497.2989872977676 | | 273 | 117.18141799210386 | | 140 | 108.63091596789913 | | 458 | 107.2728800448782 | | 588 | 106.11840798585399 | +------------------------------------------+------------------------------------------+ Although this data-set has the email addresses removed, if you&#39;re curious, you can find results of a similar analysis here. If you&#39;re familiar with the Enron case, some of those names will ring a bell. &lt;/p&gt; Conclusion Although this example is fairly trivial, it shows the capabilities, as well as succinctness, of using Phoenix and Spark together to run complex algorithms across arbitrarily large datasets. In my experience, the methods shown here extend quite well to other &quot;big data&quot; problems such as community detection and clustering, as well as anomaly detection. There are likely many other problem domains which are applicable as well Thanks for reading!" />
<link rel="canonical" href="http://localhost:4000/phoenix/entry/spark_integration_in_apache_phoenix" />
<meta property="og:url" content="http://localhost:4000/phoenix/entry/spark_integration_in_apache_phoenix" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2015-06-29T18:38:48-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Spark Integration in Apache Phoenix" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2015-06-29T18:38:48-04:00","datePublished":"2015-06-29T18:38:48-04:00","description":"Today&#39;s blog is brought to you by our latest committer and the developer behind the Spark integration in Apache Phoenix,&nbsp;Josh Mahonin, a Software Architect at Interset. PageRank with Phoenix and Spark The Phoenix SQL interface provides a lot of great analytics capabilities on top of structured HBase data. Some tasks however, such as machine learning or graph analysis, are more efficiently done using other tools like Apache Spark. Since the Phoenix 4.4.0 release, the phoenix-spark module allows us to expose Phoenix tables as RDDs or DataFrames within Spark. From there, that same data can be used with other tools within Spark, such as the machine learning library MLlib, the graph engine GraphX, or Spark Streaming. This example makes use of the Enron email test set from Stanford Network Analysis Project, and executes the GraphX implementation of PageRank on it to find interesting entities. It then saves the results back to Phoenix. Note that runnable source code is also available on Github &lt;/p&gt; Prerequisites Phoenix 4.4.0+ Spark 1.3.0+ (ensure phoenix-client JAR is in the Spark driver classpath, see setup guide ) Load sample data Login to a node with the Apache Phoenix binaries available. I will use localhost to refer to the Phoenix URL, but you may need to adjust to your local environment cd /path/to/phoenix/bin ./sqlline.py localhost Once in the SQLLine console, we&#39;ll create the tables to hold the input data, and the destination table for the pagerank results CREATE TABLE EMAIL_ENRON( MAIL_FROM BIGINT NOT NULL, MAIL_TO BIGINT NOT NULL CONSTRAINT pk PRIMARY KEY(MAIL_FROM, MAIL_TO)); CREATE TABLE EMAIL_ENRON_PAGERANK( ID BIGINT NOT NULL, RANK DOUBLE CONSTRAINT pk PRIMARY KEY(ID)); Use &#39;ctrl+d&#39; to exit SQLline Download and extract the file enron.csv.gz to a local directory, such as /tmp. We&#39;ll use &#39;psql.py&#39; to load the CSV data gunzip /tmp/enron.csv.gz ./psql.py -t EMAIL_ENRON localhost /tmp/enron.csv When finished, you should see the output: CSV Upsert complete. 367662 rows upserted Interactive analysis with spark-shell Login to a node with Spark installed. Note that the phoenix-client JAR must be available in the Spark driver classpath cd /path/to/spark/bin ./spark-shell Once you&#39;re in the spark shell, you can type, or copy the code below into the interactive shell &lt;/p&gt; import org.apache.spark.graphx._ import org.apache.phoenix.spark._ // Load the phoenix table val rdd = sc.phoenixTableAsRDD(&quot;EMAIL_ENRON&quot;, Seq(&quot;MAIL_FROM&quot;, &quot;MAIL_TO&quot;), zkUrl=Some(&quot;localhost&quot;)) // Convert to an RDD of VertexId tuples val rawEdges = rdd.map{ e =&gt; (e(&quot;MAIL_FROM&quot;).asInstanceOf[VertexId], e(&quot;MAIL_TO&quot;).asInstanceOf[VertexId]) } // Create a graph with default edge weights val graph = Graph.fromEdgeTuples(rawEdges, 1.0) // Run page rank val pr = graph.pageRank(0.001) // Save to Phoenix pr.vertices.saveToPhoenix(&quot;EMAIL_ENRON_PAGERANK&quot;, Seq(&quot;ID&quot;, &quot;RANK&quot;), zkUrl = Some(&quot;localhost&quot;)) Once finished, you can exit spark-shell with &#39;ctrl+d&#39; Results On your Phoenix node, open sqlline again &lt;/p&gt; cd /path/to/phoenix/bin ./sqlline.py localhost Let&#39;s run a query that will give us the top-ranked entities from the PageRank results SELECT * FROM EMAIL_ENRON_PAGERANK ORDER BY RANK DESC LIMIT 5; +------------------------------------------+------------------------------------------+ | ID | RANK | +------------------------------------------+------------------------------------------+ | 5038 | 497.2989872977676 | | 273 | 117.18141799210386 | | 140 | 108.63091596789913 | | 458 | 107.2728800448782 | | 588 | 106.11840798585399 | +------------------------------------------+------------------------------------------+ Although this data-set has the email addresses removed, if you&#39;re curious, you can find results of a similar analysis here. If you&#39;re familiar with the Enron case, some of those names will ring a bell. &lt;/p&gt; Conclusion Although this example is fairly trivial, it shows the capabilities, as well as succinctness, of using Phoenix and Spark together to run complex algorithms across arbitrarily large datasets. In my experience, the methods shown here extend quite well to other &quot;big data&quot; problems such as community detection and clustering, as well as anomaly detection. There are likely many other problem domains which are applicable as well Thanks for reading!","headline":"Spark Integration in Apache Phoenix","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/phoenix/entry/spark_integration_in_apache_phoenix"},"url":"http://localhost:4000/phoenix/entry/spark_integration_in_apache_phoenix"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Spark Integration in Apache Phoenix</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2015-06-29T18:38:48-04:00" itemprop="datePublished">Jun 29, 2015
      </time>â€¢ <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"James Taylor", "login"=>"jamestaylor", "email"=>"jamestaylor@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><em><br />
Today's blog is brought to you by our latest committer and the developer behind the Spark integration in Apache Phoenix,&nbsp;<strong>Josh Mahonin</strong>, a Software Architect at <a href="https://www.interset.com/"><span style="font-size: 15px; font-family: Arial; color: #1155cc; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap; background-color: transparent;">Interset</span></a></em>.</p>
<h2>PageRank with Phoenix and Spark</h2>
<p>
The Phoenix SQL interface provides a lot of great analytics capabilities on top of structured HBase data. Some tasks however, such as machine learning or graph analysis, are more efficiently done using other tools like Apache Spark.</p>
<p>
Since the Phoenix 4.4.0 release, the <a href="https://phoenix.apache.org/phoenix_spark.html">phoenix-spark</a> module allows us to expose Phoenix tables as RDDs or DataFrames within Spark. From there, that same data can be used with other tools within Spark, such as the machine learning library <a href="https://spark.apache.org/docs/latest/mllib-guide.html">MLlib</a>, the graph engine <a href="https://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a>, or <a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.	</p>
<p>This example makes use of the Enron email test set from <a href="https://snap.stanford.edu/data/email-Enron.html">Stanford Network Analysis Project</a>, and executes the GraphX implementation of PageRank on it to find interesting entities. It then saves the results back to Phoenix.</p>
<p>Note that runnable source code is also available on <a href="https://github.com/jmahonin/spark-graphx-phoenix">Github</a></p></p>
<h2>Prerequisites</h2>
<ul>
<li>Phoenix 4.4.0+</li>
<li>Spark 1.3.0+ (ensure phoenix-client JAR is in the Spark driver classpath, see<br />
<a href="https://phoenix.apache.org/phoenix_spark.html">setup guide</a> )
</li>
</ul>
<h2>Load sample data</h2>
<p>Login to a node with the Apache Phoenix binaries available. I will use <em>localhost</em> to refer to the Phoenix URL, but you may need to adjust to your local environment</p>
<pre><code>cd /path/to/phoenix/bin
./sqlline.py localhost</code></pre>
<p>Once in the SQLLine console, we'll create the tables to hold the input data, and the destination table for the pagerank results</p>
<pre><code>CREATE TABLE EMAIL_ENRON(</code>
<code>    MAIL_FROM BIGINT NOT NULL, </code>
<code>    MAIL_TO BIGINT NOT NULL </code>
<code>    CONSTRAINT pk PRIMARY KEY(MAIL_FROM, MAIL_TO));
CREATE TABLE EMAIL_ENRON_PAGERANK(</code>
<code>    ID BIGINT NOT NULL, </code>
<code>    RANK DOUBLE </code>
<code>    CONSTRAINT pk PRIMARY KEY(ID));</code></pre>
<p>Use 'ctrl+d' to exit SQLline</p>
<p>Download and extract the file <a href="https://github.com/jmahonin/spark-graphx-phoenix/blob/master/enron.csv.gz?raw=true">enron.csv.gz</a> to a local directory, such as /tmp. We'll use 'psql.py' to load the CSV data</p>
<pre><code>gunzip /tmp/enron.csv.gz
./psql.py -t EMAIL_ENRON localhost /tmp/enron.csv</code></pre>
<p>When finished, you should see the output:<br /> <code>CSV Upsert complete. 367662 rows upserted</code></p>
<h2>Interactive analysis with spark-shell</h2>
<p>Login to a node with Spark installed. Note that the phoenix-client JAR <strong>must</strong> be available in the Spark driver classpath</p>
<pre><code>cd /path/to/spark/bin
./spark-shell</code></pre>
<p>Once you're in the spark shell, you can type, or copy the code below into the interactive shell</p></p>
<blockquote>
<pre><code>import org.apache.spark.graphx._
import org.apache.phoenix.spark._

// Load the phoenix table
val rdd = sc.phoenixTableAsRDD("EMAIL_ENRON", Seq("MAIL_FROM", "MAIL_TO"), zkUrl=Some("localhost"))

// Convert to an RDD of VertexId tuples
val rawEdges = rdd.map{ e => (e("MAIL_FROM").asInstanceOf[VertexId], e("MAIL_TO").asInstanceOf[VertexId]) }

// Create a graph with default edge weights
val graph = Graph.fromEdgeTuples(rawEdges, 1.0)

// Run page rank
val pr = graph.pageRank(0.001)

// Save to Phoenix
pr.vertices.saveToPhoenix("EMAIL_ENRON_PAGERANK", Seq("ID", "RANK"), zkUrl = Some("localhost"))
</code></pre>
</blockquote>
<p>Once finished, you can exit spark-shell with 'ctrl+d'</p>
<h2>Results</h2>
<p>On your Phoenix node, open sqlline again</p></p>
<p><code><br />
cd /path/to/phoenix/bin<br />
./sqlline.py localhost<br />
</code></p>
<p>Let's run a query that will give us the top-ranked entities from the PageRank results</p>
<pre><code>SELECT * FROM EMAIL_ENRON_PAGERANK ORDER BY RANK DESC LIMIT 5;

+------------------------------------------+------------------------------------------+
|                    ID                    |                   RANK                   |
+------------------------------------------+------------------------------------------+
| 5038                                     | 497.2989872977676                        |
| 273                                      | 117.18141799210386                       |
| 140                                      | 108.63091596789913                       |
| 458                                      | 107.2728800448782                        |
| 588                                      | 106.11840798585399                       |
+------------------------------------------+------------------------------------------+</code></pre>
<p>Although this data-set has the email addresses removed, if you're curious, you can find results of a similar analysis <a href="http://sujitpal.blogspot.ca/2012/12/analyzing-enron-data-frequency.html">here</a>. If you're familiar with the Enron case, some of those names will ring a bell.</p></p>
<h2>Conclusion</h2>
<p>Although this example is fairly trivial, it shows the capabilities, as well as succinctness, of using Phoenix and Spark together to run complex algorithms across arbitrarily large datasets. In my experience, the methods shown here extend quite well to other "big data" problems such as community detection and clustering, as well as anomaly detection. There are likely many other problem domains which are applicable as well</p>
<p>Thanks for reading!</p>

  </div><a class="u-url" href="/phoenix/entry/spark_integration_in_apache_phoenix" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
