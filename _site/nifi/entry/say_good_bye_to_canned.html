<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Say Good-Bye to Canned Data | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Say Good-Bye to Canned Data" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Say Good-Bye to Canned Data Mark Payne -&nbsp; markap14@hotmail.com We&#39;ve all been there. After months of development and exhaustive testing, our killer new web service (or app or analytic or what have you) is ready for production! We&#39;ve tested it with all of the random data that we&#39;ve mocked up, including all of the samples that we&#39;ve concocted to ensure that it handles every bad input that we can imagine. We&#39;ve handled it all well, so it&#39;s time to deploy to production. So we do. And all is great! Until an hour later, when our logs start filling with errors because, despite all of our due diligence in testing, we never could have envisioned getting that as input. And now we&#39;re in a mad frenzy to fix our problem, because we&#39;re now responsible for all of the errors that are happening in production. If there&#39;s one thing that I&#39;ve learned in my years as a software developer, it&#39;s that no matter how diligent we are in testing our code, we get data in production that we just haven&#39;t accounted for. So what can we do about it? Test with live production data! Now I&#39;m not suggesting that we skip the testing phase all together and go straight into production - quite the opposite really. I&#39;m just suggesting that we test &quot;smarter, not harder.&quot; One of the benefits of Apache NiFi (incubating) is that it allows us to have real-time command and control of our data. It allows us to change our dataflow in just a few minutes to send multiple copies of our data to anywhere we want, while providing different reliability guarantees and qualities of service to different parts of our flow. So let&#39;s look at how we might accomplish this. Let&#39;s assume that we typically get our feed of data to our web service from NiFi in a flow that looks something like this: Now let&#39;s say that we want to send a duplicate copy of this data feed to our &quot;staging&quot; environment that is running the new version of our web service - or a new web service all together. We can simply copy and paste the InvokeHTTP processor that we&#39;re using to send the data to our production instance (select the processor and press Ctrl-C, Ctrl-V to copy and paste) and then right-click on the new one and choose &quot;Configure...&quot; In the Properties tab, we will change the URL to point to our staging instance. Now, all we have to do is draw a second connection from the preceding processor to our newly created InvokeHTTP. We will give it the same relationship that feeds the production instance - &quot;splits.&quot; And now any data that goes to our production instance will also be sent to our staging environment: Of course, since we know this is a staging environment, it may not be as powerful as the production instance and it may not handle the entire stream of live data. What we really would like to do is send only about 20% of our data to our staging environment. So how can we accomplish that? We can easily insert a DistributeLoad Processor just ahead of our new InvokeHTTP processor, like so: We can now configure the DistributeLoad Processor to have two relationships. We want 80% of the load to go to relationship &quot;1&quot; and 20% to go to relationship &quot;2.&quot; We can accomplish this by adding two user-defined properties. We right-click on DistributeLoad and choose &quot;Configure...&quot; In the Properties tab, click the icon to add the first property. We give it the name &quot;1&quot; and a value of 80. Then we click OK and add another property. This time, we give the property the name &quot;2&quot; and a value of &quot;20&quot;: Now, all we have to do is go to the Settings Tab and choose to Auto-Terminate Relationship 1. This will throw away 80% percent of our data. (Not to worry, we don&#39;t actually make copies of this data just to throw away 80% of it. The actual work required to &quot;clone&quot; a FlowFile is very small, as it doesn&#39;t actually copy any of the content but rather just creates a new pointer to the content.) Now we add a Connection from DistributeLoad to InvokeHTTP and use Relationship &quot;2.&quot; Start the Processors, and we&#39;ve now got 20% of the data being pushed to our staging environment: Now, we have just one more concern that we need to think about. Since we&#39;re sending data to our staging area, which may go down pretty often, as we are debugging and testing things, won&#39;t the data backup in NiFi on our production dataflow? At what point is this going to cause a problem? This is where my former comment about NiFi offering &quot;different Quality of Service guarantees to different parts of the flow&quot; comes in. For this endpoint, we just want to try to send the data and if it can&#39;t handle the data, we don&#39;t want to risk causing issues in our production environment. To ensure that this happens, we right-click on the connection that feeds the new InvokeHTTP processor and click &quot;Configure...&quot; (You&#39;ll have to first stop the connection&#39;s source and destination processors in order to modify it). In the settings tab here, we have an option for &quot;File expiration.&quot; The default is &quot;0 sec,&quot; which means that the data will not age off. Let&#39;s change this value to 3 minutes. Now, when we click Apply, we can see that the Connection&#39;s label has a small &quot;clock&quot; icon on it, indicating that the connection has an expiration set. Any FlowFile in the connection that becomes more than 3 minutes old will automatically be deleted. This means that we will buffer up to three minutes worth of data in our production instance to send to staging environment but no more. We still will not expire any data that is waiting to go to the production instance. Because of this capability, we can extend our example a bit to perform load testing as well. While in this example we decided that we only wanted to send 20% of our data to the staging environment, we could easily remove the DistributeLoad processor all together. In this way, we will send 100% of our production data to the staging environment, as long as it is able to handle the data rate. However, if it falls behind, it won&#39;t hurt our production servers because they&#39;ll destroy any data more than 3 minutes old. If concerns were to arise, we can disable this in a matter of seconds: simply stop the DistributeLoad processor and the SplitText processor feeding it, remove the connection between them, and restart the SplitText processor. As always, I&#39;d love to hear feedback in the Comments section about how we could improve, or how you&#39;ve solved similar problems." />
<meta property="og:description" content="Say Good-Bye to Canned Data Mark Payne -&nbsp; markap14@hotmail.com We&#39;ve all been there. After months of development and exhaustive testing, our killer new web service (or app or analytic or what have you) is ready for production! We&#39;ve tested it with all of the random data that we&#39;ve mocked up, including all of the samples that we&#39;ve concocted to ensure that it handles every bad input that we can imagine. We&#39;ve handled it all well, so it&#39;s time to deploy to production. So we do. And all is great! Until an hour later, when our logs start filling with errors because, despite all of our due diligence in testing, we never could have envisioned getting that as input. And now we&#39;re in a mad frenzy to fix our problem, because we&#39;re now responsible for all of the errors that are happening in production. If there&#39;s one thing that I&#39;ve learned in my years as a software developer, it&#39;s that no matter how diligent we are in testing our code, we get data in production that we just haven&#39;t accounted for. So what can we do about it? Test with live production data! Now I&#39;m not suggesting that we skip the testing phase all together and go straight into production - quite the opposite really. I&#39;m just suggesting that we test &quot;smarter, not harder.&quot; One of the benefits of Apache NiFi (incubating) is that it allows us to have real-time command and control of our data. It allows us to change our dataflow in just a few minutes to send multiple copies of our data to anywhere we want, while providing different reliability guarantees and qualities of service to different parts of our flow. So let&#39;s look at how we might accomplish this. Let&#39;s assume that we typically get our feed of data to our web service from NiFi in a flow that looks something like this: Now let&#39;s say that we want to send a duplicate copy of this data feed to our &quot;staging&quot; environment that is running the new version of our web service - or a new web service all together. We can simply copy and paste the InvokeHTTP processor that we&#39;re using to send the data to our production instance (select the processor and press Ctrl-C, Ctrl-V to copy and paste) and then right-click on the new one and choose &quot;Configure...&quot; In the Properties tab, we will change the URL to point to our staging instance. Now, all we have to do is draw a second connection from the preceding processor to our newly created InvokeHTTP. We will give it the same relationship that feeds the production instance - &quot;splits.&quot; And now any data that goes to our production instance will also be sent to our staging environment: Of course, since we know this is a staging environment, it may not be as powerful as the production instance and it may not handle the entire stream of live data. What we really would like to do is send only about 20% of our data to our staging environment. So how can we accomplish that? We can easily insert a DistributeLoad Processor just ahead of our new InvokeHTTP processor, like so: We can now configure the DistributeLoad Processor to have two relationships. We want 80% of the load to go to relationship &quot;1&quot; and 20% to go to relationship &quot;2.&quot; We can accomplish this by adding two user-defined properties. We right-click on DistributeLoad and choose &quot;Configure...&quot; In the Properties tab, click the icon to add the first property. We give it the name &quot;1&quot; and a value of 80. Then we click OK and add another property. This time, we give the property the name &quot;2&quot; and a value of &quot;20&quot;: Now, all we have to do is go to the Settings Tab and choose to Auto-Terminate Relationship 1. This will throw away 80% percent of our data. (Not to worry, we don&#39;t actually make copies of this data just to throw away 80% of it. The actual work required to &quot;clone&quot; a FlowFile is very small, as it doesn&#39;t actually copy any of the content but rather just creates a new pointer to the content.) Now we add a Connection from DistributeLoad to InvokeHTTP and use Relationship &quot;2.&quot; Start the Processors, and we&#39;ve now got 20% of the data being pushed to our staging environment: Now, we have just one more concern that we need to think about. Since we&#39;re sending data to our staging area, which may go down pretty often, as we are debugging and testing things, won&#39;t the data backup in NiFi on our production dataflow? At what point is this going to cause a problem? This is where my former comment about NiFi offering &quot;different Quality of Service guarantees to different parts of the flow&quot; comes in. For this endpoint, we just want to try to send the data and if it can&#39;t handle the data, we don&#39;t want to risk causing issues in our production environment. To ensure that this happens, we right-click on the connection that feeds the new InvokeHTTP processor and click &quot;Configure...&quot; (You&#39;ll have to first stop the connection&#39;s source and destination processors in order to modify it). In the settings tab here, we have an option for &quot;File expiration.&quot; The default is &quot;0 sec,&quot; which means that the data will not age off. Let&#39;s change this value to 3 minutes. Now, when we click Apply, we can see that the Connection&#39;s label has a small &quot;clock&quot; icon on it, indicating that the connection has an expiration set. Any FlowFile in the connection that becomes more than 3 minutes old will automatically be deleted. This means that we will buffer up to three minutes worth of data in our production instance to send to staging environment but no more. We still will not expire any data that is waiting to go to the production instance. Because of this capability, we can extend our example a bit to perform load testing as well. While in this example we decided that we only wanted to send 20% of our data to the staging environment, we could easily remove the DistributeLoad processor all together. In this way, we will send 100% of our production data to the staging environment, as long as it is able to handle the data rate. However, if it falls behind, it won&#39;t hurt our production servers because they&#39;ll destroy any data more than 3 minutes old. If concerns were to arise, we can disable this in a matter of seconds: simply stop the DistributeLoad processor and the SplitText processor feeding it, remove the connection between them, and restart the SplitText processor. As always, I&#39;d love to hear feedback in the Comments section about how we could improve, or how you&#39;ve solved similar problems." />
<link rel="canonical" href="http://localhost:4000/nifi/entry/say_good_bye_to_canned" />
<meta property="og:url" content="http://localhost:4000/nifi/entry/say_good_bye_to_canned" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-11-14T13:30:35-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Say Good-Bye to Canned Data" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2019-11-14T13:30:35-05:00","datePublished":"2019-11-14T13:30:35-05:00","description":"Say Good-Bye to Canned Data Mark Payne -&nbsp; markap14@hotmail.com We&#39;ve all been there. After months of development and exhaustive testing, our killer new web service (or app or analytic or what have you) is ready for production! We&#39;ve tested it with all of the random data that we&#39;ve mocked up, including all of the samples that we&#39;ve concocted to ensure that it handles every bad input that we can imagine. We&#39;ve handled it all well, so it&#39;s time to deploy to production. So we do. And all is great! Until an hour later, when our logs start filling with errors because, despite all of our due diligence in testing, we never could have envisioned getting that as input. And now we&#39;re in a mad frenzy to fix our problem, because we&#39;re now responsible for all of the errors that are happening in production. If there&#39;s one thing that I&#39;ve learned in my years as a software developer, it&#39;s that no matter how diligent we are in testing our code, we get data in production that we just haven&#39;t accounted for. So what can we do about it? Test with live production data! Now I&#39;m not suggesting that we skip the testing phase all together and go straight into production - quite the opposite really. I&#39;m just suggesting that we test &quot;smarter, not harder.&quot; One of the benefits of Apache NiFi (incubating) is that it allows us to have real-time command and control of our data. It allows us to change our dataflow in just a few minutes to send multiple copies of our data to anywhere we want, while providing different reliability guarantees and qualities of service to different parts of our flow. So let&#39;s look at how we might accomplish this. Let&#39;s assume that we typically get our feed of data to our web service from NiFi in a flow that looks something like this: Now let&#39;s say that we want to send a duplicate copy of this data feed to our &quot;staging&quot; environment that is running the new version of our web service - or a new web service all together. We can simply copy and paste the InvokeHTTP processor that we&#39;re using to send the data to our production instance (select the processor and press Ctrl-C, Ctrl-V to copy and paste) and then right-click on the new one and choose &quot;Configure...&quot; In the Properties tab, we will change the URL to point to our staging instance. Now, all we have to do is draw a second connection from the preceding processor to our newly created InvokeHTTP. We will give it the same relationship that feeds the production instance - &quot;splits.&quot; And now any data that goes to our production instance will also be sent to our staging environment: Of course, since we know this is a staging environment, it may not be as powerful as the production instance and it may not handle the entire stream of live data. What we really would like to do is send only about 20% of our data to our staging environment. So how can we accomplish that? We can easily insert a DistributeLoad Processor just ahead of our new InvokeHTTP processor, like so: We can now configure the DistributeLoad Processor to have two relationships. We want 80% of the load to go to relationship &quot;1&quot; and 20% to go to relationship &quot;2.&quot; We can accomplish this by adding two user-defined properties. We right-click on DistributeLoad and choose &quot;Configure...&quot; In the Properties tab, click the icon to add the first property. We give it the name &quot;1&quot; and a value of 80. Then we click OK and add another property. This time, we give the property the name &quot;2&quot; and a value of &quot;20&quot;: Now, all we have to do is go to the Settings Tab and choose to Auto-Terminate Relationship 1. This will throw away 80% percent of our data. (Not to worry, we don&#39;t actually make copies of this data just to throw away 80% of it. The actual work required to &quot;clone&quot; a FlowFile is very small, as it doesn&#39;t actually copy any of the content but rather just creates a new pointer to the content.) Now we add a Connection from DistributeLoad to InvokeHTTP and use Relationship &quot;2.&quot; Start the Processors, and we&#39;ve now got 20% of the data being pushed to our staging environment: Now, we have just one more concern that we need to think about. Since we&#39;re sending data to our staging area, which may go down pretty often, as we are debugging and testing things, won&#39;t the data backup in NiFi on our production dataflow? At what point is this going to cause a problem? This is where my former comment about NiFi offering &quot;different Quality of Service guarantees to different parts of the flow&quot; comes in. For this endpoint, we just want to try to send the data and if it can&#39;t handle the data, we don&#39;t want to risk causing issues in our production environment. To ensure that this happens, we right-click on the connection that feeds the new InvokeHTTP processor and click &quot;Configure...&quot; (You&#39;ll have to first stop the connection&#39;s source and destination processors in order to modify it). In the settings tab here, we have an option for &quot;File expiration.&quot; The default is &quot;0 sec,&quot; which means that the data will not age off. Let&#39;s change this value to 3 minutes. Now, when we click Apply, we can see that the Connection&#39;s label has a small &quot;clock&quot; icon on it, indicating that the connection has an expiration set. Any FlowFile in the connection that becomes more than 3 minutes old will automatically be deleted. This means that we will buffer up to three minutes worth of data in our production instance to send to staging environment but no more. We still will not expire any data that is waiting to go to the production instance. Because of this capability, we can extend our example a bit to perform load testing as well. While in this example we decided that we only wanted to send 20% of our data to the staging environment, we could easily remove the DistributeLoad processor all together. In this way, we will send 100% of our production data to the staging environment, as long as it is able to handle the data rate. However, if it falls behind, it won&#39;t hurt our production servers because they&#39;ll destroy any data more than 3 minutes old. If concerns were to arise, we can disable this in a matter of seconds: simply stop the DistributeLoad processor and the SplitText processor feeding it, remove the connection between them, and restart the SplitText processor. As always, I&#39;d love to hear feedback in the Comments section about how we could improve, or how you&#39;ve solved similar problems.","headline":"Say Good-Bye to Canned Data","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/nifi/entry/say_good_bye_to_canned"},"url":"http://localhost:4000/nifi/entry/say_good_bye_to_canned"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Say Good-Bye to Canned Data</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-11-14T13:30:35-05:00" itemprop="datePublished">Nov 14, 2019
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"markap14@apache.org", "login"=>"markap14", "email"=>"markap14@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1>
Say Good-Bye to Canned Data<br />
</h1>
<p>
   <span class="author">Mark Payne -&nbsp;</span><br />
   <span class="author"><a href="mailto:markap14@hotmail.com">markap14@hotmail.com</a></span></p>
<hr />
<p>
We've all been there. After months of development and exhaustive testing, our killer new web service (or app or analytic or what have you) is ready for production!<br />
We've tested it with all of the random data that we've mocked up, including all of the samples that we've concocted to ensure that it handles every bad<br />
input that we can imagine. We've handled it all well, so it's time to deploy to production. So we do. And all is great!</p>
<p>
Until an hour later, when our logs start filling with errors because, despite all of our due diligence in testing, we never could have<br />
envisioned getting <strong>that</strong> as input. And now we're in a mad frenzy to fix our problem, because we're now responsible for all of the<br />
errors that are happening in production.</p>
<p>
If there's one thing that I've learned in my years as a software developer, it's that no matter how diligent we are in testing our code,<br />
we get data in production that we just haven't accounted for.</p>
<p>
So what can we do about it? Test with live production data!</p>
<p>
Now I'm not suggesting that we skip the testing phase all together and go straight into production - quite the opposite really. I'm just suggesting that we test<br />
"smarter, not harder."  One of the benefits of <a href="http://nifi.incubator.apache.org">Apache NiFi (incubating)</a> is that it allows us to have real-time command and control of our data. It allows us to change our<br />
dataflow in just a few minutes to send multiple<br />
copies of our data to anywhere we want, while providing different reliability guarantees and qualities of service to different parts of our flow. So let's look at how we might<br />
accomplish this.</p>
<p>
Let's assume that we typically get our feed of data to our web service from NiFi in a flow that looks something like this:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/145cd4a9-e624-4a2e-8e61-83aef7654a74" alt="Original Flow" /></p>
<p>
Now let's say that we want to send a duplicate copy of this data feed to our "staging" environment that is running the new version of our web service - or a new web service all together. We can simply copy and paste the<br />
InvokeHTTP processor that we're using to send the data to our production instance (select the processor and press Ctrl-C, Ctrl-V to copy and paste) and then right-click on the new one and choose "Configure..." In the Properties tab, we will change the URL<br />
to point to our staging instance. Now, all we have to do is draw a second connection from the preceding processor to our newly created InvokeHTTP. We will give it the same relationship that feeds the production instance - "splits."<br />
And now any data that goes to our production instance will also be sent to our staging environment:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/d627bb1f-5016-4c0d-b7aa-89198e9cfa11" alt="Flow with second InvokeHTTP" /></p>
<p>
Of course, since we know this is a staging environment, it may not be as powerful as the production instance and it may not handle the entire stream of live data.<br />
What we really would like to do is send only about 20% of our data to our staging<br />
environment. So how can we accomplish that? We can easily insert a DistributeLoad Processor just ahead of our new InvokeHTTP processor, like so:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/6fbb7678-b421-47ea-b0da-37b8ec08702f" alt="Flow with DistributeLoad added in" /></p>
<p>
We can now configure the DistributeLoad Processor to have two relationships. We want 80% of the load to go to relationship "1" and 20% to go to relationship "2." We can accomplish this by adding<br />
two user-defined properties. We right-click on DistributeLoad and choose "Configure..." In the Properties tab, click the <img src="https://blogs.apache.org/nifi/mediaresource/07ec2f7b-f931-4dbe-b07d-4394303c773a"> icon to add the first property.<br />
We give it the name "1" and a value of 80. Then we click OK and add another property. This time, we give the property<br />
the name "2" and a value of "20":</p>
<p><img src="https://blogs.apache.org/nifi/mediaresource/fb4865e4-5cb0-453e-aef6-e704c7e1f8e8" alt="Configure DistributeLoad" class="centered" style="margin-top: 1.5em; margin-bottom: 3em;" /></p>
<p>
Now, all we have to do is go to the Settings Tab<br />
and choose to Auto-Terminate Relationship 1. This will throw away 80% percent of our data. (Not to worry, we don't actually make copies of this data just to throw away 80% of it. The actual work required<br />
to "clone" a FlowFile is very small, as it doesn't actually copy any of the content but rather just creates a new pointer to the content.) Now we add a Connection from DistributeLoad to InvokeHTTP and<br />
use Relationship "2." Start the Processors, and we've now got 20% of the data being pushed to our staging environment:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/d247a7f8-9908-4180-bd61-1d991e7f2cc5" alt="20% of data going to staging area" /></p>
<p>
Now, we have just one more concern that we need to think about. Since we're sending data to our staging area, which may go down pretty often, as we are debugging and testing things, won't the data<br />
backup in NiFi on our production dataflow? At what point is this going to cause a problem?</p>
<p>
This is where my former comment about NiFi offering "different Quality of Service guarantees to different parts of the flow" comes in. For this endpoint, we just want to try to send the data and if it can't handle<br />
the data, we don't want to risk causing issues in our production environment. To ensure that this happens, we right-click on the connection that feeds the new InvokeHTTP processor and click "Configure..." (You'll have to first stop<br />
the connection's source and destination processors in order to modify it). In the settings tab here, we have an option for "File expiration." The default is "0 sec," which means that the data will not<br />
age off. Let's change this value to 3 minutes.</p>
<p><img class="centered" src="https://blogs.apache.org/nifi/mediaresource/21572f1d-e585-47ff-bb50-ffa7777589cd" style="margin-top: 1.5em; margin-bottom: 3em;" /></p>
<p>
Now, when we click Apply, we can see that the Connection's label has a small "clock" icon on it, indicating that the connection has an expiration set.</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/d5219184-cb3e-4d74-b8c5-be9042d93596" alt="Age off after 3 minutes" /></p>
<p>
Any FlowFile in the connection that becomes more than 3<br />
minutes old will automatically be deleted. This means that we will buffer up to three minutes worth of data in our production instance to send to staging environment but no more. We still will not expire<br />
any data that is waiting to go to the production instance. Because of this capability, we can extend our example a bit to perform load testing as well. While in this example we decided that we only wanted to<br />
send 20% of our data to the staging environment, we could easily remove the DistributeLoad processor all together. In this way, we will send 100% of our production data to the staging environment, as long<br />
as it is able to handle the data rate. However, if it falls behind, it won't hurt our production servers because they'll destroy any data more than 3 minutes old. If concerns were to arise, we can disable<br />
this in a matter of seconds: simply stop the DistributeLoad processor and the SplitText processor feeding it, remove the connection between them, and restart the SplitText processor.</p>
<p>
As always, I'd love to hear feedback in the Comments section about how we could improve, or how you've solved similar problems.</p>

  </div><a class="u-url" href="/nifi/entry/say_good_bye_to_canned" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
