<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Mail outage post-mortem | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Mail outage post-mortem" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Overview:During the afternoon of May 6th we began experiencing delays in mail delivery of 1-2 hours. Initial efforts at remediation seemed to clear this up but on the morning of May 7th the problem worsened and we proactively disabled mail service to deal with the failure. This outage affected all ASF mailing lists and mail forwarding. The service remained unavailable until May 10th, and it took almost 5 additional days to fully flush the backlog of messages. You can find a timeline here that was kept during the incident: https://blogs.apache.org/infra/entry/mail_outageThis was a catastrophic failure for the Apache Software Foundation as email is core to virtually every operation and is our primary communication medium. &nbsp;What happened: The mail service at the ASF is composed of three physical servers. Two of these are external facing mail exchangers that receive mail. The final server handles mailing list expansion, alias forwarding and mail delivery in general. That latter server had two volumes that experienced a disk outage each. This degraded performance substantially and led to the mail delays seen on May 6th and 7th. The service was proactively disabled on May 7th in an attempt to let the arrays rebuild without the significant disk I/O overhead caused by processing the large mail backlog. Ultimately multiple attempts to rebuild the underlying arrays failed and eventually other drives in the array where the data volume was stored failed rendering recovery a hopeless task on May 8th. We began working to restore backups from our offsite backup location to our primary US datacenter. When this began to take longer than expected, additional concurrent efforts began to restore service in one of our secondary datacenters as well as in a public cloud instance. Ultimately we ended up completing the restoration to our primary US datacenter first and were able to bring the service online. When the service resumed, we had an estimated 10 million message backlog in addition to our normal 1.7-2 million ongoing daily message flow. The amount of backlogged mail taxed the existing infrastructure and architecture of the mail service and took almost 5 days to completely clear. What worked:Our backups were sufficient to allow us to restore the service in good working order. Early precautions taken when we discovered the problem combined with our backups resulted in no data loss from the incident. Our mail exchangers continued to work during the outage and held incoming mail until the service was restored. What didn&#39;t work:Our monitoring was not sufficient to identify the problem or alert us to the symptoms. No spare hard drives for this class of machine were on-hand in our primary datacenter.&nbsp;The restore time from our remote backups took an excessively long time. This was partially due to the large size of the restore data, and partially due to the transport method used for the data. After the service was restored we had approximately a 10M message backlog that took days to clear.The primary administrator of the service was on vacation, and the remaining infrastructure contractors were not intimately familiar with the service.&nbsp;Our documentation was insufficient to easily restore the service in a rapid manner by folks without intimate knowledge.&nbsp;Remediation plan:Our immediate action items: Update the documentation to be current/diagram mail flow. Improve the monitoring of the mail service itself as well as the hardware. Insure we have adequate spares on hand for the majority of our core services. Place our mail server under configuration management to reduce our MTTR Medium-to-Long term initiatives. Crosstraining contractors in all critical services Work on moving to a more fault-tolerant/redundant architecture More fully deploy our config management and automated provisioning across our infrastructure so MTTR is reduced. &nbsp;" />
<meta property="og:description" content="Overview:During the afternoon of May 6th we began experiencing delays in mail delivery of 1-2 hours. Initial efforts at remediation seemed to clear this up but on the morning of May 7th the problem worsened and we proactively disabled mail service to deal with the failure. This outage affected all ASF mailing lists and mail forwarding. The service remained unavailable until May 10th, and it took almost 5 additional days to fully flush the backlog of messages. You can find a timeline here that was kept during the incident: https://blogs.apache.org/infra/entry/mail_outageThis was a catastrophic failure for the Apache Software Foundation as email is core to virtually every operation and is our primary communication medium. &nbsp;What happened: The mail service at the ASF is composed of three physical servers. Two of these are external facing mail exchangers that receive mail. The final server handles mailing list expansion, alias forwarding and mail delivery in general. That latter server had two volumes that experienced a disk outage each. This degraded performance substantially and led to the mail delays seen on May 6th and 7th. The service was proactively disabled on May 7th in an attempt to let the arrays rebuild without the significant disk I/O overhead caused by processing the large mail backlog. Ultimately multiple attempts to rebuild the underlying arrays failed and eventually other drives in the array where the data volume was stored failed rendering recovery a hopeless task on May 8th. We began working to restore backups from our offsite backup location to our primary US datacenter. When this began to take longer than expected, additional concurrent efforts began to restore service in one of our secondary datacenters as well as in a public cloud instance. Ultimately we ended up completing the restoration to our primary US datacenter first and were able to bring the service online. When the service resumed, we had an estimated 10 million message backlog in addition to our normal 1.7-2 million ongoing daily message flow. The amount of backlogged mail taxed the existing infrastructure and architecture of the mail service and took almost 5 days to completely clear. What worked:Our backups were sufficient to allow us to restore the service in good working order. Early precautions taken when we discovered the problem combined with our backups resulted in no data loss from the incident. Our mail exchangers continued to work during the outage and held incoming mail until the service was restored. What didn&#39;t work:Our monitoring was not sufficient to identify the problem or alert us to the symptoms. No spare hard drives for this class of machine were on-hand in our primary datacenter.&nbsp;The restore time from our remote backups took an excessively long time. This was partially due to the large size of the restore data, and partially due to the transport method used for the data. After the service was restored we had approximately a 10M message backlog that took days to clear.The primary administrator of the service was on vacation, and the remaining infrastructure contractors were not intimately familiar with the service.&nbsp;Our documentation was insufficient to easily restore the service in a rapid manner by folks without intimate knowledge.&nbsp;Remediation plan:Our immediate action items: Update the documentation to be current/diagram mail flow. Improve the monitoring of the mail service itself as well as the hardware. Insure we have adequate spares on hand for the majority of our core services. Place our mail server under configuration management to reduce our MTTR Medium-to-Long term initiatives. Crosstraining contractors in all critical services Work on moving to a more fault-tolerant/redundant architecture More fully deploy our config management and automated provisioning across our infrastructure so MTTR is reduced. &nbsp;" />
<link rel="canonical" href="http://localhost:4000/infra/entry/mail_outage_post_mortem" />
<meta property="og:url" content="http://localhost:4000/infra/entry/mail_outage_post_mortem" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2014-05-28T14:03:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Mail outage post-mortem" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2014-05-28T14:03:00-04:00","datePublished":"2014-05-28T14:03:00-04:00","description":"Overview:During the afternoon of May 6th we began experiencing delays in mail delivery of 1-2 hours. Initial efforts at remediation seemed to clear this up but on the morning of May 7th the problem worsened and we proactively disabled mail service to deal with the failure. This outage affected all ASF mailing lists and mail forwarding. The service remained unavailable until May 10th, and it took almost 5 additional days to fully flush the backlog of messages. You can find a timeline here that was kept during the incident: https://blogs.apache.org/infra/entry/mail_outageThis was a catastrophic failure for the Apache Software Foundation as email is core to virtually every operation and is our primary communication medium. &nbsp;What happened: The mail service at the ASF is composed of three physical servers. Two of these are external facing mail exchangers that receive mail. The final server handles mailing list expansion, alias forwarding and mail delivery in general. That latter server had two volumes that experienced a disk outage each. This degraded performance substantially and led to the mail delays seen on May 6th and 7th. The service was proactively disabled on May 7th in an attempt to let the arrays rebuild without the significant disk I/O overhead caused by processing the large mail backlog. Ultimately multiple attempts to rebuild the underlying arrays failed and eventually other drives in the array where the data volume was stored failed rendering recovery a hopeless task on May 8th. We began working to restore backups from our offsite backup location to our primary US datacenter. When this began to take longer than expected, additional concurrent efforts began to restore service in one of our secondary datacenters as well as in a public cloud instance. Ultimately we ended up completing the restoration to our primary US datacenter first and were able to bring the service online. When the service resumed, we had an estimated 10 million message backlog in addition to our normal 1.7-2 million ongoing daily message flow. The amount of backlogged mail taxed the existing infrastructure and architecture of the mail service and took almost 5 days to completely clear. What worked:Our backups were sufficient to allow us to restore the service in good working order. Early precautions taken when we discovered the problem combined with our backups resulted in no data loss from the incident. Our mail exchangers continued to work during the outage and held incoming mail until the service was restored. What didn&#39;t work:Our monitoring was not sufficient to identify the problem or alert us to the symptoms. No spare hard drives for this class of machine were on-hand in our primary datacenter.&nbsp;The restore time from our remote backups took an excessively long time. This was partially due to the large size of the restore data, and partially due to the transport method used for the data. After the service was restored we had approximately a 10M message backlog that took days to clear.The primary administrator of the service was on vacation, and the remaining infrastructure contractors were not intimately familiar with the service.&nbsp;Our documentation was insufficient to easily restore the service in a rapid manner by folks without intimate knowledge.&nbsp;Remediation plan:Our immediate action items: Update the documentation to be current/diagram mail flow. Improve the monitoring of the mail service itself as well as the hardware. Insure we have adequate spares on hand for the majority of our core services. Place our mail server under configuration management to reduce our MTTR Medium-to-Long term initiatives. Crosstraining contractors in all critical services Work on moving to a more fault-tolerant/redundant architecture More fully deploy our config management and automated provisioning across our infrastructure so MTTR is reduced. &nbsp;","headline":"Mail outage post-mortem","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/infra/entry/mail_outage_post_mortem"},"url":"http://localhost:4000/infra/entry/mail_outage_post_mortem"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Mail outage post-mortem</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2014-05-28T14:03:00-04:00" itemprop="datePublished">May 28, 2014
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"David Nalley", "login"=>"ke4qqq", "email"=>"ke4qqq@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><span style="color: #222222; font-family: arial; font-size: small;"><b>Overview:</b></span><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">During the afternoon of May 6th we began experiencing delays in mail delivery of 1-2 hours. Initial efforts at remediation seemed to clear this up but on the morning of May 7th the problem worsened and we proactively disabled mail service to deal with the failure. This outage affected all ASF mailing lists and mail forwarding. The service remained unavailable until May 10th, and it took almost 5 additional days to fully flush the backlog of messages. </span><br style="color: #222222; font-family: arial; font-size: small;" /><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">You can find a timeline here that was kept during the incident: https://blogs.apache.org/infra/entry/mail_outage</span><br style="color: #222222; font-family: arial; font-size: small;" /><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">This was a catastrophic failure for the Apache Software Foundation as email is core to virtually every operation and is our primary communication medium. &nbsp;</span><br style="color: #222222; font-family: arial; font-size: small;" /><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;"><b>What happened:</b> </span><br style="color: #222222; font-family: arial; font-size: small;" /><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">The mail service at the ASF is composed of three physical servers. Two of these are external facing mail exchangers that receive mail. The final server handles mailing list expansion, alias forwarding and mail delivery in general. That latter server had two volumes that experienced a disk outage each. This degraded performance substantially and led to the mail delays seen on May 6th and 7th. The service was proactively disabled on May 7th in an attempt to let the arrays rebuild without the significant disk I/O overhead caused by processing the large mail backlog. Ultimately multiple attempts to rebuild the underlying arrays failed and eventually other drives in the array where the data volume was stored failed rendering recovery a hopeless task on May 8th. We began working to restore backups from our offsite backup location to our primary US datacenter. When this began to take longer than expected, additional concurrent efforts began to restore service in one of our secondary datacenters as well as in a public cloud instance. Ultimately we ended up completing the restoration to our primary US datacenter first and were able to bring the service online. When the service resumed, we had an estimated 10 million message backlog in addition to our normal 1.7-2 million ongoing daily message flow. The amount of backlogged mail taxed the existing infrastructure and architecture of the mail service and took almost 5 days to completely clear. </span><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;"></span></p>
<p><span style="color: #222222; font-family: arial; font-size: small;"><b>What worked:</b></span><br style="color: #222222; font-family: arial; font-size: small;" /><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">Our backups were sufficient to allow us to restore the service in good working order. </span><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">Early precautions taken when we discovered the problem combined with our backups resulted in no data loss from the incident. </span><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">Our mail exchangers continued to work during the outage and held incoming mail until the service was restored. </span><br style="color: #222222; font-family: arial; font-size: small;" /><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;"><b>What didn't work:</b></span><br style="color: #222222; font-family: arial; font-size: small;" /><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">Our monitoring was not sufficient to identify the problem or alert us to the symptoms. </span><br style="color: #222222; font-family: arial; font-size: small;" /><font color="#222222" face="arial" size="2">No spare hard drives for this class of machine were on-hand in our primary datacenter.&nbsp;</font><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">The restore time from our remote backups took an excessively long time. This was partially due to the large size of the restore data, and partially due to the transport method used for the data. </span><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">After the service was restored we had approximately a 10M message backlog that took days to clear.</span><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">The primary administrator of the service was on vacation, and the remaining infrastructure contractors were not intimately familiar with the service.&nbsp;</span><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">Our documentation was insufficient to easily restore the service in a rapid manner by folks without intimate knowledge.&nbsp;</span><br style="color: #222222; font-family: arial; font-size: small;" /><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;"><b>Remediation plan:</b></span><br style="color: #222222; font-family: arial; font-size: small;" /><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">Our immediate action items:</span><br style="color: #222222; font-family: arial; font-size: small;" /> </p>
<ul>
<li><span style="color: #222222; font-family: arial; font-size: small;">Update the documentation to be current/diagram mail flow.</span></li>
<li><span style="font-size: small; color: #222222; font-family: arial;">Improve the monitoring of the mail service itself as well as the hardware.</span><span style="font-size: small; color: #222222; font-family: arial;"> </span></li>
<li><span style="font-size: small; color: #222222; font-family: arial;">Insure we have adequate spares on hand for the majority of our core services.</span><span style="font-size: small; color: #222222; font-family: arial;"> </span></li>
<li><span style="font-size: small; color: #222222; font-family: arial;">Place our mail server under configuration management to reduce our MTTR</span><span style="font-size: small; color: #222222; font-family: arial;"> </span></li>
</ul>
<p><br style="color: #222222; font-family: arial; font-size: small;" /><span style="color: #222222; font-family: arial; font-size: small;">Medium-to-Long term initiatives.</span><br style="color: #222222; font-family: arial; font-size: small;" /> </p>
<ul>
<li><span style="color: #222222; font-family: arial; font-size: small;">Crosstraining contractors in all critical services</span></li>
<li><span style="color: #222222; font-family: arial; font-size: small;">Work on moving to a more fault-tolerant/redundant architecture</span></li>
<li><span style="color: #222222; font-family: arial; font-size: small;">More fully deploy our config management and automated provisioning across our infrastructure so MTTR is reduced.</span></li>
</ul>
<p>&nbsp;</p>

  </div><a class="u-url" href="/infra/entry/mail_outage_post_mortem" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
