<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Apache Hama announces v0.7 Release! | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Apache Hama announces v0.7 Release!" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Apache Hama team is pleased to announce the release of Hama v0.7 with new features and improvements. Hama is a High-Performance BSP computing engine, which can be used to perform compute-intensive general scientific BSP applications, Google&rsquo;s Pregel-like graph applications, and machine learning algorithms. What are the major changes from the last release? The important new feature of this release is that support the Mesos and Yet Another Resource Negotiator (YARN), so you&rsquo;re able to submit your BSP applications to the existing open source and enterprise clusters e.g., CDH, HDP, and Mesosphere without any installation. In addition, we reinforced machine learning package by adding algorithms such as Max-Flow, K-Core, ANN, ..., etc. There are also big improvements in the queue and messaging systems. We now use own outgoing/incoming message manager instead of using Java&#39;s built-in queues. It stores messages in serialized form in a set of bundles (or a single bundle) to reduce the memory usage and RPC overhead. Unsafe serialization is used to serialize Vertex and its message objects more quickly. Another important improvement is the enhanced graph package. Instead of sending each message individually, we package the messages per vertex and send a packaged message to their assigned destination nodes. With this we achieved significant improvement in the performance of graph applications. The attached benchmarks were done to test scalability and performance of PageRank algorithm for random generated 1 billion edges graph using Apache Hama and Giraph on Amazon EMR 30 nodes cluster. Note that the aggregators was used for detecting the convergence condition in case of Apache Hama. What&rsquo;s Next? After a month of testing and benchmarking this version will bring substantial performance improvements together with important bug fixes which significantly improve the platform stability. We look forward to add more and more and see our community grow. The primary objective of the technical plans are: Add stream input format for listening messages coming from 3rd party applications, and incremental learning algorithms. Improve reliability of system e.g., fault tolerance, HA, ..., etc. More machine learning algorithms, such as ensemble classifier, SVM, DNN, ..., etc Where I can download it? The release artifacts are published and ready for you to download either from the Apache mirrors or from the Maven repository. We welcome your help, feedback, and suggestions. For more information on how to report problems, and to get involved, visit the Hama project website[1] and wiki[2]. [1]. Apache Hama Website: https://hama.apache.org/ [2]. Apache Hama Wiki: https://wiki.apache.org/hama/" />
<meta property="og:description" content="Apache Hama team is pleased to announce the release of Hama v0.7 with new features and improvements. Hama is a High-Performance BSP computing engine, which can be used to perform compute-intensive general scientific BSP applications, Google&rsquo;s Pregel-like graph applications, and machine learning algorithms. What are the major changes from the last release? The important new feature of this release is that support the Mesos and Yet Another Resource Negotiator (YARN), so you&rsquo;re able to submit your BSP applications to the existing open source and enterprise clusters e.g., CDH, HDP, and Mesosphere without any installation. In addition, we reinforced machine learning package by adding algorithms such as Max-Flow, K-Core, ANN, ..., etc. There are also big improvements in the queue and messaging systems. We now use own outgoing/incoming message manager instead of using Java&#39;s built-in queues. It stores messages in serialized form in a set of bundles (or a single bundle) to reduce the memory usage and RPC overhead. Unsafe serialization is used to serialize Vertex and its message objects more quickly. Another important improvement is the enhanced graph package. Instead of sending each message individually, we package the messages per vertex and send a packaged message to their assigned destination nodes. With this we achieved significant improvement in the performance of graph applications. The attached benchmarks were done to test scalability and performance of PageRank algorithm for random generated 1 billion edges graph using Apache Hama and Giraph on Amazon EMR 30 nodes cluster. Note that the aggregators was used for detecting the convergence condition in case of Apache Hama. What&rsquo;s Next? After a month of testing and benchmarking this version will bring substantial performance improvements together with important bug fixes which significantly improve the platform stability. We look forward to add more and more and see our community grow. The primary objective of the technical plans are: Add stream input format for listening messages coming from 3rd party applications, and incremental learning algorithms. Improve reliability of system e.g., fault tolerance, HA, ..., etc. More machine learning algorithms, such as ensemble classifier, SVM, DNN, ..., etc Where I can download it? The release artifacts are published and ready for you to download either from the Apache mirrors or from the Maven repository. We welcome your help, feedback, and suggestions. For more information on how to report problems, and to get involved, visit the Hama project website[1] and wiki[2]. [1]. Apache Hama Website: https://hama.apache.org/ [2]. Apache Hama Wiki: https://wiki.apache.org/hama/" />
<link rel="canonical" href="http://localhost:4000/hama/entry/apache_hama_announces_v0_7" />
<meta property="og:url" content="http://localhost:4000/hama/entry/apache_hama_announces_v0_7" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2015-06-15T04:04:35-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Apache Hama announces v0.7 Release!" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2015-06-15T04:04:35-04:00","datePublished":"2015-06-15T04:04:35-04:00","description":"Apache Hama team is pleased to announce the release of Hama v0.7 with new features and improvements. Hama is a High-Performance BSP computing engine, which can be used to perform compute-intensive general scientific BSP applications, Google&rsquo;s Pregel-like graph applications, and machine learning algorithms. What are the major changes from the last release? The important new feature of this release is that support the Mesos and Yet Another Resource Negotiator (YARN), so you&rsquo;re able to submit your BSP applications to the existing open source and enterprise clusters e.g., CDH, HDP, and Mesosphere without any installation. In addition, we reinforced machine learning package by adding algorithms such as Max-Flow, K-Core, ANN, ..., etc. There are also big improvements in the queue and messaging systems. We now use own outgoing/incoming message manager instead of using Java&#39;s built-in queues. It stores messages in serialized form in a set of bundles (or a single bundle) to reduce the memory usage and RPC overhead. Unsafe serialization is used to serialize Vertex and its message objects more quickly. Another important improvement is the enhanced graph package. Instead of sending each message individually, we package the messages per vertex and send a packaged message to their assigned destination nodes. With this we achieved significant improvement in the performance of graph applications. The attached benchmarks were done to test scalability and performance of PageRank algorithm for random generated 1 billion edges graph using Apache Hama and Giraph on Amazon EMR 30 nodes cluster. Note that the aggregators was used for detecting the convergence condition in case of Apache Hama. What&rsquo;s Next? After a month of testing and benchmarking this version will bring substantial performance improvements together with important bug fixes which significantly improve the platform stability. We look forward to add more and more and see our community grow. The primary objective of the technical plans are: Add stream input format for listening messages coming from 3rd party applications, and incremental learning algorithms. Improve reliability of system e.g., fault tolerance, HA, ..., etc. More machine learning algorithms, such as ensemble classifier, SVM, DNN, ..., etc Where I can download it? The release artifacts are published and ready for you to download either from the Apache mirrors or from the Maven repository. We welcome your help, feedback, and suggestions. For more information on how to report problems, and to get involved, visit the Hama project website[1] and wiki[2]. [1]. Apache Hama Website: https://hama.apache.org/ [2]. Apache Hama Wiki: https://wiki.apache.org/hama/","headline":"Apache Hama announces v0.7 Release!","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/hama/entry/apache_hama_announces_v0_7"},"url":"http://localhost:4000/hama/entry/apache_hama_announces_v0_7"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Apache Hama announces v0.7 Release!</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2015-06-15T04:04:35-04:00" itemprop="datePublished">Jun 15, 2015
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Edward J. Yoon", "login"=>"edwardyoon", "email"=>"edwardyoon@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Apache Hama team is pleased to announce the release of Hama v0.7 with new features and improvements.</p>
<p>Hama is a High-Performance BSP computing engine, which can be used to perform compute-intensive general scientific BSP applications, Google&rsquo;s Pregel-like graph applications, and machine learning algorithms.</p>
<p><b>What are the major changes from the last release?</b></p>
<p>The important new feature of this release is that support the Mesos and Yet Another Resource Negotiator (YARN), so you&rsquo;re able to submit your BSP applications to the existing open source and enterprise clusters e.g., CDH, HDP, and Mesosphere without any installation. In addition, we reinforced machine learning package by adding algorithms such as Max-Flow, K-Core, ANN, ..., etc.</p>
<p>There are also big improvements in the queue and messaging systems. We now use own outgoing/incoming message manager instead of using Java's built-in queues. It stores messages in serialized form in a set of bundles (or a single bundle) to reduce the memory usage and RPC overhead. Unsafe serialization is used to serialize Vertex and its message objects more quickly. Another important improvement is the enhanced graph package. Instead of sending each message individually, we package the messages per vertex and send a packaged message to their assigned destination nodes. With this we achieved significant improvement in the performance of graph applications. The attached benchmarks were done to test scalability and performance of PageRank algorithm for random generated 1 billion edges graph using Apache Hama and Giraph on Amazon EMR 30 nodes cluster. Note that the aggregators was used for detecting the convergence condition in case of Apache Hama.</p>
<table align="center">
<tr>
<td><img src="https://lh5.googleusercontent.com/-iRaQhZbiO3o/VWujxDoSM8I/AAAAAAAAE-Y/Iodd-tc2yrg/w433-h289-no/201505211353771_EM6S04A2%255B1%255D.gif" height="240">
</td>
<td><img src="https://lh3.googleusercontent.com/-KR3TQNELI-A/VWujxPIqfCI/AAAAAAAAE-U/BxWzPhZFuL4/w482-h290-no/201505150920041_LK7CT9SZ%255B1%255D.gif" height="240">
</td>
</tr>
</table>
<p><b>What&rsquo;s Next?</b></p>
<p>After a month of testing and benchmarking this version will bring substantial performance improvements together with important bug fixes which significantly improve the platform stability. We look forward to add more and more and see our community grow. The primary objective of the technical plans are:</p>
<ul>
<li>Add stream input format for listening messages coming from 3rd party applications, and incremental learning algorithms.</li>
<li>Improve reliability of system e.g., fault tolerance, HA, ..., etc.</li>
<li>More machine learning algorithms, such as ensemble classifier, SVM, DNN, ..., etc</li>
</ul>
<p><b>Where I can download it?</b></p>
<p>The release artifacts are published and ready for you to download either from the Apache mirrors or from the Maven repository. We welcome your help, feedback, and suggestions. For more information on how to report problems, and to get involved, visit the Hama project website[1] and wiki[2]. </p>
<p>[1]. Apache Hama Website: <a href="https://hama.apache.org/">https://hama.apache.org/</a><br />
[2]. Apache Hama Wiki: <a href="https://wiki.apache.org/hama/">https://wiki.apache.org/hama/</a></p>

  </div><a class="u-url" href="/hama/entry/apache_hama_announces_v0_7" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
