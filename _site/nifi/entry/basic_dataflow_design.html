<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Apache NiFi: Thinking Differently About Dataflow | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Apache NiFi: Thinking Differently About Dataflow" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Apache NiFi: Thinking Differently About DataFlow Mark Payne -&nbsp; markap14@hotmail.com Recently a question was posed to the Apache NiFi (Incubating) Developer Mailing List about how best to use Apache NiFi to perform Extract, Transform, Load (ETL) types of tasks. The question was &quot;Is it possible to have NiFi service setup and running and allow for multiple dataflows to be designed and deployed (running) at the same time?&quot; The idea here was to create several disparate dataflows that run alongside one another in parallel. Data comes from Source X and it&#39;s processed this way. That&#39;s one dataflow. Other data comes from Source Y and it&#39;s processed this way. That&#39;s a second dataflow entirely. Typically, this is how we think about dataflow when we design it with an ETL tool. And this is a pretty common question for new NiFi users. With NiFi, though, we tend to think about designing dataflows a little bit differently. Rather than having several disparate, &quot;stovepiped&quot; flows, the preferred approach with NiFi is to have several inputs feed into the same dataflow. Data can then be easily routed (via RouteOnAttribute, for example) to &quot;one-off subflows&quot; if need be. One of the benefits to having several disparate dataflows, though, is that it makes it much easier to answer when someone comes to you and says &quot;I sent you a file last week. What did you do with it?&quot; or &quot;How do you process data that comes from this source?&quot; You may not know exactly what happened to a specific file that they sent you, step-by-step, because of the different decision points in the flow, but at least you have a good idea by looking at the layout of the dataflow. So we can avoid merging the data if we would like. For the sake of an example, let&#39;s assume that we have 4 different data sources. For each of them, they are going to send us some text data that needs to be pushed into HDFS. Maybe it&#39;s compressed, maybe it&#39;s not. So your flow will look like this: Now, let&#39;s say that that you&#39;ve got a new requirement. When you&#39;re sending text data to HDFS, each file that is pushed to HDFS needs to have 1,000 lines of text or less (yes, that&#39;s a contrived example and that&#39;s probably never a good idea, but the point is valid.) Now, consider how much work it is to make all of those modifications. And let&#39;s hope that you don&#39;t miss any! If we continue down this path, this can get hairy quickly, as we have several different dataflows side-by-side on the same graph. In order to aid in the visual representation, we can use Process Groups to provide a nice logical separation. If we do that for each of those, we end up with something like: We can then double-click each of those Process Groups and see/edit what&#39;s inside. But we still have the issue of having to change 4 different flows to make the change mentioned. So let us consider the alternate approach of merging it all into a single dataflow, and we end up with a flow like this: Now, we have all of the data going to a single stream. If we want to update it, we just insert one new Processor: And we&#39;re done. We don&#39;t have to make this change to insert a SplitText processor 3 more times. The concern that we have here, though, as mentioned earlier, is that if all of the data is mixed together, as the dataflow grows larger, how do we know what happens to data that came from Source X, for example? This is where the Data Provenance feature comes in. In the top right-hand corner there&#39;s a toolbar with 8 icons. The 4th one is the Provenance icon (). If we click on that, we can then search for data that has been processed. For this example, let&#39;s simply searched for RECEIVE events. This shows us all of the RECEIVE events that this instance of NiFi has seen within the time range searched: If we click the Lineage Graph icon () on the right, for the first file, we see exactly what happened to this piece of data: We see that a RECEIVE event occurred, and that generated a FlowFile. That FlowFile&#39;s attributes were then modified, its content was modified, and then the FlowFile was forked, and dropped. At this point, we can right-click on each of these event nodes and choose to view the details of the event. For the RECEIVE event, we see: From here, we can see that the RECEIVE event took place at 16:55:51 EST on 01/11/2015. The component that reported the event was named &quot;Data From Source X&quot; and was a GetFile Processor. We can also see that the URI of the data was file:/C:/temp/in/debug/LICENSE.gz. If we follow the lineage of that FlowFile, we see that the next event is an ATTRIBUTES_MODIFIED Event: Since the event is of type ATTRIBUTES_MODIFIED, it immediately begs the question &quot;What attributes were modified?&quot; So clicking the &quot;Attributes&quot; tab shows us this: As with any Provenance Event, we can see all of the attributes that were present on the FlowFile when the event occurred. Of interest here, we can see that the value of the &quot;mime.type&quot; attribute was changed from &quot;No value set&quot; (the attribute didn&#39;t exist) to &quot;application/gzip&quot;. The next event in our lineage is a CONTENT_MODIFIED Event. If we view the details here, we will see: Here, we can see that the content was modified by a Processor named Decompress. This makes sense, since the previous Event showed us that the MIME Type was &quot;application/gzip&quot;. After decompressing the data, we arrive at a FORK Event: This event shows us that the FORK happened by the SplitText Processor. That is, the SplitText Processor broke a large FlowFile into many smaller FlowFiles. On the right-hand side of this dialog, we see that the file was broken into six different &quot;child&quot; FlowFiles. This is where things get fun! If we then close this dialog, we can right-click on the FORK Event and choose the &quot;Expand&quot; option. This will then pull back the lineage for each of those children, providing us with a more holistic view of what happened to this piece of data: Now, we can see that each of those six children was sent somewhere. Viewing the details of these events shows us where they were sent: The first file, for instance, was sent via the PutHDFS Processor with the filename &quot;/nifi/blogs/thinking-differently/LICENSE.gz&quot;. This occurred at 16:55:53 EST on 01/11/2015. We can also see in this dialog the &quot;Lineage Duration&quot; was &quot;00:00:02.712&quot; or 2.712 seconds. The &quot;Lineage Duration&quot; field tells us how long elapsed between the time when the original source data was received and the time at which this event occurred. Finally, we have the DROP event. The DROP event signifies the end of line for a FlowFile. If we look at the details of this event, we see: Of note here, we see that the DROP event was emitted by PutHDFS. That is, PutHDFS was the last component in NiFi to process this piece of information. We can also see in the &quot;Details&quot; field why the FlowFile was dropped: it was Auto-terminated by the &quot;success&quot; relationship. NiFi&#39;s Data Provenance capability allows us to understand exactly what happens to each piece of data that is received. We are given a directed graph that shows when a FlowFile was received, when it was modified, when it was routed in a particular way, and when and where it was sent - as well as which component performed the action. We are also able to see, for each event, the attributes (or metadata) associated with the data so that we can understand why the particular event occurred. Additionally, when many pieces of data are merged together or a single piece of data is split apart, we are able to understand fully the provenance of this data from the that it was received until the time at which it exited the flow. This makes it very easy to answer the question &quot;I sent you a file last week. What did you do with it?&quot; while providing a much more holistic view of the enterprise dataflow than would be available if we used many disparate flows. Hopefully this post helps you to understand not only the way that we like to setup the flows with NiFi but also the benefits that we have as a result and the features that allow us to overcome any challenges that this approach may create." />
<meta property="og:description" content="Apache NiFi: Thinking Differently About DataFlow Mark Payne -&nbsp; markap14@hotmail.com Recently a question was posed to the Apache NiFi (Incubating) Developer Mailing List about how best to use Apache NiFi to perform Extract, Transform, Load (ETL) types of tasks. The question was &quot;Is it possible to have NiFi service setup and running and allow for multiple dataflows to be designed and deployed (running) at the same time?&quot; The idea here was to create several disparate dataflows that run alongside one another in parallel. Data comes from Source X and it&#39;s processed this way. That&#39;s one dataflow. Other data comes from Source Y and it&#39;s processed this way. That&#39;s a second dataflow entirely. Typically, this is how we think about dataflow when we design it with an ETL tool. And this is a pretty common question for new NiFi users. With NiFi, though, we tend to think about designing dataflows a little bit differently. Rather than having several disparate, &quot;stovepiped&quot; flows, the preferred approach with NiFi is to have several inputs feed into the same dataflow. Data can then be easily routed (via RouteOnAttribute, for example) to &quot;one-off subflows&quot; if need be. One of the benefits to having several disparate dataflows, though, is that it makes it much easier to answer when someone comes to you and says &quot;I sent you a file last week. What did you do with it?&quot; or &quot;How do you process data that comes from this source?&quot; You may not know exactly what happened to a specific file that they sent you, step-by-step, because of the different decision points in the flow, but at least you have a good idea by looking at the layout of the dataflow. So we can avoid merging the data if we would like. For the sake of an example, let&#39;s assume that we have 4 different data sources. For each of them, they are going to send us some text data that needs to be pushed into HDFS. Maybe it&#39;s compressed, maybe it&#39;s not. So your flow will look like this: Now, let&#39;s say that that you&#39;ve got a new requirement. When you&#39;re sending text data to HDFS, each file that is pushed to HDFS needs to have 1,000 lines of text or less (yes, that&#39;s a contrived example and that&#39;s probably never a good idea, but the point is valid.) Now, consider how much work it is to make all of those modifications. And let&#39;s hope that you don&#39;t miss any! If we continue down this path, this can get hairy quickly, as we have several different dataflows side-by-side on the same graph. In order to aid in the visual representation, we can use Process Groups to provide a nice logical separation. If we do that for each of those, we end up with something like: We can then double-click each of those Process Groups and see/edit what&#39;s inside. But we still have the issue of having to change 4 different flows to make the change mentioned. So let us consider the alternate approach of merging it all into a single dataflow, and we end up with a flow like this: Now, we have all of the data going to a single stream. If we want to update it, we just insert one new Processor: And we&#39;re done. We don&#39;t have to make this change to insert a SplitText processor 3 more times. The concern that we have here, though, as mentioned earlier, is that if all of the data is mixed together, as the dataflow grows larger, how do we know what happens to data that came from Source X, for example? This is where the Data Provenance feature comes in. In the top right-hand corner there&#39;s a toolbar with 8 icons. The 4th one is the Provenance icon (). If we click on that, we can then search for data that has been processed. For this example, let&#39;s simply searched for RECEIVE events. This shows us all of the RECEIVE events that this instance of NiFi has seen within the time range searched: If we click the Lineage Graph icon () on the right, for the first file, we see exactly what happened to this piece of data: We see that a RECEIVE event occurred, and that generated a FlowFile. That FlowFile&#39;s attributes were then modified, its content was modified, and then the FlowFile was forked, and dropped. At this point, we can right-click on each of these event nodes and choose to view the details of the event. For the RECEIVE event, we see: From here, we can see that the RECEIVE event took place at 16:55:51 EST on 01/11/2015. The component that reported the event was named &quot;Data From Source X&quot; and was a GetFile Processor. We can also see that the URI of the data was file:/C:/temp/in/debug/LICENSE.gz. If we follow the lineage of that FlowFile, we see that the next event is an ATTRIBUTES_MODIFIED Event: Since the event is of type ATTRIBUTES_MODIFIED, it immediately begs the question &quot;What attributes were modified?&quot; So clicking the &quot;Attributes&quot; tab shows us this: As with any Provenance Event, we can see all of the attributes that were present on the FlowFile when the event occurred. Of interest here, we can see that the value of the &quot;mime.type&quot; attribute was changed from &quot;No value set&quot; (the attribute didn&#39;t exist) to &quot;application/gzip&quot;. The next event in our lineage is a CONTENT_MODIFIED Event. If we view the details here, we will see: Here, we can see that the content was modified by a Processor named Decompress. This makes sense, since the previous Event showed us that the MIME Type was &quot;application/gzip&quot;. After decompressing the data, we arrive at a FORK Event: This event shows us that the FORK happened by the SplitText Processor. That is, the SplitText Processor broke a large FlowFile into many smaller FlowFiles. On the right-hand side of this dialog, we see that the file was broken into six different &quot;child&quot; FlowFiles. This is where things get fun! If we then close this dialog, we can right-click on the FORK Event and choose the &quot;Expand&quot; option. This will then pull back the lineage for each of those children, providing us with a more holistic view of what happened to this piece of data: Now, we can see that each of those six children was sent somewhere. Viewing the details of these events shows us where they were sent: The first file, for instance, was sent via the PutHDFS Processor with the filename &quot;/nifi/blogs/thinking-differently/LICENSE.gz&quot;. This occurred at 16:55:53 EST on 01/11/2015. We can also see in this dialog the &quot;Lineage Duration&quot; was &quot;00:00:02.712&quot; or 2.712 seconds. The &quot;Lineage Duration&quot; field tells us how long elapsed between the time when the original source data was received and the time at which this event occurred. Finally, we have the DROP event. The DROP event signifies the end of line for a FlowFile. If we look at the details of this event, we see: Of note here, we see that the DROP event was emitted by PutHDFS. That is, PutHDFS was the last component in NiFi to process this piece of information. We can also see in the &quot;Details&quot; field why the FlowFile was dropped: it was Auto-terminated by the &quot;success&quot; relationship. NiFi&#39;s Data Provenance capability allows us to understand exactly what happens to each piece of data that is received. We are given a directed graph that shows when a FlowFile was received, when it was modified, when it was routed in a particular way, and when and where it was sent - as well as which component performed the action. We are also able to see, for each event, the attributes (or metadata) associated with the data so that we can understand why the particular event occurred. Additionally, when many pieces of data are merged together or a single piece of data is split apart, we are able to understand fully the provenance of this data from the that it was received until the time at which it exited the flow. This makes it very easy to answer the question &quot;I sent you a file last week. What did you do with it?&quot; while providing a much more holistic view of the enterprise dataflow than would be available if we used many disparate flows. Hopefully this post helps you to understand not only the way that we like to setup the flows with NiFi but also the benefits that we have as a result and the features that allow us to overcome any challenges that this approach may create." />
<link rel="canonical" href="http://localhost:4000/nifi/entry/basic_dataflow_design" />
<meta property="og:url" content="http://localhost:4000/nifi/entry/basic_dataflow_design" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-11-14T13:30:41-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Apache NiFi: Thinking Differently About Dataflow" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2019-11-14T13:30:41-05:00","datePublished":"2019-11-14T13:30:41-05:00","description":"Apache NiFi: Thinking Differently About DataFlow Mark Payne -&nbsp; markap14@hotmail.com Recently a question was posed to the Apache NiFi (Incubating) Developer Mailing List about how best to use Apache NiFi to perform Extract, Transform, Load (ETL) types of tasks. The question was &quot;Is it possible to have NiFi service setup and running and allow for multiple dataflows to be designed and deployed (running) at the same time?&quot; The idea here was to create several disparate dataflows that run alongside one another in parallel. Data comes from Source X and it&#39;s processed this way. That&#39;s one dataflow. Other data comes from Source Y and it&#39;s processed this way. That&#39;s a second dataflow entirely. Typically, this is how we think about dataflow when we design it with an ETL tool. And this is a pretty common question for new NiFi users. With NiFi, though, we tend to think about designing dataflows a little bit differently. Rather than having several disparate, &quot;stovepiped&quot; flows, the preferred approach with NiFi is to have several inputs feed into the same dataflow. Data can then be easily routed (via RouteOnAttribute, for example) to &quot;one-off subflows&quot; if need be. One of the benefits to having several disparate dataflows, though, is that it makes it much easier to answer when someone comes to you and says &quot;I sent you a file last week. What did you do with it?&quot; or &quot;How do you process data that comes from this source?&quot; You may not know exactly what happened to a specific file that they sent you, step-by-step, because of the different decision points in the flow, but at least you have a good idea by looking at the layout of the dataflow. So we can avoid merging the data if we would like. For the sake of an example, let&#39;s assume that we have 4 different data sources. For each of them, they are going to send us some text data that needs to be pushed into HDFS. Maybe it&#39;s compressed, maybe it&#39;s not. So your flow will look like this: Now, let&#39;s say that that you&#39;ve got a new requirement. When you&#39;re sending text data to HDFS, each file that is pushed to HDFS needs to have 1,000 lines of text or less (yes, that&#39;s a contrived example and that&#39;s probably never a good idea, but the point is valid.) Now, consider how much work it is to make all of those modifications. And let&#39;s hope that you don&#39;t miss any! If we continue down this path, this can get hairy quickly, as we have several different dataflows side-by-side on the same graph. In order to aid in the visual representation, we can use Process Groups to provide a nice logical separation. If we do that for each of those, we end up with something like: We can then double-click each of those Process Groups and see/edit what&#39;s inside. But we still have the issue of having to change 4 different flows to make the change mentioned. So let us consider the alternate approach of merging it all into a single dataflow, and we end up with a flow like this: Now, we have all of the data going to a single stream. If we want to update it, we just insert one new Processor: And we&#39;re done. We don&#39;t have to make this change to insert a SplitText processor 3 more times. The concern that we have here, though, as mentioned earlier, is that if all of the data is mixed together, as the dataflow grows larger, how do we know what happens to data that came from Source X, for example? This is where the Data Provenance feature comes in. In the top right-hand corner there&#39;s a toolbar with 8 icons. The 4th one is the Provenance icon (). If we click on that, we can then search for data that has been processed. For this example, let&#39;s simply searched for RECEIVE events. This shows us all of the RECEIVE events that this instance of NiFi has seen within the time range searched: If we click the Lineage Graph icon () on the right, for the first file, we see exactly what happened to this piece of data: We see that a RECEIVE event occurred, and that generated a FlowFile. That FlowFile&#39;s attributes were then modified, its content was modified, and then the FlowFile was forked, and dropped. At this point, we can right-click on each of these event nodes and choose to view the details of the event. For the RECEIVE event, we see: From here, we can see that the RECEIVE event took place at 16:55:51 EST on 01/11/2015. The component that reported the event was named &quot;Data From Source X&quot; and was a GetFile Processor. We can also see that the URI of the data was file:/C:/temp/in/debug/LICENSE.gz. If we follow the lineage of that FlowFile, we see that the next event is an ATTRIBUTES_MODIFIED Event: Since the event is of type ATTRIBUTES_MODIFIED, it immediately begs the question &quot;What attributes were modified?&quot; So clicking the &quot;Attributes&quot; tab shows us this: As with any Provenance Event, we can see all of the attributes that were present on the FlowFile when the event occurred. Of interest here, we can see that the value of the &quot;mime.type&quot; attribute was changed from &quot;No value set&quot; (the attribute didn&#39;t exist) to &quot;application/gzip&quot;. The next event in our lineage is a CONTENT_MODIFIED Event. If we view the details here, we will see: Here, we can see that the content was modified by a Processor named Decompress. This makes sense, since the previous Event showed us that the MIME Type was &quot;application/gzip&quot;. After decompressing the data, we arrive at a FORK Event: This event shows us that the FORK happened by the SplitText Processor. That is, the SplitText Processor broke a large FlowFile into many smaller FlowFiles. On the right-hand side of this dialog, we see that the file was broken into six different &quot;child&quot; FlowFiles. This is where things get fun! If we then close this dialog, we can right-click on the FORK Event and choose the &quot;Expand&quot; option. This will then pull back the lineage for each of those children, providing us with a more holistic view of what happened to this piece of data: Now, we can see that each of those six children was sent somewhere. Viewing the details of these events shows us where they were sent: The first file, for instance, was sent via the PutHDFS Processor with the filename &quot;/nifi/blogs/thinking-differently/LICENSE.gz&quot;. This occurred at 16:55:53 EST on 01/11/2015. We can also see in this dialog the &quot;Lineage Duration&quot; was &quot;00:00:02.712&quot; or 2.712 seconds. The &quot;Lineage Duration&quot; field tells us how long elapsed between the time when the original source data was received and the time at which this event occurred. Finally, we have the DROP event. The DROP event signifies the end of line for a FlowFile. If we look at the details of this event, we see: Of note here, we see that the DROP event was emitted by PutHDFS. That is, PutHDFS was the last component in NiFi to process this piece of information. We can also see in the &quot;Details&quot; field why the FlowFile was dropped: it was Auto-terminated by the &quot;success&quot; relationship. NiFi&#39;s Data Provenance capability allows us to understand exactly what happens to each piece of data that is received. We are given a directed graph that shows when a FlowFile was received, when it was modified, when it was routed in a particular way, and when and where it was sent - as well as which component performed the action. We are also able to see, for each event, the attributes (or metadata) associated with the data so that we can understand why the particular event occurred. Additionally, when many pieces of data are merged together or a single piece of data is split apart, we are able to understand fully the provenance of this data from the that it was received until the time at which it exited the flow. This makes it very easy to answer the question &quot;I sent you a file last week. What did you do with it?&quot; while providing a much more holistic view of the enterprise dataflow than would be available if we used many disparate flows. Hopefully this post helps you to understand not only the way that we like to setup the flows with NiFi but also the benefits that we have as a result and the features that allow us to overcome any challenges that this approach may create.","headline":"Apache NiFi: Thinking Differently About Dataflow","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/nifi/entry/basic_dataflow_design"},"url":"http://localhost:4000/nifi/entry/basic_dataflow_design"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Apache NiFi: Thinking Differently About Dataflow</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-11-14T13:30:41-05:00" itemprop="datePublished">Nov 14, 2019
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"markap14@apache.org", "login"=>"markap14", "email"=>"markap14@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1>
Apache NiFi: Thinking Differently About DataFlow<br />
</h1>
<p>
   <span class="author">Mark Payne -&nbsp;</span><br />
   <span class="author"><a href="mailto:markap14@hotmail.com">markap14@hotmail.com</a></span></p>
<hr />
<p>
Recently a question was posed to the Apache NiFi (Incubating) Developer<br />
<a href="http://mail-archives.apache.org/mod_mbox/incubator-nifi-dev/201501.mbox/%3C95DC1A334F3B2F4BA0F990EF1C1FD507278E1ACC8B%40USATL02ERMS50.amer.prgx.com%3E">Mailing List</a><br />
about how best to use Apache NiFi to perform Extract, Transform, Load (ETL) types of tasks. The question was "Is it possible to have NiFi service setup<br />
and running and allow for multiple dataflows to be designed and deployed (running) at the same time?"</p>
<p>
The idea here was to create several disparate dataflows that run alongside one another in parallel.<br />
Data comes from Source X and it's processed this way. That's one dataflow.<br />
Other data comes from Source Y and it's processed this way. That's a second dataflow entirely.<br />
Typically, this is how we think about dataflow when we design it with an ETL tool. And this is a<br />
pretty common question for new NiFi users. With NiFi, though, we tend to think about designing dataflows<br />
a little bit differently. Rather than having several disparate, "stovepiped" flows, the preferred approach with<br />
NiFi is to have several inputs feed into the same dataflow. Data can then be easily routed (via RouteOnAttribute,<br />
for example) to "one-off subflows" if need be.</p>
<p>
One of the benefits to having several disparate dataflows, though, is that it makes it much easier<br />
to answer when someone comes to you and says "I sent you a file last week. What did you do with it?" or<br />
"How do you process data that comes from this source?" You may not know exactly what happened to a specific file<br />
that they sent you, step-by-step, because of the different decision points in the flow,<br />
but at least you have a good idea by looking at the layout of the dataflow.</p>
<p>
So we can avoid merging the data if we would like. For the sake of an example, let's assume that we have 4<br />
different data sources. For each of them, they are going to send us some text data that needs to be pushed into<br />
HDFS. Maybe it's compressed, maybe it's not. So your flow will look like this:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/3726d969-b80b-46fc-8b11-65810c770c98" alt="Disparate Flows" /></p>
<p>
Now, let's say that that you've got a new requirement. When you're sending text data to HDFS, each file that is<br />
pushed to HDFS needs to have 1,000 lines of text or less (yes, that's a contrived example and that's probably<br />
never a good idea, but the point is valid.) Now, consider how much work it is to make all of those modifications.<br />
And let's hope that you don't miss any!</p>
<p>
If we continue down this path, this can get hairy quickly, as we have several different dataflows side-by-side<br />
on the same graph. In order to aid in the visual representation, we can use Process Groups<br />
to provide a nice logical separation. If we do that for each of those, we end up with something like:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/3dd03551-563f-473c-a9a3-43a988f38e0a" alt="Grouped Disparate Flows" /></p>
<p>
We can then double-click each of those Process Groups and see/edit what's inside. But we still have the issue of<br />
having to change 4 different flows to make the change mentioned.</p>
<p>
So let us consider the alternate approach of merging it all into a single dataflow, and we end up with a flow like this:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/91f15182-d4e7-4fda-9180-c30948729f31" alt="Merged Flow" /></p>
<p>
Now, we have all of the data going to a single stream. If we want to update it, we just insert one new Processor:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/fe110d5e-7b5f-440b-b475-f0fbfc77748c" alt="Updated Merged Flow" /></p>
<p>
And we're done. We don't have to make this change to insert a SplitText processor 3 more times.</p>
<p>
The concern that we have here, though, as mentioned earlier, is that if all of the data is mixed together, as the dataflow<br />
grows larger, how do we know what happens to data that came from Source X, for example?</p>
<p>
This is where the Data Provenance feature comes in. In the top right-hand corner there's a toolbar with 8 icons. The 4th one is<br />
the Provenance icon (<img src="https://blogs.apache.org/nifi/mediaresource/ba0a6ccb-aa5a-434f-bdb1-8be0c77f456e" alt="Provenance Icon" />).<br />
If we click on that, we can then search for data that has been processed. For this example, let's simply searched for RECEIVE events.</p>
<p>
This shows us all of the RECEIVE events that this instance of NiFi has seen within the time range searched:</p>
<p><img class="centered" width="1366" src="https://blogs.apache.org/nifi/mediaresource/996bb99e-c89f-4bc8-8a44-81535a07c47e" alt="Provenance Search Results" style="margin-top: 1.5em; margin-bottom: 3em;" /></p>
<p>
If we click the Lineage Graph icon (<img src="https://blogs.apache.org/nifi/mediaresource/cbbc9e7b-cba1-4dec-bb74-bd6b2f76da58" />) on the right,<br />
for the first file, we see exactly what happened to this piece of data:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/8e1d921e-bcbc-4e6b-b23c-00c20a590a22" alt="Data Lineage" /></p>
<p>
We see that a RECEIVE event occurred, and that generated a FlowFile. That FlowFile's attributes were then modified,<br />
its content was modified, and then the FlowFile was forked, and dropped. At this point, we can right-click on each of these event nodes<br />
and choose to view the details of the event. For the RECEIVE event, we see:</p>
<p><img class="centered" style="margin-top: 1.5em; margin-bottom: 3em;" src="https://blogs.apache.org/nifi/mediaresource/404bc06f-b394-44c2-a01a-b67c1367b014" alt="Receive Event" /></p>
<p>
From here, we can see that the RECEIVE event took place at 16:55:51 EST on 01/11/2015. The component that reported the event was named "Data From Source X" and was a<br />
GetFile Processor. We can also see that the URI of the data was file:/C:/temp/in/debug/LICENSE.gz. If we follow the lineage of that FlowFile, we see that the next event is<br />
an ATTRIBUTES_MODIFIED Event:</p>
<p><img class="centered" style="margin-top: 1.5em; margin-bottom: 3em;" src="https://blogs.apache.org/nifi/mediaresource/751bc911-238f-4c6d-8196-fbc3db9db738" alt="Attrs Modified Event" /></p>
<p>
Since the event is of type ATTRIBUTES_MODIFIED, it immediately begs the question "What attributes were modified?" So clicking the "Attributes" tab shows us this:</p>
<p><img class="centered" style="margin-top: 1.5em; margin-bottom: 3em;" src="https://blogs.apache.org/nifi/mediaresource/c75c386d-34c3-4b66-9d77-71726f8717d6" alt="Attrs Modified Event" /></p>
<p>
As with any Provenance Event, we can see all of the attributes that were present on the FlowFile when the event occurred. Of interest here, we can see that the value of the "mime.type"<br />
attribute was changed from "<i>No value set</i>" (the attribute didn't exist) to "application/gzip". The next event in our lineage is a CONTENT_MODIFIED Event. If we view the details<br />
here, we will see:</p>
<p><img class="centered" style="margin-top: 1.5em; margin-bottom: 3em;" src="https://blogs.apache.org/nifi/mediaresource/c8a92a1a-84fc-43f0-923c-81c5a660168c" alt="Content Modified Event" /></p>
<p>
Here, we can see that the content was modified by a Processor named Decompress. This makes sense, since the previous Event showed us that the MIME Type was "application/gzip".<br />
After decompressing the data, we arrive at a FORK Event:</p>
<p><img class="centered" style="margin-top: 1.5em; margin-bottom: 3em;" src="https://blogs.apache.org/nifi/mediaresource/8cda0d45-dc64-406c-b957-98a495dfda3e" alt="Fork Event" /></p>
<p>
This event shows us that the FORK happened by the SplitText Processor. That is, the SplitText Processor broke a large FlowFile into many smaller FlowFiles. On the right-hand<br />
side of this dialog, we see that the file was broken into six different "child" FlowFiles. This is where things get fun! If we then close this dialog, we can right-click on the FORK Event<br />
and choose the "Expand" option. This will then pull back the lineage for each of those children, providing us with a more holistic view of what happened to this piece of data:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/556e03ef-0e9a-456c-ba4f-2c9222c89933" alt="Expanded Lineage" /></p>
<p>
Now, we can see that each of those six children was sent somewhere. Viewing the details of these events shows us where they were sent:</p>
<p><img class="centered" style="margin-top: 1.5em; margin-bottom: 3em;" src="https://blogs.apache.org/nifi/mediaresource/5c1aba35-2340-440e-bb47-a2d3dbed75b4" alt="Send Event" /></p>
<p>
The first file, for instance, was sent via the PutHDFS Processor with the filename "/nifi/blogs/thinking-differently/LICENSE.gz". This occurred at 16:55:53 EST on 01/11/2015. We can<br />
also see in this dialog the "Lineage Duration" was "00:00:02.712" or 2.712 seconds. The "Lineage Duration" field tells us how long elapsed between the time when the original source<br />
data was received and the time at which this event occurred.</p>
<p>
Finally, we have the DROP event. The DROP event signifies the end of line for a FlowFile. If we look at the details of this event, we see:</p>
<p><img class="centered" style="margin-top: 1.5em; margin-bottom: 3em;" src="https://blogs.apache.org/nifi/mediaresource/e1eca707-1187-40ec-84ac-f9701250a6fa" alt="Drop Event" /></p>
<p>
Of note here, we see that the DROP event was emitted by PutHDFS. That is, PutHDFS was the last component in NiFi to process this piece of information. We can also see in the<br />
"Details" field why the FlowFile was dropped: it was Auto-terminated by the "success" relationship.</p>
<p>
NiFi's Data Provenance capability allows us to understand exactly what happens to each piece of data that is received. We are<br />
given a directed graph that shows when a FlowFile was received, when it was modified, when it was routed in a particular way,<br />
and when and where it was sent - as well as which component performed the action. We are also able to see, for each event,<br />
the attributes (or metadata) associated with the data so that we can understand why the particular event occurred. Additionally,<br />
when many pieces of data are merged together or a single piece of data is split apart, we are able to understand fully the provenance<br />
of this data from the that it was received until the time at which it exited the flow. This makes it very easy to answer the question<br />
"I sent you a file last week. What did you do with it?" while providing a much more holistic view of the enterprise dataflow than<br />
would be available if we used many disparate flows.</p>
<p>
Hopefully this post helps you to understand not only the way that we like to setup the flows with NiFi but<br />
also the benefits that we have as a result and the features that allow us to overcome any challenges<br />
that this approach may create.</p>

  </div><a class="u-url" href="/nifi/entry/basic_dataflow_design" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
