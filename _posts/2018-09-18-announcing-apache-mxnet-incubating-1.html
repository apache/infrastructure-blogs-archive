---
layout: post
status: PUBLISHED
published: true
title: Announcing Apache MXNet (incubating) 1.3.0 Release
author:
  display_name: Sheng Zha
  login: zhasheng
  email: zhasheng@apache.org
author_login: zhasheng
author_email: zhasheng@apache.org
id: 5a573d9e-33e2-4972-ba52-885328531c35
date: '2018-09-18 15:46:37 -0400'
categories:
- Technology
tags:
- release
- apachemxnet
- mxnet
- 1.3.0
comments:
- id: 0
  author: Gregory
  author_email: gregorywillett265@gmail.com
  author_url: https://payforessayz.com/lab-report-writing-services/
  date: '2019-04-18 19:47:25 -0400'
  content: An incredibly useful improvements. As for me one of the best updates for
    the last time.  This is going to be useful for my work at https://payforessayz.com/lab-report-writing-services/.
    Great!
- id: 0
  author: tiozyscomi1970
  author_email: taisiruha1976@gmail.com
  author_url: https://blogs.apache.org/click/entry/apache_click_v2_3_01
  date: '2019-07-20 17:16:52 -0400'
  content: This is some great work. The choice of colors is very nice and helps bring
    a certain dark vibe to it. So cool!! Love the colors and light.
- id: 0
  author: taisneakcarcewr1976
  author_email: pantmostseme1989@gmail.com
  author_url: https://blogs.apache.org/couchdb/entry/couchdb_conf_videos
  date: '2019-07-20 18:29:53 -0400'
  content: u've made really usefull product for that business, great! love how your
    design dont took attract from main product, but all together its looks like something
    solid, great!! Vavavooo....... cool!
- id: 0
  author: zidemoosul1976
  author_email: liakalogend1974@gmail.com
  author_url: https://blogs.apache.org/openejb/entry/apache_tomee_1_0_0
  date: '2019-07-20 20:31:40 -0400'
  content: Your work is beautiful. I'm a fan now! Thanks @Quentin Li ! It's indeed
    an spectacular place, I really enjoyed shooting it!
- id: 0
  author: bsolazerlie1980
  author_email: jecuwisis1986@gmail.com
  author_url: https://blogs.apache.org/couchdb/entry/apache_couchdb_1_2_0
  date: '2019-07-21 01:56:08 -0400'
  content: Herzlichen Dank Birgit, es freut mich wirklich sehr zu h&ouml;ren! @Birgit
    Schlosser Awesome work!
- id: 0
  author: vesmaraser1980
  author_email: erinomem1972@gmail.com
  author_url: https://blogs.apache.org/openejb/entry/apache_openejb_3_1_3
  date: '2019-07-21 03:34:21 -0400'
  content: Amazing idea and concepts! Great job here! Feel free to check out my last
    project.
- id: 0
  author: fikungcosseo1970
  author_email: jaipharnera1976@gmail.com
  author_url: https://blogs.apache.org/sentry/entry/apache_sentry_architecture_overview
  date: '2019-07-21 05:34:31 -0400'
  content: Big papa at it again Great job with the illustrations!
- id: 0
  author: silvakenkgan1975
  author_email: presiceplin1973@gmail.com
  author_url: https://blogs.apache.org/openejb/entry/how_much_time_have_you
  date: '2019-07-21 06:14:33 -0400'
  content: Amazing work! love the messages as well as the fantastic typography touch.
    Your project is so nice !
- id: 0
  author: rerogecert1981
  author_email: taipsychubtraf1982@gmail.com
  author_url: https://blogs.apache.org/james/entry/apache_james_server_2_3
  date: '2019-07-21 06:47:31 -0400'
  content: sooo coool! love it! thank you @Uttam Lilani @DINO 3AVR @arianna correia
    fiallo
- id: 0
  author: ganiledo1978
  author_email: gulfvecnewpwork1976@gmail.com
  author_url: https://blogs.apache.org/trafficserver/entry/traffic_server_hackathon_at_apachecon
  date: '2019-07-21 09:28:52 -0400'
  content: Super! Awesome!! The minnie is pretty dope.
- id: 0
  author: vorstheviperg1982
  author_email: catthawrkate1986@gmail.com
  author_url: https://blogs.apache.org/couchdb/entry/cloudant_and_ibm_our_commitment
  date: '2019-07-21 11:08:09 -0400'
  content: muy limpio y clasico a la vez! bien! NICE JOB.
- id: 0
  author: Jack
  author_email: casinority.com@gmail.com
  author_url: ''
  date: '2019-12-05 10:22:37 -0500'
  content: "Thanks for recommendations.\r\nhttps://casinority.com/za/"
- id: 0
  author: Vegus168
  author_email: vegusx1337@gmail.com
  author_url: https://vegus168x.blogspot.com/
  date: '2020-02-20 20:46:53 -0500'
  content: thank you for sharing your knowledge and experience with me
- id: 3
  author: tersprenostoh1989
  author_email: moozevisur1974@gmail.com
  author_url: https://blogs.apache.org/openejb/entry/user_blog_how_to_unit
  date: '2019-07-21 08:56:12 -0400'
  content: Sympa! Nice creative & execution Charis
- id: 4
  author: acinsorre1984
  author_email: undertili1987@gmail.com
  author_url: https://blogs.apache.org/hama/entry/how_will_hama_bsp_different
  date: '2019-07-21 08:24:18 -0400'
  content: What a beautiful project. Strong but poetic pics also. Your technique is
    awesome but I think you photograph with your heart. Thanks for sharing your art.
    Gr8 job!
- id: 6
  author: kholalingan1975
  author_email: veuskyldeslay1979@gmail.com
  author_url: https://blogs.apache.org/apex/entry/apache_apex_malhar_3_6
  date: '2019-07-20 18:02:26 -0400'
  content: awesome packages, simple and beautiful Great design!:)
- id: 13
  author: mediabox hd for IOS
  author_email: mediaboxhdxyz@gmail.com
  author_url: https://mediaboxhd.xyz/mediabox-hd-ios/
  date: '2019-07-11 05:42:09 -0400'
  content: Watching movies in mediabox hd is a great experience.
- id: 20
  author: exvetlawhitt1981
  author_email: gosribani1984@gmail.com
  author_url: https://blogs.apache.org/openejb/entry/ejbcontext_getcallerprincipal_improvements
  date: '2019-07-20 23:49:02 -0400'
  content: Just beautiful! Your works look very professional! I think we have a lot
    to learn from each other :)
- id: 30
  author: Jack
  author_email: jacmesj@gmail.com
  author_url: ''
  date: '2018-09-28 13:21:00 -0400'
  content: "Nice release... Thanks!\r\n\r\nJack G.\r\nhttps://www.gunaydinmesajlari.org"
- id: 34
  author: evondioni1982
  author_email: simpcosnata1978@gmail.com
  author_url: https://blogs.apache.org/james/entry/apache_james_mime4j_0_7
  date: '2019-07-21 10:02:49 -0400'
  content: Great! Good Job and Best wishes for your future projects. Kindly give attention
    to my projects. Good Job and Best wishes for your future projects. Kindly give
    attention to my projects))
- id: 36
  author: Free sky
  author_email: flo3ds@gmail.com
  author_url: http://www.free-sky.fr
  date: '2019-01-18 15:08:32 -0500'
  content: "Radio astronomie antenne tuto\r\nhttp://www.free-sky.fr\r\nprogrammation"
- id: 45
  author: xyediskesearch1988
  author_email: hoftbounare1981@gmail.com
  author_url: https://blogs.apache.org/hadoop/entry/welcome-to-apache-hadoop
  date: '2019-07-20 17:38:40 -0400'
  content: Beautiful work! And love that you show your sketches and study drawing.
    Wow, so good
- id: 76
  author: rackrerethe1984
  author_email: eradlihe1987@gmail.com
  author_url: https://blogs.apache.org/openejb/entry/user_blog_effective_unit_testing
  date: '2019-07-21 07:20:01 -0400'
  content: 这个神态太勾魂了~ Incredible :)
- id: 85
  author: beetv
  author_email: beetvapp@gmail.com
  author_url: ''
  date: '2019-02-07 06:09:45 -0500'
  content: I have watched movies and tv shows on my android device using the beetv
    app (https://beetvapp.com/) which is one of the finest apps out there.
- id: 95
  author: david cameron
  author_email: davidcameron7040@gmail.com
  author_url: https://www.killerdumps.com/
  date: '2019-02-07 07:41:53 -0500'
  content: "Thanks for the great content admin,\r\nI like to see more quality content
    on your website. you explained everything nicely.\r\n\r\nThe Cisco Certifications
    exam is one of the toughest exams for IT professionals. Cisco focuses on network
    hardware and devices such as routers and network switches. This is why its certification
    program is geared toward the information technology field.\r\nKillerDumps is a
    reliable and trusty website that provides the most reliable Cisco dumps. Our Cisco
    Dump issue will help you pass the exam in first attempt. All Cisco exams are regularly
    updated and approved by our experts&rsquo; professionals.\r\n\r\nIf you want to
    pass Cisco exam\r\nYou can get <a href=\"https://www.killerdumps.com/cisco-exams\">Cisco
    Exams Dumps</a>."
- id: 97
  author: rmenobfeesou1989
  author_email: ramcupechilf1974@gmail.com
  author_url: https://blogs.apache.org/logging/entry/website_upgrade
  date: '2019-07-21 00:37:45 -0400'
  content: Dope!! Great job, I'm obsessed with those illustrations!
- id: 151
  author: hendry jullius
  author_email: hendryjullius@gmail.com
  author_url: ''
  date: '2018-10-26 07:26:14 -0400'
  content: The Apache software foundation software MXNet now has experimental support
    for the Clojure programming language. The MXNet Clojure package brings state-of-the-art
    deep learning to the Clojure community. It has many more features, enhancements,
    and usability improvements! In this blog post.http://pariscitytourguide.com/private-tours-paris/paris-city-tour/
- id: 717
  author: erosopka1988
  author_email: anenweaxi1971@gmail.com
  author_url: https://blogs.apache.org/openejb/entry/user_blog_how_to_unit
  date: '2019-07-21 05:01:47 -0400'
  content: A haunting, creepy and sad commentary on the relationship between humans
    and their world !!! Incredible vision and execution, Florian !!! Thank you for
    this incredible work !!! very high tech!
- id: 22135
  author: linkmivimis1977
  author_email: miemaiphimor1981@gmail.com
  author_url: https://blogs.apache.org/logging/entry/apache_log4php_2_2_1
  date: '2019-07-21 10:35:52 -0400'
  content: I like the art direction here Such an interesting insight of what the future
    might hold for you. Really isn't something that the mainstream media and film
    producers have in mind. This collection shows how creative people can get.
- id: 90679
  author: aa
  author_email: hendryjullius@gmail.com
  author_url: ''
  date: '2018-10-26 06:31:31 -0400'
  content: good http://www.gmail.com
- id: 28962825
  author: freesulatder1985
  author_email: wadismaho1988@gmail.com
  author_url: https://blogs.apache.org/openejb/entry/apache_tomee_at_jax_london
  date: '2019-07-21 07:52:09 -0400'
  content: damn so good! the red and blue work so well together. Love it
permalink: mxnet/entry/announcing-apache-mxnet-incubating-1
---
<p>Today the Apache MXNet community is pleased to announce the 1.3 release of the Apache MXNet deep learning framework. We would like to thank the Apache MXNet community for all their valuable contributions towards the MXNet 1.3 release.</p>
<p>With this release, MXNet has Gluon package enhancements, ONNX export, experimental Clojure bindings, TensorRT integration, and many more features, enhancements and usability improvements! In this blog post, we briefly summarize some of the high-level features and improvements. For a comprehensive list of major features and bug fixes, read the <a href="https://github.com/apache/incubator-mxnet/releases/tag/1.3.0">Apache MXNet 1.3.0 release notes</a>.</p>
<p><a href="https://github.com/apache/incubator-mxnet/releases/tag/1.3.0"><img src="https://blogs.apache.org/mxnet/mediaresource/fd748206-7cc5-4617-b57d-9560bdd1fca5?t=true" alt="mxnet-1.3.0.png"></img></a></p>
<h2 id="gluon-package-enhancements">Gluon package enhancements</h2>
<p><strong>Gluon RNN layers are now hybridizable</strong>: With this feature, Gluon RNN layers such as <a href="https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.RNN"><code>gluon.rnn.RNN</code></a>, <a href="https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.LSTM"><code>gluon.rnn.LSTM</code></a> and <a href="https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#mxnet.gluon.rnn.GRU"><code>gluon.rnn.GRU</code></a> can be converted to <a href="https://gluon.mxnet.io/chapter07_distributed-learning/hybridize.html"><code>HybridBlock</code>s</a>. Now, many dynamic networks that are based on <a href="https://mxnet.incubator.apache.org/api/python/gluon/rnn.html#recurrent-layers">Gluon RNN layers</a> can be completely hybridized, exported and used in the inference APIs in other language bindings such as C/C++, Scala, R, etc.</p>
<p><strong>Support for sparse tensor</strong>: Gluon <a href="https://gluon.mxnet.io/chapter07_distributed-learning/hybridize.html"><code>HybridBlock</code>s</a> now support hybridization with sparse operators. To enable sparse gradients in <a href="https://mxnet.incubator.apache.org/api/python/gluon/nn.html#mxnet.gluon.nn.Embedding"><code>gluon.nn.Embedding</code></a>, simply set <code>sparse_grad=True</code>. Furthermore, <a href="https://mxnet.incubator.apache.org/api/python/gluon/contrib.html#mxnet.gluon.contrib.nn.SparseEmbedding"><code>gluon.contrib.nn.SparseEmbedding</code></a> provides an example of leveraging sparse parameters to reduce communication cost and memory consumption for multi-GPU training with large embeddings.</p>
<p><strong>Support for Synchronized Cross-GPU Batch Norm</strong>: Gluon now supports Synchronized Batch Normalization, available as <a href="https://mxnet.incubator.apache.org/api/python/gluon/contrib.html#mxnet.gluon.contrib.nn.SyncBatchNorm">gluon.contrib.nn.SyncBatchNorm</a>. This enables stable training on large-scale networks with high memory consumption such as FCN for image segmentation.</p>
<p><strong>Updated Gluon model zoo</strong>: <a href="https://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html">Gluon Vision Model Zoo</a> now provides MobileNetV2 pre-trained models. Updated existing pre-trained models to provide state-of-the-art performance on all ResNet v1, ResNet v2, and vgg16, vgg19, vgg16_bn, vgg19_bn models.</p>
<h2 id="introducing-new-clojure-bindings-with-mxnet">Introducing new Clojure bindings with MXNet</h2>
<p>MXNet now has experimental support for the Clojure programming language. The MXNet Clojure package brings state-of-the-art deep learning to the Clojure community. It enables Clojure developers to code and to execute tensor computation on multiple CPUs or GPUs. It also enables users to write seamless tensor/matrix computations with multiple GPUs in Clojure. Now users can construct and customize state-of-art deep learning models in Clojure, and apply them to tasks such as image classification and data science challenges. To start using Clojure package in MXNet, check out the <a href="http://mxnet.incubator.apache.org/api/clojure/index.html#clojure-api-tutorials">Clojure tutorials</a> and <a href="http://mxnet.incubator.apache.org/api/clojure/index.html">Clojure API documentation</a>.</p>
<h2 id="introducing-control-flow-operators">Introducing control flow operators</h2>
<p>This is the first step towards optimizing dynamic neural networks with variable computation graphs. This release adds symbolic and imperative control flow operators such as <a href="https://mxnet.incubator.apache.org/api/python/ndarray/contrib.html#mxnet.ndarray.contrib.foreach"><code>foreach</code></a>, <a href="https://mxnet.incubator.apache.org/api/python/ndarray/contrib.html#mxnet.ndarray.contrib.while_loop"><code>while_loop</code></a> and <a href="https://mxnet.incubator.apache.org/api/python/ndarray/contrib.html#mxnet.ndarray.contrib.cond"><code>cond</code></a>. To learn more about how to use these operators, check out the <a href="https://mxnet.incubator.apache.org/tutorials/control_flow/ControlFlowTutorial.html">Control Flow Operators tutorial</a>.</p>
<h2 id="performance-improvements">Performance improvements</h2>
<p><strong>TensorRT runtime integration</strong>: <a href="https://developer.nvidia.com/tensorrt">TensorRT</a> provides significant acceleration of model inference on NVIDIA GPUs compared to running the full graph in MXNet using unfused GPU operators. In addition to faster fp32 inference, <a href="https://developer.nvidia.com/tensorrt">TensorRT</a> optimizes fp16 inference and is capable of int8 inference (provided the quantization steps are performed). Besides increasing throughput, <a href="https://developer.nvidia.com/tensorrt">TensorRT</a> significantly reduces inference latency, especially for small batches. With 1.3 release, MXNet introduces the <a href="http://mxnet.incubator.apache.org/tutorials/tensorrt/inference_with_trt.html">runtime integration of TensorRT (experimental)</a>, in order to accelerate inference. Follow the <a href="https://cwiki.apache.org/confluence/display/MXNET/How+to+use+MXNet-TensorRT+integration">MXNet-TensorRT article</a> on the <a href="https://cwiki.apache.org/confluence/display/MXNET/MXNet+Home">MXNet developer wiki</a> to learn more about how to use this feature.</p>
<p><strong>MKL-DNN enhancements</strong>: <a href="https://01.org/mkl-dnn">MKL-DNN</a> is an open source library from Intel that contains a set of CPU-optimized deep learning operators. In the previous release, MXNet introduced <a href="http://mxnet.apache.org/faq/perf.html?highlight=kmp_affinity#intel-cpu">integration with MKL-DNN</a> to accelerate training and inference execution on CPU. With 1.3 release, we have increased support for these activation functions: <code>sigmoid</code>, <code>tanh</code> and <code>softrelu</code>.</p>
<h2 id="onnx-export-support">ONNX export support</h2>
<p><strong>Export MXNet models to ONNX format</strong>: MXNet 1.2 provided users a way to import ONNX models into MXNet for inference. More details are available in <a href="https://medium.com/apache-mxnet/mxnet-1-2-adds-built-in-support-for-onnx-e2c7450ffc28">this ONNX blog post</a>. With the latest 1.3 release, users can now export MXNet models into ONNX format and import those models into other deep learning frameworks for inference! Check out the <a href="http://mxnet.incubator.apache.org/tutorials/onnx/export_mxnet_to_onnx.html">MXNet to ONNX exporter tutorial</a> to learn more about how to use the <a href="http://mxnet.incubator.apache.org/api/python/contrib/onnx.html">mxnet.contrib.onnx</a> API.</p>
<h2 id="other-experimental-features">Other experimental features</h2>
<ol type="1">
<li>
<p>Apart from what we have covered above, MXNet now has support for:</p>
</li>
<li>
<p>A new memory pool type for GPU memory which is more suitable for all the workloads with dynamic-shape inputs and outputs. Set an environment variable as <code>MXNET_GPU_MEM_POOL_TYPE=Round</code> to enable this feature. <a href="https://cwiki.apache.org/confluence/display/MXNET/Single+machine+All+Reduce+Topology-aware+Communication">Topology-aware Allreduce approach</a> for single-machine GPU training. Train up to <a href="https://cwiki.apache.org/confluence/display/MXNET/Single+machine+All+Reduce+Topology-aware+Communication#SinglemachineAllReduceTopology-awareCommunication-End-to-EndResults">6.6x and 5.9x faster on AlexNet and VGG</a> compared to MXNet 1.2. Activate this feature using the <a href="https://mxnet.incubator.apache.org/faq/env_var.html#control-the-data-communication">&ldquo;control the data communication&rdquo; environmental variables</a>.</p>
</li>
<li>
<p><a href="https://cwiki.apache.org/confluence/display/MXNET/Scala+Type-safe+API+Design+Doc">Improved Scala APIs</a> that focus on providing type safety and a better user experience. <a href="https://mxnet.incubator.apache.org/api/scala/docs/index.html#org.apache.mxnet.SymbolAPI$"><code>Symbol.api</code></a> and <a href="https://mxnet.incubator.apache.org/api/scala/docs/index.html#org.apache.mxnet.NDArrayAPI$"><code>NDArray.api</code></a> bring a new set of functions that have a complete signature. The documentation for all of the arguments also integrates directly with IntelliJ IDEA. The new and improved Scala examples demonstrate usage of these new APIs.</p>
</li>
</ol>
<p>Check out further details on these features in <a href="https://github.com/apache/incubator-mxnet/releases/tag/1.3.0">full release notes</a>.</p>
<h2 id="maintenance-improvements">Maintenance improvements</h2>
<p>In addition to adding and extending new functionalities, the release also focusses on stability and refinements.</p>
<p>The community fixed <a href="https://github.com/apache/incubator-mxnet/projects/9">130 unstable tests</a> improving MXNet&rsquo;s stability and reliability. The <a href="https://github.com/apache/incubator-mxnet/pull/11626">MXNet Model Backwards Compatibility Checker</a> was introduced. This is an automated test on MXNet&rsquo;s continuous integration platform that verifies saved models&rsquo; backward compatibility. This helps ensure that models created with older versions of MXNet can be loaded and used with the newer versions.</p>
<h2 id="getting-started-with-mxnet">Getting started with MXNet</h2>
<p>Getting started with MXNet is simple, visit the <a href="https://mxnet.incubator.apache.org/install/index.html?platform=Linux&amp;language=Python&amp;processor=CPU">install page</a> to get started. <a href="https://pypi.org/project/mxnet/">PyPI packages</a> are available to install for Windows, Linux, and Mac.</p>
<p>To learn more about MXNet Gluon package and deep learning, you can follow our <a href="https://medium.com/apache-mxnet/mxnet-gluon-in-60-minutes-3d49eccaf266">60-minute crash course</a>, and then later complete <a href="https://gluon.mxnet.io/">this comprehensive set of tutorials</a>, which covers everything from an introduction to deep learning to how to implement cutting-edge neural network models. You can also check out lots of material on <a href="https://mxnet.incubator.apache.org/tutorials/index.html">MXNet tutorials</a>, <a href="https://medium.com/apache-mxnet">MXNet blog posts</a> (<a href="https://zh.mxnet.io/">中文</a>), <a href="https://www.youtube.com/channel/UCQua2ZAkbr_Shsgfk1LCy6A">MXNet YouTube channel</a> (<a href="http://www.youtube.com/c/MXNetGluon%E4%B8%AD%E6%96%87%E9%A2%91%E9%81%93">中文</a>). Have fun with MXNet 1.3.0!</p>
<h2 id="acknowledgments">Acknowledgments</h2>
<p>We would like to thank everyone who contributed to the 1.3.0 release:</p>
<p>Aaron Markham, Abhinav Sharma, access2rohit, Alex Li, Alexander Alexandrov, Alexander Zai, Amol Lele, Andrew Ayres, Anirudh Acharya, Anirudh Subramanian, Ankit Khedia, Anton Chernov, aplikaplik, Arunkumar V Ramanan, Asmus Hetzel, Aston Zhang, bl0, Ben Kamphaus, brli, Burin Choomnuan, Burness Duan, Caenorst, Cliff Woolley, Carin Meier, cclauss, Carl Tsai, Chance Bair, chinakook, Chudong Tian, ciyong, ctcyang, Da Zheng, Dang Trung Kien, Deokjae Lee, Dick Carter, Didier A., Eric Junyuan Xie, Faldict, Felix Hieber, Francisco Facioni, Frank Liu, Gnanesh, Hagay Lupesko, Haibin Lin, Hang Zhang, Hao Jin, Hao Li, Haozhi Qi, hasanmua, Hu Shiwen, Huilin Qu, Indhu Bharathi, Istvan Fehervari, JackieWu, Jake Lee, James MacGlashan, jeremiedb, Jerry Zhang, Jian Guo, Jin Huang, jimdunn, Jingbei Li, Jun Wu, Kalyanee Chendke, Kellen Sunderland, Kovas Boguta, kpmurali, Kurman Karabukaev, Lai Wei, Leonard Lausen, luobao-intel, Junru Shao, Lianmin Zheng, Lin Yuan, lufenamazon, Marco de Abreu, Marek Kolodziej, Manu Seth, Matthew Brookhart, Milan Desai, Mingkun Huang, miteshyh, Mu Li, Nan Zhu, Naveen Swamy, Nehal J Wani, PatricZhao, Paul Stadig, Pedro Larroy, perdasilva, Philip Hyunsu Cho, Pishen Tsai, Piyush Ghai, Pracheer Gupta, Przemyslaw Tredak, Qiang Kou, Qing Lan, qiuhan, Rahul Huilgol, Rakesh Vasudevan, Ray Zhang, Robert Stone, Roshani Nagmote, Sam Skalicky, Sandeep Krishnamurthy, Sebastian Bodenstein, Sergey Kolychev, Sergey Sokolov, Sheng Zha, Shen Zhu, Sheng-Ying, Shuai Zheng, slitsey, Simon, Sina Afrooze, Soji Adeshina, solin319, Soonhwan-Kwon, starimpact, Steffen Rochel, Taliesin Beynon, Tao Lv, Thom Lane, Thomas Delteil, Tianqi Chen, Todd Sundsted, Tong He, Vandana Kannan, vdantu, Vishaal Kapoor, wangzhe, xcgoner, Wei Wu, Wen-Yang Chu, Xingjian Shi, Xinyu Chen, yifeim, Yizhi Liu, YouRancestor, Yuelin Zhang, Yu-Xiang Wang, Yuan Tang, Yuntao Chen, Zach Kimberg, Zhennan Qin, Zhi Zhang, zhiyuan-huang, Ziyue Huang, Ziyi Mu, Zhuo Zhang.</p>
<p>&hellip; and thanks to all of the Apache MXNet community supporters, spreading knowledge and helping to grow the community!</p>
