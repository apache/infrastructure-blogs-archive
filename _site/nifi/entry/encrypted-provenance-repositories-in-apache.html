<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Encrypted Provenance Repositories in Apache NiFi | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Encrypted Provenance Repositories in Apache NiFi" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Encrypted Provenance Repositories in Apache NiFi Andy LoPresto -&nbsp; @yolopey There has been a surprising level of Twitter demand for more security-focused NiFi blogs. I&rsquo;ll try to address this underserved market with a post about a new feature in Apache NiFi 1.2.0 &ndash; encrypted provenance repositories (NIFI-3388). As a mentor of mine often said, &ldquo;You don&rsquo;t understand something until you can teach it.&rdquo; In this article, we&rsquo;ll find out if I actually understand the code I wrote. First I&rsquo;ll give a brief explanation of the provenance feature for anyone unfamiliar with it, describe how the existing implementations work, and introduce a new option that is sure to have people very excited (no guarantee of excited people actually offered). What is Provenance? Provenance is a term we use in the NiFi community with a very specific intent. The great bastion of knowledge, Wikipedia, defines it as &ldquo;the chronology of the ownership, custody or location of a historical object.&rdquo; Substitute &ldquo;data&rdquo; for &ldquo;historical object&rdquo;, and you&rsquo;re on the right track for NiFi. A serious concern in any dataflow routing &amp; management platform is &ldquo;what happens to the data?&rdquo; and &ldquo;when?&rdquo;. Only slightly less important is &ldquo;how do we prove it?&rdquo; NiFi allows users to answer these questions by automatically recording everything that happens to your data on a very granular level. Think of &ldquo;data lineage&rdquo; as a family tree where each person has a Ken Burns biography &ndash; a very complete history of everything they did and how it relates to everyone else. Any event (data being created, ingested, routed, modified, tagged, viewed, exported, or deleted) is recorded, along with the time, the identity of the component that acted on it, where it was sent, what was changed, etc. Not only does this extensive dataset allow users to query the history of their data, it enables some other powerful features as well. Explore Users can explore the provenance chain (the directed graph representation of the provenance events) of any flowfile to review the path it followed through the data flow. This allows both broad and focused analysis of things like timing bottlenecks, system performance, and identifying critical paths. Sometimes open-ended exploration of this data will reveal or inspire potential improvements in the flow itself. Replay The provenance data is also crucial to a key feature of NiFi &ndash; allowing the user to replay flowfiles. As long as the provenance data has not been aged off and the referenced content is still available in the content repository, any flowfile can be replayed from any point in the flow. This greatly tightens the flow development lifecycle &ndash; rather than a &ldquo;build and deploy&rdquo; cycle, this encourages rapid refinement of a flow watching live (or slightly stale, but consistent) data proceed through different branches. A metaphor we use frequently in the NiFi community is that the data, much like water, is always flowing. NiFi is less about building permanent water pipes, and more like digging irrigation ditches from a constantly-flowing river. While replay supports rapid flow refinement, the open secret is that it was developed for a different reason. NiFi connects many disparate systems, and in an enterprise environment, these are often owned and managed by different teams. Sometimes, data coming from System A, managed by Alice&rsquo;s team, flows through a NiFi instance run by Norman, and then is routed to Bob&rsquo;s System B. Saturday at 0300, an urgent alert comes into Norman saying that Bob hasn&rsquo;t received data for the last 3 hours. A quick check of NiFi&rsquo;s stats shows that Alice&rsquo;s app is still producing messages on the correct Kafka topic and NiFi is consuming and delivering the data. After some troubleshooting, the RCA (root cause analysis) is that while the data was being written to a triage directory managed by Bob&rsquo;s team, their application server was down, and the triage directory has aged out half of the delivered data. In other loosely-coupled systems, the data may be lost forever. Not only does NiFi&rsquo;s provenance allow Norman to prove that the data was delivered (important for the inevitable &ldquo;blame game&rdquo; that will be played by managers and budget people on Monday), but then to reconcile the &ldquo;we&rsquo;re all on the same team&rdquo; mentality and help Bob out by replaying all the data that was lost back through the same flow. &ldquo;My CPU is a neural-net processor, a learning computer&rdquo; One of the buzzwords that you can&rsquo;t go 20 minutes without hearing today is &ldquo;machine learning&rdquo; (that&rsquo;s two words). While this means different things to different people (seriously, ask a data scientist sometime if you&rsquo;re having trouble falling asleep), it&rsquo;s generally accepted as &ldquo;the computer learning something without being explicitly programmed&rdquo;. Will Song has done some fascinating research on using NiFi provenance data for anomaly detection. Rather than examining the actual content of a flowfile (which can inhabit a broad, aschematic domain or be in a format that is not easily consumed or parsed), Will took the approach of examining the provenance metadata, which is tightly defined. By identifying anomalous data in timing, routing, etc., he could build a clustering model and predictor system. The possibilities for NiFi are impressive &ndash; from early fault detection (think identifying timing increases to predict failing external hardware or network issues), to flow recommender systems (a Markov chain lookup of the most frequent follow-on processors when Processor X is added to the flow or configured with a certain collection of attributes), to flow recovery (self-healing flows that can intelligently re-route data when a specific destination is not available or bypass an expensive enrichment step when the flow volume increases above a threshold rather than delay downstream delivery). Uphill Both Ways The Provenance Repository was originally developed to provide a basic storage facility and sequential iteration. As noted by Mark Payne in NIFI-3356: Provide a newly refactored provenance repository The Persistent Provenance Repository has been redesigned a few different times over several years. The original design for the repository was to provide storage of events and sequential iteration over those events via a Reporting Task. After that, we added the ability to compress the data so that it could be held longer. We then introduced the notion of indexing and searching via Lucene. We&rsquo;ve since made several more modifications to try to boost performance. At this point, however, the repository is still the bottleneck for many flows that handle large volumes of small FlowFiles. We need a new implementation that is based around the current goals for the repository and that can provide better throughput. The PersistentProvenanceRepository served well for a long time, and while it is still the default implementation for a vanilla installation of NiFi, Mark did amazing work building a completely redesigned backing store called the WriteAheadProvenanceRepository. Faster, Stronger, Better The WriteAheadProvenanceRepository (or WAPR from here on out) uses a write-ahead log for the backing EventStore, rather than writing directly to journal files as the PersistentProvenanceRepository did. By combining the EventStore, which simply reports back an EventIdentifier to locate the written data, with an EventIndex (powered by Apache Lucene), the two components can work in conjunction to provide high throughput and efficient querying and retrieval. Out of the box, WAPR is about 3x as fast as PPR, and this scales better as more disks and CPU resources are made available. In addition, with the Lucene EventIndex, event records are immediately available for query and retrieval, as opposed to the batch processing and ingesting done by PPR. The WAPR implementation follows the classic Java design pattern philosophy &ldquo;composition over inheritance&rdquo;, so the underlying EventStore field belonging to the repository contains RecordWriter and RecordReader members. By providing a factory for each of these fields during EventStore construction, the store is responsible for instantiating these objects when necessary, but the repository itself can inject the relevant behavior through DI (Dependency Injection)/IoC (Inversion of Control). This is a crucial decision that made implementing the encrypted version much easier and cleaner. Encrypt All The Things! That&rsquo;s 8500 characters to get to the point of this article &ndash; why and how do we encrypt the provenance repository? The why is pretty straightforward &ndash; why not? In all seriousness, provenance data contains, in addition to generic timing and routing metadata, the value of the attributes at each point in the lineage. Many of these values can be quite sensitive. While NiFi has access controls for the REST API and UI, these details are written in plaintext (or compressed via GZIP) to the backing file system. While NiFi was originally designed with the expectation that it would run on managed hardware, many users are now requesting cloud deployments, or as I call it, &ldquo;storing the crown jewels in someone else&rsquo;s safe&rdquo;. As part of an ongoing effort to harden NiFi for deployment on remote/&rdquo;untrusted&rdquo; hardware, we are continually looking for exposed surfaces. I have plans to provide an encrypted version of all three repositories (content and flowfile are the other two), and provenance seemed like the place to start, as it is the most ephemeral and the WAPR was most recently written and likely to be the cleanest, learning the most from the continual community feedback. The how isn&rsquo;t much more complicated. Because of Mark&rsquo;s clean architecture, I was able to extend the WAPR and intercept the RecordReaderFactory and RecordWriterFactory that were injected into the EventStore constructor and simply replace those with my own implementations. This means the EncryptedWriteAheadProvenanceRepository file is a grand total of 159 lines (the unit test is 346). The classes we will actually examine here are the KeyProvider interface and its two (current) implementations StaticKeyProvider and FileBasedKeyProvider, the ProvenanceEventEncryptor interface and its sole (current) implementation AESProvenanceEventEncryptor, and the new implementations of EncryptedSchemaRecordReader and EncryptedSchemaRecordWriter. Vinz Clortho, Keymaster of Gozer Providing access to encryption keys is one of the great challenges of data protection. Handling key material protection and availability, as well as key migration, rotation, and expiry are broad and complicated topics. For the initial implementation, I focused on two providers &ndash; StaticKeyProvider and FileBasedKeyProvider. The common interface is quite simple, providing four methods (Javadoc elided): public interface KeyProvider { SecretKey getKey(String keyId) throws KeyManagementException; boolean keyExists(String keyId); List getAvailableKeyIds(); boolean addKey(String keyId, SecretKey key) throws OperationNotSupportedException, KeyManagementException; } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; The methods are fairly self-documenting &ndash; by storing the keyId alongside the encrypted data, we are able to encrypt records with varying keys over time, reducing the risk of key compromise to be more granular. The static implementation simply contains a HashMap of key IDs to SecretKey objects. It is initialized by reading the $NIFI_HOME/conf/nifi.properties file and reading the provided keys. These keys can be in plaintext or encrypted (along with any other sensitive configuration value) using the $ ./bin/encrypt-config.sh tool provided by the NiFi Toolkit. if (StaticKeyProvider.class.getName().equals(implementationClassName)) { // Get all the keys (map) from config if (CryptoUtils.isValidKeyProvider(implementationClassName, config.getKeyProviderLocation(), config.getKeyId(), config.getEncryptionKeys())) { Map&lt;String, SecretKey&gt; formedKeys = config.getEncryptionKeys().entrySet().stream() .collect(Collectors.toMap( Map.Entry::getKey, e -&gt; { try { return CryptoUtils.formKeyFromHex(e.getValue()); } catch (KeyManagementException e1) { // This should never happen because the hex has already been validated logger.error(&quot;Encountered an error: &quot;, e1); return null; } })); keyProvider = new StaticKeyProvider(formedKeys); } else { final String msg = &quot;The StaticKeyProvider definition is not valid&quot;; logger.error(msg); throw new KeyManagementException(msg); } } The file-based implementation expands on this by reading from a separate file (which can be located on a remote volume, etc.) and reading a key-value listing of key IDs and keys. The key material is Base64-encoded, encrypted and signed (using AES G/CM) hexadecimal representations. Currently neither implementation supports the addKey method, but in the future, I expect a PKCS11-compatible HSM (Hardware Security Module) provider as well as bridges to sensitive value containers like Square&rsquo;s KeyWhiz and Hashicorp&rsquo;s Vault. The KeyProvider interface and the implementations are also contained within the provenance package and module, but I expect to refactor them out to a framework-level service as part of changes in NIFI-3890: Create Key Management Controller Service. Superfluous Interface Development I&rsquo;m kind of joking in the section title &ndash; the ProvenanceEventEncryptor interface has only four methods and only one implementation, but I wanted to ensure it could be cleanly extended in the future. public interface ProvenanceEventEncryptor { void initialize(KeyProvider keyProvider) throws KeyManagementException; byte[] encrypt(byte[] plainRecord, String recordId, String keyId) throws EncryptionException; byte[] decrypt(byte[] encryptedRecord, String recordId) throws EncryptionException; String getNextKeyId() throws KeyManagementException; } Warning: Crypto nerd stuff ahead The provided implementation uses AES (Advanced Encryption Standard) in G/CM (Galois/Counter Mode). AES is a symmetric encryption cipher, a variant of the Rijndael cipher, a substitution-permutation network, with a fixed block size of 128 bits and a key length of 128, 192, or 256 bits. This is in contrast to one of its most-common predecessors, DES (Data Encryption Standard), which used a Feistel network. If you want more detail than this, buy me a beer sometime and sit back. If you absolutely do not want more detail, consider yourself sane, and accept that it is sufficient for what we are covering here. In addition to the cipher selection, G/CM is an AEAD (Authenticated Encryption with Associated Data) mode of operation, which means that not only does it provide confidentiality (only people with the secret key can decrypt the cipher text), it also provides integrity (the cipher text cannot be undetectably modified by a party without the secret key). This is crucial for authenticity, especially in the application of provenance data, and common modes like ECB, CBC, and CTR do not provide this feature. An alternative construction would be to use a hash-based message authentication code (HMAC) like HMAC/SHA-256 or BLAKE2 with a separate key over the cipher text, but G/CM satisfies the requirements without a separate key and operation, so it&rsquo;s a personal favorite for scenarios like this. In the future, there may be ChaCha20-Poly1305 implementations for better performance, RSA or GPG implementations for asymmetric encryption using public-private key pairs, or even HSM implementations for encryption performed on remote/network-attached encryption appliances with completely contained keys. Regardless, the actual interface contract is quite straightforward. The encrypt() and decrypt() methods accept arbitrary byte[] messages and some metadata, check that the specified key exists and is valid, and then perform the desired cryptographic operation (including serializing/deserializing the encryption metadata) and return the resulting byte[]. How to Lose a Super Bowl and Other Great Interceptions We now turn to the pieces that actually make the repository work (everything up to this point has been pretty standard point-n&rsquo;-click cryptography). The EncryptedSchemaRecordWriter and EncryptedSchemaRecordReader classes that we define will be wrappers extending and adding bonus functionality to the EventIdFirstSchemaRecord* classes. By providing different factories to the EventStore constructor as mentioned earlier, we&rsquo;ll provide compatible instances which intercept the byte[] serialization/deserialization and also encrypt/decrypt the data. This means we don&rsquo;t need any additional work to handle event indexing/querying, compression, etc. This saves us about 800 lines of repetition. The encryption is easy &ndash; as stated above, the record is already serialized to a byte[] by the existing record writer, and that byte[] is handed to the ProvenanceEventEncryptor already described. The EncryptionMetadata (key ID, algorithm, IV, version, and cipher text length) is also serialized and prepended to allow on-demand retrieval and decryption (as a good crypto student, you did know we were going to use unique and non-predictable IVs for every event record, right?). This also allows records encrypted by two different keys to reside side-by-side in the repository with no ill effects. The decryption operation is simply the inverse &ndash; retrieve the blob of data identified by the event ID (and read using random access via stored offset, not sequential reads) from the repository, retrieve the key and decrypt it, and then pass the plaintext serialized form to the delegated readRecord() method to be rehydrated into an object. Configuring The App As described in the Apache NiFi User Guide and Apache NiFi Admin Guide (light reading for insomniacs), the encrypted provenance repository does need a little bit of configuration in nifi.properties. Every property is verbosely described on that page, but here is the simplest valid configuration: nifi.provenance.repository.implementation=org.apache.nifi.provenance.EncryptedWriteAheadProvenanceRepository nifi.provenance.repository.debug.frequency=100 nifi.provenance.repository.encryption.key.provider.implementation=org.apache.nifi.provenance.StaticKeyProvider nifi.provenance.repository.encryption.key.provider.location= nifi.provenance.repository.encryption.key.id=Key1 nifi.provenance.repository.encryption.key=0123456789ABCDEFFEDCBA98765432100123456789ABCDEFFEDCBA9876543210 Does It Actually Work? Encryption always adds costs to any software. I ran some basic performance tests to provide some metrics in PR 1686, and I found that with low flow volume, using an encrypted provenance repository was more than twice as fast as the old PPR and almost identical to the new WAPR. This led me to double-take and question &ldquo;is it actually encrypting anything?&rdquo; I used my handy Hex Fiend to examine the actual files on disk to ensure the data was being encrypted. Here you can see the EncryptionMetadata being serialized via Java serialization and the cipher text of the event record following. Ok, so once the data is encrypted, is it still useful in the app? Sure enough, a provenance query returns perfectly human-readable records through the REST API and the UI. With high flow volume, I did &ldquo;luckily&rdquo; see more performance cost. Still, running with an encrypted provenance repository, the flow could handle about 13k events per second. While the flow performance was slower than the original PPR, the provenance queries were almost identical (and sometimes even faster). Long story short, if you like writing encrypted provenance data to your disk, NiFi&rsquo;s got you covered. Back to the Future There is still plenty of work to do here. Repository implementation migration is not as smooth as it could be, tools for migration and key rotation would be nice, the KeyProvider and other services can be extracted to the framework level, content and flowfile repository implementations are still necessary, and provenance records themselves do not have cryptographic signatures over their content (for chain of custody, governance, and integrity guarantees). The User Guide has a section devoted just to &ldquo;Potential Issues&rdquo; involved. As provenance data isn&rsquo;t intended to be stored long term in NiFi, but offloaded to a glacial store like Apache Atlas, these aren&rsquo;t priority issues. I would recommend you try the encrypted provenance repository on non-business-critical data at first, but in our tests, it has been pretty stable." />
<meta property="og:description" content="Encrypted Provenance Repositories in Apache NiFi Andy LoPresto -&nbsp; @yolopey There has been a surprising level of Twitter demand for more security-focused NiFi blogs. I&rsquo;ll try to address this underserved market with a post about a new feature in Apache NiFi 1.2.0 &ndash; encrypted provenance repositories (NIFI-3388). As a mentor of mine often said, &ldquo;You don&rsquo;t understand something until you can teach it.&rdquo; In this article, we&rsquo;ll find out if I actually understand the code I wrote. First I&rsquo;ll give a brief explanation of the provenance feature for anyone unfamiliar with it, describe how the existing implementations work, and introduce a new option that is sure to have people very excited (no guarantee of excited people actually offered). What is Provenance? Provenance is a term we use in the NiFi community with a very specific intent. The great bastion of knowledge, Wikipedia, defines it as &ldquo;the chronology of the ownership, custody or location of a historical object.&rdquo; Substitute &ldquo;data&rdquo; for &ldquo;historical object&rdquo;, and you&rsquo;re on the right track for NiFi. A serious concern in any dataflow routing &amp; management platform is &ldquo;what happens to the data?&rdquo; and &ldquo;when?&rdquo;. Only slightly less important is &ldquo;how do we prove it?&rdquo; NiFi allows users to answer these questions by automatically recording everything that happens to your data on a very granular level. Think of &ldquo;data lineage&rdquo; as a family tree where each person has a Ken Burns biography &ndash; a very complete history of everything they did and how it relates to everyone else. Any event (data being created, ingested, routed, modified, tagged, viewed, exported, or deleted) is recorded, along with the time, the identity of the component that acted on it, where it was sent, what was changed, etc. Not only does this extensive dataset allow users to query the history of their data, it enables some other powerful features as well. Explore Users can explore the provenance chain (the directed graph representation of the provenance events) of any flowfile to review the path it followed through the data flow. This allows both broad and focused analysis of things like timing bottlenecks, system performance, and identifying critical paths. Sometimes open-ended exploration of this data will reveal or inspire potential improvements in the flow itself. Replay The provenance data is also crucial to a key feature of NiFi &ndash; allowing the user to replay flowfiles. As long as the provenance data has not been aged off and the referenced content is still available in the content repository, any flowfile can be replayed from any point in the flow. This greatly tightens the flow development lifecycle &ndash; rather than a &ldquo;build and deploy&rdquo; cycle, this encourages rapid refinement of a flow watching live (or slightly stale, but consistent) data proceed through different branches. A metaphor we use frequently in the NiFi community is that the data, much like water, is always flowing. NiFi is less about building permanent water pipes, and more like digging irrigation ditches from a constantly-flowing river. While replay supports rapid flow refinement, the open secret is that it was developed for a different reason. NiFi connects many disparate systems, and in an enterprise environment, these are often owned and managed by different teams. Sometimes, data coming from System A, managed by Alice&rsquo;s team, flows through a NiFi instance run by Norman, and then is routed to Bob&rsquo;s System B. Saturday at 0300, an urgent alert comes into Norman saying that Bob hasn&rsquo;t received data for the last 3 hours. A quick check of NiFi&rsquo;s stats shows that Alice&rsquo;s app is still producing messages on the correct Kafka topic and NiFi is consuming and delivering the data. After some troubleshooting, the RCA (root cause analysis) is that while the data was being written to a triage directory managed by Bob&rsquo;s team, their application server was down, and the triage directory has aged out half of the delivered data. In other loosely-coupled systems, the data may be lost forever. Not only does NiFi&rsquo;s provenance allow Norman to prove that the data was delivered (important for the inevitable &ldquo;blame game&rdquo; that will be played by managers and budget people on Monday), but then to reconcile the &ldquo;we&rsquo;re all on the same team&rdquo; mentality and help Bob out by replaying all the data that was lost back through the same flow. &ldquo;My CPU is a neural-net processor, a learning computer&rdquo; One of the buzzwords that you can&rsquo;t go 20 minutes without hearing today is &ldquo;machine learning&rdquo; (that&rsquo;s two words). While this means different things to different people (seriously, ask a data scientist sometime if you&rsquo;re having trouble falling asleep), it&rsquo;s generally accepted as &ldquo;the computer learning something without being explicitly programmed&rdquo;. Will Song has done some fascinating research on using NiFi provenance data for anomaly detection. Rather than examining the actual content of a flowfile (which can inhabit a broad, aschematic domain or be in a format that is not easily consumed or parsed), Will took the approach of examining the provenance metadata, which is tightly defined. By identifying anomalous data in timing, routing, etc., he could build a clustering model and predictor system. The possibilities for NiFi are impressive &ndash; from early fault detection (think identifying timing increases to predict failing external hardware or network issues), to flow recommender systems (a Markov chain lookup of the most frequent follow-on processors when Processor X is added to the flow or configured with a certain collection of attributes), to flow recovery (self-healing flows that can intelligently re-route data when a specific destination is not available or bypass an expensive enrichment step when the flow volume increases above a threshold rather than delay downstream delivery). Uphill Both Ways The Provenance Repository was originally developed to provide a basic storage facility and sequential iteration. As noted by Mark Payne in NIFI-3356: Provide a newly refactored provenance repository The Persistent Provenance Repository has been redesigned a few different times over several years. The original design for the repository was to provide storage of events and sequential iteration over those events via a Reporting Task. After that, we added the ability to compress the data so that it could be held longer. We then introduced the notion of indexing and searching via Lucene. We&rsquo;ve since made several more modifications to try to boost performance. At this point, however, the repository is still the bottleneck for many flows that handle large volumes of small FlowFiles. We need a new implementation that is based around the current goals for the repository and that can provide better throughput. The PersistentProvenanceRepository served well for a long time, and while it is still the default implementation for a vanilla installation of NiFi, Mark did amazing work building a completely redesigned backing store called the WriteAheadProvenanceRepository. Faster, Stronger, Better The WriteAheadProvenanceRepository (or WAPR from here on out) uses a write-ahead log for the backing EventStore, rather than writing directly to journal files as the PersistentProvenanceRepository did. By combining the EventStore, which simply reports back an EventIdentifier to locate the written data, with an EventIndex (powered by Apache Lucene), the two components can work in conjunction to provide high throughput and efficient querying and retrieval. Out of the box, WAPR is about 3x as fast as PPR, and this scales better as more disks and CPU resources are made available. In addition, with the Lucene EventIndex, event records are immediately available for query and retrieval, as opposed to the batch processing and ingesting done by PPR. The WAPR implementation follows the classic Java design pattern philosophy &ldquo;composition over inheritance&rdquo;, so the underlying EventStore field belonging to the repository contains RecordWriter and RecordReader members. By providing a factory for each of these fields during EventStore construction, the store is responsible for instantiating these objects when necessary, but the repository itself can inject the relevant behavior through DI (Dependency Injection)/IoC (Inversion of Control). This is a crucial decision that made implementing the encrypted version much easier and cleaner. Encrypt All The Things! That&rsquo;s 8500 characters to get to the point of this article &ndash; why and how do we encrypt the provenance repository? The why is pretty straightforward &ndash; why not? In all seriousness, provenance data contains, in addition to generic timing and routing metadata, the value of the attributes at each point in the lineage. Many of these values can be quite sensitive. While NiFi has access controls for the REST API and UI, these details are written in plaintext (or compressed via GZIP) to the backing file system. While NiFi was originally designed with the expectation that it would run on managed hardware, many users are now requesting cloud deployments, or as I call it, &ldquo;storing the crown jewels in someone else&rsquo;s safe&rdquo;. As part of an ongoing effort to harden NiFi for deployment on remote/&rdquo;untrusted&rdquo; hardware, we are continually looking for exposed surfaces. I have plans to provide an encrypted version of all three repositories (content and flowfile are the other two), and provenance seemed like the place to start, as it is the most ephemeral and the WAPR was most recently written and likely to be the cleanest, learning the most from the continual community feedback. The how isn&rsquo;t much more complicated. Because of Mark&rsquo;s clean architecture, I was able to extend the WAPR and intercept the RecordReaderFactory and RecordWriterFactory that were injected into the EventStore constructor and simply replace those with my own implementations. This means the EncryptedWriteAheadProvenanceRepository file is a grand total of 159 lines (the unit test is 346). The classes we will actually examine here are the KeyProvider interface and its two (current) implementations StaticKeyProvider and FileBasedKeyProvider, the ProvenanceEventEncryptor interface and its sole (current) implementation AESProvenanceEventEncryptor, and the new implementations of EncryptedSchemaRecordReader and EncryptedSchemaRecordWriter. Vinz Clortho, Keymaster of Gozer Providing access to encryption keys is one of the great challenges of data protection. Handling key material protection and availability, as well as key migration, rotation, and expiry are broad and complicated topics. For the initial implementation, I focused on two providers &ndash; StaticKeyProvider and FileBasedKeyProvider. The common interface is quite simple, providing four methods (Javadoc elided): public interface KeyProvider { SecretKey getKey(String keyId) throws KeyManagementException; boolean keyExists(String keyId); List getAvailableKeyIds(); boolean addKey(String keyId, SecretKey key) throws OperationNotSupportedException, KeyManagementException; } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; The methods are fairly self-documenting &ndash; by storing the keyId alongside the encrypted data, we are able to encrypt records with varying keys over time, reducing the risk of key compromise to be more granular. The static implementation simply contains a HashMap of key IDs to SecretKey objects. It is initialized by reading the $NIFI_HOME/conf/nifi.properties file and reading the provided keys. These keys can be in plaintext or encrypted (along with any other sensitive configuration value) using the $ ./bin/encrypt-config.sh tool provided by the NiFi Toolkit. if (StaticKeyProvider.class.getName().equals(implementationClassName)) { // Get all the keys (map) from config if (CryptoUtils.isValidKeyProvider(implementationClassName, config.getKeyProviderLocation(), config.getKeyId(), config.getEncryptionKeys())) { Map&lt;String, SecretKey&gt; formedKeys = config.getEncryptionKeys().entrySet().stream() .collect(Collectors.toMap( Map.Entry::getKey, e -&gt; { try { return CryptoUtils.formKeyFromHex(e.getValue()); } catch (KeyManagementException e1) { // This should never happen because the hex has already been validated logger.error(&quot;Encountered an error: &quot;, e1); return null; } })); keyProvider = new StaticKeyProvider(formedKeys); } else { final String msg = &quot;The StaticKeyProvider definition is not valid&quot;; logger.error(msg); throw new KeyManagementException(msg); } } The file-based implementation expands on this by reading from a separate file (which can be located on a remote volume, etc.) and reading a key-value listing of key IDs and keys. The key material is Base64-encoded, encrypted and signed (using AES G/CM) hexadecimal representations. Currently neither implementation supports the addKey method, but in the future, I expect a PKCS11-compatible HSM (Hardware Security Module) provider as well as bridges to sensitive value containers like Square&rsquo;s KeyWhiz and Hashicorp&rsquo;s Vault. The KeyProvider interface and the implementations are also contained within the provenance package and module, but I expect to refactor them out to a framework-level service as part of changes in NIFI-3890: Create Key Management Controller Service. Superfluous Interface Development I&rsquo;m kind of joking in the section title &ndash; the ProvenanceEventEncryptor interface has only four methods and only one implementation, but I wanted to ensure it could be cleanly extended in the future. public interface ProvenanceEventEncryptor { void initialize(KeyProvider keyProvider) throws KeyManagementException; byte[] encrypt(byte[] plainRecord, String recordId, String keyId) throws EncryptionException; byte[] decrypt(byte[] encryptedRecord, String recordId) throws EncryptionException; String getNextKeyId() throws KeyManagementException; } Warning: Crypto nerd stuff ahead The provided implementation uses AES (Advanced Encryption Standard) in G/CM (Galois/Counter Mode). AES is a symmetric encryption cipher, a variant of the Rijndael cipher, a substitution-permutation network, with a fixed block size of 128 bits and a key length of 128, 192, or 256 bits. This is in contrast to one of its most-common predecessors, DES (Data Encryption Standard), which used a Feistel network. If you want more detail than this, buy me a beer sometime and sit back. If you absolutely do not want more detail, consider yourself sane, and accept that it is sufficient for what we are covering here. In addition to the cipher selection, G/CM is an AEAD (Authenticated Encryption with Associated Data) mode of operation, which means that not only does it provide confidentiality (only people with the secret key can decrypt the cipher text), it also provides integrity (the cipher text cannot be undetectably modified by a party without the secret key). This is crucial for authenticity, especially in the application of provenance data, and common modes like ECB, CBC, and CTR do not provide this feature. An alternative construction would be to use a hash-based message authentication code (HMAC) like HMAC/SHA-256 or BLAKE2 with a separate key over the cipher text, but G/CM satisfies the requirements without a separate key and operation, so it&rsquo;s a personal favorite for scenarios like this. In the future, there may be ChaCha20-Poly1305 implementations for better performance, RSA or GPG implementations for asymmetric encryption using public-private key pairs, or even HSM implementations for encryption performed on remote/network-attached encryption appliances with completely contained keys. Regardless, the actual interface contract is quite straightforward. The encrypt() and decrypt() methods accept arbitrary byte[] messages and some metadata, check that the specified key exists and is valid, and then perform the desired cryptographic operation (including serializing/deserializing the encryption metadata) and return the resulting byte[]. How to Lose a Super Bowl and Other Great Interceptions We now turn to the pieces that actually make the repository work (everything up to this point has been pretty standard point-n&rsquo;-click cryptography). The EncryptedSchemaRecordWriter and EncryptedSchemaRecordReader classes that we define will be wrappers extending and adding bonus functionality to the EventIdFirstSchemaRecord* classes. By providing different factories to the EventStore constructor as mentioned earlier, we&rsquo;ll provide compatible instances which intercept the byte[] serialization/deserialization and also encrypt/decrypt the data. This means we don&rsquo;t need any additional work to handle event indexing/querying, compression, etc. This saves us about 800 lines of repetition. The encryption is easy &ndash; as stated above, the record is already serialized to a byte[] by the existing record writer, and that byte[] is handed to the ProvenanceEventEncryptor already described. The EncryptionMetadata (key ID, algorithm, IV, version, and cipher text length) is also serialized and prepended to allow on-demand retrieval and decryption (as a good crypto student, you did know we were going to use unique and non-predictable IVs for every event record, right?). This also allows records encrypted by two different keys to reside side-by-side in the repository with no ill effects. The decryption operation is simply the inverse &ndash; retrieve the blob of data identified by the event ID (and read using random access via stored offset, not sequential reads) from the repository, retrieve the key and decrypt it, and then pass the plaintext serialized form to the delegated readRecord() method to be rehydrated into an object. Configuring The App As described in the Apache NiFi User Guide and Apache NiFi Admin Guide (light reading for insomniacs), the encrypted provenance repository does need a little bit of configuration in nifi.properties. Every property is verbosely described on that page, but here is the simplest valid configuration: nifi.provenance.repository.implementation=org.apache.nifi.provenance.EncryptedWriteAheadProvenanceRepository nifi.provenance.repository.debug.frequency=100 nifi.provenance.repository.encryption.key.provider.implementation=org.apache.nifi.provenance.StaticKeyProvider nifi.provenance.repository.encryption.key.provider.location= nifi.provenance.repository.encryption.key.id=Key1 nifi.provenance.repository.encryption.key=0123456789ABCDEFFEDCBA98765432100123456789ABCDEFFEDCBA9876543210 Does It Actually Work? Encryption always adds costs to any software. I ran some basic performance tests to provide some metrics in PR 1686, and I found that with low flow volume, using an encrypted provenance repository was more than twice as fast as the old PPR and almost identical to the new WAPR. This led me to double-take and question &ldquo;is it actually encrypting anything?&rdquo; I used my handy Hex Fiend to examine the actual files on disk to ensure the data was being encrypted. Here you can see the EncryptionMetadata being serialized via Java serialization and the cipher text of the event record following. Ok, so once the data is encrypted, is it still useful in the app? Sure enough, a provenance query returns perfectly human-readable records through the REST API and the UI. With high flow volume, I did &ldquo;luckily&rdquo; see more performance cost. Still, running with an encrypted provenance repository, the flow could handle about 13k events per second. While the flow performance was slower than the original PPR, the provenance queries were almost identical (and sometimes even faster). Long story short, if you like writing encrypted provenance data to your disk, NiFi&rsquo;s got you covered. Back to the Future There is still plenty of work to do here. Repository implementation migration is not as smooth as it could be, tools for migration and key rotation would be nice, the KeyProvider and other services can be extracted to the framework level, content and flowfile repository implementations are still necessary, and provenance records themselves do not have cryptographic signatures over their content (for chain of custody, governance, and integrity guarantees). The User Guide has a section devoted just to &ldquo;Potential Issues&rdquo; involved. As provenance data isn&rsquo;t intended to be stored long term in NiFi, but offloaded to a glacial store like Apache Atlas, these aren&rsquo;t priority issues. I would recommend you try the encrypted provenance repository on non-business-critical data at first, but in our tests, it has been pretty stable." />
<link rel="canonical" href="http://localhost:4000/nifi/entry/encrypted-provenance-repositories-in-apache" />
<meta property="og:url" content="http://localhost:4000/nifi/entry/encrypted-provenance-repositories-in-apache" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-07-11T17:28:14-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Encrypted Provenance Repositories in Apache NiFi" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2017-07-11T17:28:14-04:00","datePublished":"2017-07-11T17:28:14-04:00","description":"Encrypted Provenance Repositories in Apache NiFi Andy LoPresto -&nbsp; @yolopey There has been a surprising level of Twitter demand for more security-focused NiFi blogs. I&rsquo;ll try to address this underserved market with a post about a new feature in Apache NiFi 1.2.0 &ndash; encrypted provenance repositories (NIFI-3388). As a mentor of mine often said, &ldquo;You don&rsquo;t understand something until you can teach it.&rdquo; In this article, we&rsquo;ll find out if I actually understand the code I wrote. First I&rsquo;ll give a brief explanation of the provenance feature for anyone unfamiliar with it, describe how the existing implementations work, and introduce a new option that is sure to have people very excited (no guarantee of excited people actually offered). What is Provenance? Provenance is a term we use in the NiFi community with a very specific intent. The great bastion of knowledge, Wikipedia, defines it as &ldquo;the chronology of the ownership, custody or location of a historical object.&rdquo; Substitute &ldquo;data&rdquo; for &ldquo;historical object&rdquo;, and you&rsquo;re on the right track for NiFi. A serious concern in any dataflow routing &amp; management platform is &ldquo;what happens to the data?&rdquo; and &ldquo;when?&rdquo;. Only slightly less important is &ldquo;how do we prove it?&rdquo; NiFi allows users to answer these questions by automatically recording everything that happens to your data on a very granular level. Think of &ldquo;data lineage&rdquo; as a family tree where each person has a Ken Burns biography &ndash; a very complete history of everything they did and how it relates to everyone else. Any event (data being created, ingested, routed, modified, tagged, viewed, exported, or deleted) is recorded, along with the time, the identity of the component that acted on it, where it was sent, what was changed, etc. Not only does this extensive dataset allow users to query the history of their data, it enables some other powerful features as well. Explore Users can explore the provenance chain (the directed graph representation of the provenance events) of any flowfile to review the path it followed through the data flow. This allows both broad and focused analysis of things like timing bottlenecks, system performance, and identifying critical paths. Sometimes open-ended exploration of this data will reveal or inspire potential improvements in the flow itself. Replay The provenance data is also crucial to a key feature of NiFi &ndash; allowing the user to replay flowfiles. As long as the provenance data has not been aged off and the referenced content is still available in the content repository, any flowfile can be replayed from any point in the flow. This greatly tightens the flow development lifecycle &ndash; rather than a &ldquo;build and deploy&rdquo; cycle, this encourages rapid refinement of a flow watching live (or slightly stale, but consistent) data proceed through different branches. A metaphor we use frequently in the NiFi community is that the data, much like water, is always flowing. NiFi is less about building permanent water pipes, and more like digging irrigation ditches from a constantly-flowing river. While replay supports rapid flow refinement, the open secret is that it was developed for a different reason. NiFi connects many disparate systems, and in an enterprise environment, these are often owned and managed by different teams. Sometimes, data coming from System A, managed by Alice&rsquo;s team, flows through a NiFi instance run by Norman, and then is routed to Bob&rsquo;s System B. Saturday at 0300, an urgent alert comes into Norman saying that Bob hasn&rsquo;t received data for the last 3 hours. A quick check of NiFi&rsquo;s stats shows that Alice&rsquo;s app is still producing messages on the correct Kafka topic and NiFi is consuming and delivering the data. After some troubleshooting, the RCA (root cause analysis) is that while the data was being written to a triage directory managed by Bob&rsquo;s team, their application server was down, and the triage directory has aged out half of the delivered data. In other loosely-coupled systems, the data may be lost forever. Not only does NiFi&rsquo;s provenance allow Norman to prove that the data was delivered (important for the inevitable &ldquo;blame game&rdquo; that will be played by managers and budget people on Monday), but then to reconcile the &ldquo;we&rsquo;re all on the same team&rdquo; mentality and help Bob out by replaying all the data that was lost back through the same flow. &ldquo;My CPU is a neural-net processor, a learning computer&rdquo; One of the buzzwords that you can&rsquo;t go 20 minutes without hearing today is &ldquo;machine learning&rdquo; (that&rsquo;s two words). While this means different things to different people (seriously, ask a data scientist sometime if you&rsquo;re having trouble falling asleep), it&rsquo;s generally accepted as &ldquo;the computer learning something without being explicitly programmed&rdquo;. Will Song has done some fascinating research on using NiFi provenance data for anomaly detection. Rather than examining the actual content of a flowfile (which can inhabit a broad, aschematic domain or be in a format that is not easily consumed or parsed), Will took the approach of examining the provenance metadata, which is tightly defined. By identifying anomalous data in timing, routing, etc., he could build a clustering model and predictor system. The possibilities for NiFi are impressive &ndash; from early fault detection (think identifying timing increases to predict failing external hardware or network issues), to flow recommender systems (a Markov chain lookup of the most frequent follow-on processors when Processor X is added to the flow or configured with a certain collection of attributes), to flow recovery (self-healing flows that can intelligently re-route data when a specific destination is not available or bypass an expensive enrichment step when the flow volume increases above a threshold rather than delay downstream delivery). Uphill Both Ways The Provenance Repository was originally developed to provide a basic storage facility and sequential iteration. As noted by Mark Payne in NIFI-3356: Provide a newly refactored provenance repository The Persistent Provenance Repository has been redesigned a few different times over several years. The original design for the repository was to provide storage of events and sequential iteration over those events via a Reporting Task. After that, we added the ability to compress the data so that it could be held longer. We then introduced the notion of indexing and searching via Lucene. We&rsquo;ve since made several more modifications to try to boost performance. At this point, however, the repository is still the bottleneck for many flows that handle large volumes of small FlowFiles. We need a new implementation that is based around the current goals for the repository and that can provide better throughput. The PersistentProvenanceRepository served well for a long time, and while it is still the default implementation for a vanilla installation of NiFi, Mark did amazing work building a completely redesigned backing store called the WriteAheadProvenanceRepository. Faster, Stronger, Better The WriteAheadProvenanceRepository (or WAPR from here on out) uses a write-ahead log for the backing EventStore, rather than writing directly to journal files as the PersistentProvenanceRepository did. By combining the EventStore, which simply reports back an EventIdentifier to locate the written data, with an EventIndex (powered by Apache Lucene), the two components can work in conjunction to provide high throughput and efficient querying and retrieval. Out of the box, WAPR is about 3x as fast as PPR, and this scales better as more disks and CPU resources are made available. In addition, with the Lucene EventIndex, event records are immediately available for query and retrieval, as opposed to the batch processing and ingesting done by PPR. The WAPR implementation follows the classic Java design pattern philosophy &ldquo;composition over inheritance&rdquo;, so the underlying EventStore field belonging to the repository contains RecordWriter and RecordReader members. By providing a factory for each of these fields during EventStore construction, the store is responsible for instantiating these objects when necessary, but the repository itself can inject the relevant behavior through DI (Dependency Injection)/IoC (Inversion of Control). This is a crucial decision that made implementing the encrypted version much easier and cleaner. Encrypt All The Things! That&rsquo;s 8500 characters to get to the point of this article &ndash; why and how do we encrypt the provenance repository? The why is pretty straightforward &ndash; why not? In all seriousness, provenance data contains, in addition to generic timing and routing metadata, the value of the attributes at each point in the lineage. Many of these values can be quite sensitive. While NiFi has access controls for the REST API and UI, these details are written in plaintext (or compressed via GZIP) to the backing file system. While NiFi was originally designed with the expectation that it would run on managed hardware, many users are now requesting cloud deployments, or as I call it, &ldquo;storing the crown jewels in someone else&rsquo;s safe&rdquo;. As part of an ongoing effort to harden NiFi for deployment on remote/&rdquo;untrusted&rdquo; hardware, we are continually looking for exposed surfaces. I have plans to provide an encrypted version of all three repositories (content and flowfile are the other two), and provenance seemed like the place to start, as it is the most ephemeral and the WAPR was most recently written and likely to be the cleanest, learning the most from the continual community feedback. The how isn&rsquo;t much more complicated. Because of Mark&rsquo;s clean architecture, I was able to extend the WAPR and intercept the RecordReaderFactory and RecordWriterFactory that were injected into the EventStore constructor and simply replace those with my own implementations. This means the EncryptedWriteAheadProvenanceRepository file is a grand total of 159 lines (the unit test is 346). The classes we will actually examine here are the KeyProvider interface and its two (current) implementations StaticKeyProvider and FileBasedKeyProvider, the ProvenanceEventEncryptor interface and its sole (current) implementation AESProvenanceEventEncryptor, and the new implementations of EncryptedSchemaRecordReader and EncryptedSchemaRecordWriter. Vinz Clortho, Keymaster of Gozer Providing access to encryption keys is one of the great challenges of data protection. Handling key material protection and availability, as well as key migration, rotation, and expiry are broad and complicated topics. For the initial implementation, I focused on two providers &ndash; StaticKeyProvider and FileBasedKeyProvider. The common interface is quite simple, providing four methods (Javadoc elided): public interface KeyProvider { SecretKey getKey(String keyId) throws KeyManagementException; boolean keyExists(String keyId); List getAvailableKeyIds(); boolean addKey(String keyId, SecretKey key) throws OperationNotSupportedException, KeyManagementException; } &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; The methods are fairly self-documenting &ndash; by storing the keyId alongside the encrypted data, we are able to encrypt records with varying keys over time, reducing the risk of key compromise to be more granular. The static implementation simply contains a HashMap of key IDs to SecretKey objects. It is initialized by reading the $NIFI_HOME/conf/nifi.properties file and reading the provided keys. These keys can be in plaintext or encrypted (along with any other sensitive configuration value) using the $ ./bin/encrypt-config.sh tool provided by the NiFi Toolkit. if (StaticKeyProvider.class.getName().equals(implementationClassName)) { // Get all the keys (map) from config if (CryptoUtils.isValidKeyProvider(implementationClassName, config.getKeyProviderLocation(), config.getKeyId(), config.getEncryptionKeys())) { Map&lt;String, SecretKey&gt; formedKeys = config.getEncryptionKeys().entrySet().stream() .collect(Collectors.toMap( Map.Entry::getKey, e -&gt; { try { return CryptoUtils.formKeyFromHex(e.getValue()); } catch (KeyManagementException e1) { // This should never happen because the hex has already been validated logger.error(&quot;Encountered an error: &quot;, e1); return null; } })); keyProvider = new StaticKeyProvider(formedKeys); } else { final String msg = &quot;The StaticKeyProvider definition is not valid&quot;; logger.error(msg); throw new KeyManagementException(msg); } } The file-based implementation expands on this by reading from a separate file (which can be located on a remote volume, etc.) and reading a key-value listing of key IDs and keys. The key material is Base64-encoded, encrypted and signed (using AES G/CM) hexadecimal representations. Currently neither implementation supports the addKey method, but in the future, I expect a PKCS11-compatible HSM (Hardware Security Module) provider as well as bridges to sensitive value containers like Square&rsquo;s KeyWhiz and Hashicorp&rsquo;s Vault. The KeyProvider interface and the implementations are also contained within the provenance package and module, but I expect to refactor them out to a framework-level service as part of changes in NIFI-3890: Create Key Management Controller Service. Superfluous Interface Development I&rsquo;m kind of joking in the section title &ndash; the ProvenanceEventEncryptor interface has only four methods and only one implementation, but I wanted to ensure it could be cleanly extended in the future. public interface ProvenanceEventEncryptor { void initialize(KeyProvider keyProvider) throws KeyManagementException; byte[] encrypt(byte[] plainRecord, String recordId, String keyId) throws EncryptionException; byte[] decrypt(byte[] encryptedRecord, String recordId) throws EncryptionException; String getNextKeyId() throws KeyManagementException; } Warning: Crypto nerd stuff ahead The provided implementation uses AES (Advanced Encryption Standard) in G/CM (Galois/Counter Mode). AES is a symmetric encryption cipher, a variant of the Rijndael cipher, a substitution-permutation network, with a fixed block size of 128 bits and a key length of 128, 192, or 256 bits. This is in contrast to one of its most-common predecessors, DES (Data Encryption Standard), which used a Feistel network. If you want more detail than this, buy me a beer sometime and sit back. If you absolutely do not want more detail, consider yourself sane, and accept that it is sufficient for what we are covering here. In addition to the cipher selection, G/CM is an AEAD (Authenticated Encryption with Associated Data) mode of operation, which means that not only does it provide confidentiality (only people with the secret key can decrypt the cipher text), it also provides integrity (the cipher text cannot be undetectably modified by a party without the secret key). This is crucial for authenticity, especially in the application of provenance data, and common modes like ECB, CBC, and CTR do not provide this feature. An alternative construction would be to use a hash-based message authentication code (HMAC) like HMAC/SHA-256 or BLAKE2 with a separate key over the cipher text, but G/CM satisfies the requirements without a separate key and operation, so it&rsquo;s a personal favorite for scenarios like this. In the future, there may be ChaCha20-Poly1305 implementations for better performance, RSA or GPG implementations for asymmetric encryption using public-private key pairs, or even HSM implementations for encryption performed on remote/network-attached encryption appliances with completely contained keys. Regardless, the actual interface contract is quite straightforward. The encrypt() and decrypt() methods accept arbitrary byte[] messages and some metadata, check that the specified key exists and is valid, and then perform the desired cryptographic operation (including serializing/deserializing the encryption metadata) and return the resulting byte[]. How to Lose a Super Bowl and Other Great Interceptions We now turn to the pieces that actually make the repository work (everything up to this point has been pretty standard point-n&rsquo;-click cryptography). The EncryptedSchemaRecordWriter and EncryptedSchemaRecordReader classes that we define will be wrappers extending and adding bonus functionality to the EventIdFirstSchemaRecord* classes. By providing different factories to the EventStore constructor as mentioned earlier, we&rsquo;ll provide compatible instances which intercept the byte[] serialization/deserialization and also encrypt/decrypt the data. This means we don&rsquo;t need any additional work to handle event indexing/querying, compression, etc. This saves us about 800 lines of repetition. The encryption is easy &ndash; as stated above, the record is already serialized to a byte[] by the existing record writer, and that byte[] is handed to the ProvenanceEventEncryptor already described. The EncryptionMetadata (key ID, algorithm, IV, version, and cipher text length) is also serialized and prepended to allow on-demand retrieval and decryption (as a good crypto student, you did know we were going to use unique and non-predictable IVs for every event record, right?). This also allows records encrypted by two different keys to reside side-by-side in the repository with no ill effects. The decryption operation is simply the inverse &ndash; retrieve the blob of data identified by the event ID (and read using random access via stored offset, not sequential reads) from the repository, retrieve the key and decrypt it, and then pass the plaintext serialized form to the delegated readRecord() method to be rehydrated into an object. Configuring The App As described in the Apache NiFi User Guide and Apache NiFi Admin Guide (light reading for insomniacs), the encrypted provenance repository does need a little bit of configuration in nifi.properties. Every property is verbosely described on that page, but here is the simplest valid configuration: nifi.provenance.repository.implementation=org.apache.nifi.provenance.EncryptedWriteAheadProvenanceRepository nifi.provenance.repository.debug.frequency=100 nifi.provenance.repository.encryption.key.provider.implementation=org.apache.nifi.provenance.StaticKeyProvider nifi.provenance.repository.encryption.key.provider.location= nifi.provenance.repository.encryption.key.id=Key1 nifi.provenance.repository.encryption.key=0123456789ABCDEFFEDCBA98765432100123456789ABCDEFFEDCBA9876543210 Does It Actually Work? Encryption always adds costs to any software. I ran some basic performance tests to provide some metrics in PR 1686, and I found that with low flow volume, using an encrypted provenance repository was more than twice as fast as the old PPR and almost identical to the new WAPR. This led me to double-take and question &ldquo;is it actually encrypting anything?&rdquo; I used my handy Hex Fiend to examine the actual files on disk to ensure the data was being encrypted. Here you can see the EncryptionMetadata being serialized via Java serialization and the cipher text of the event record following. Ok, so once the data is encrypted, is it still useful in the app? Sure enough, a provenance query returns perfectly human-readable records through the REST API and the UI. With high flow volume, I did &ldquo;luckily&rdquo; see more performance cost. Still, running with an encrypted provenance repository, the flow could handle about 13k events per second. While the flow performance was slower than the original PPR, the provenance queries were almost identical (and sometimes even faster). Long story short, if you like writing encrypted provenance data to your disk, NiFi&rsquo;s got you covered. Back to the Future There is still plenty of work to do here. Repository implementation migration is not as smooth as it could be, tools for migration and key rotation would be nice, the KeyProvider and other services can be extracted to the framework level, content and flowfile repository implementations are still necessary, and provenance records themselves do not have cryptographic signatures over their content (for chain of custody, governance, and integrity guarantees). The User Guide has a section devoted just to &ldquo;Potential Issues&rdquo; involved. As provenance data isn&rsquo;t intended to be stored long term in NiFi, but offloaded to a glacial store like Apache Atlas, these aren&rsquo;t priority issues. I would recommend you try the encrypted provenance repository on non-business-critical data at first, but in our tests, it has been pretty stable.","headline":"Encrypted Provenance Repositories in Apache NiFi","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/nifi/entry/encrypted-provenance-repositories-in-apache"},"url":"http://localhost:4000/nifi/entry/encrypted-provenance-repositories-in-apache"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Encrypted Provenance Repositories in Apache NiFi</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2017-07-11T17:28:14-04:00" itemprop="datePublished">Jul 11, 2017
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Andy LoPresto", "login"=>"alopresto", "email"=>"alopresto@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1>Encrypted Provenance Repositories in Apache NiFi</h1>
<p>
   <span class="author">Andy LoPresto -&nbsp;</span><br />
   <span class="author"><a href="https://twitter.com/yolopey">@yolopey</a></span></p>
<hr />
<p>There has been a surprising level of Twitter demand for more security-focused NiFi blogs. I&rsquo;ll try to address this underserved market with a post about a new feature in <a href="https://nifi.apache.org">Apache NiFi</a> <a href="https://archive.apache.org/dist/nifi/1.2.0/">1.2.0</a> &ndash; encrypted provenance repositories (<a href="https://issues.apache.org/jira/browse/NIFI-3388">NIFI-3388</a>). As a mentor of mine often said, &ldquo;You don&rsquo;t understand something until you can teach it.&rdquo; In this article, we&rsquo;ll find out if I actually understand the code I wrote. First I&rsquo;ll give a brief explanation of the provenance feature for anyone unfamiliar with it, describe how the existing implementations work, and introduce a new option that is sure to have people very excited (<em>no guarantee of excited people actually offered</em>).</p>
<h2 id="what-is-provenance">What is Provenance?</h2>
<p>Provenance is a term we use in the NiFi community with a very specific intent. The great bastion of knowledge, Wikipedia, <a href="https://en.wikipedia.org/wiki/Provenance">defines it</a> as &ldquo;the chronology of the ownership, custody or location of a historical object.&rdquo; Substitute &ldquo;data&rdquo; for &ldquo;historical object&rdquo;, and you&rsquo;re on the right track for NiFi. A serious concern in any dataflow routing &amp; management platform is &ldquo;what happens to the data?&rdquo; and &ldquo;when?&rdquo;. Only slightly less important is &ldquo;how do we prove it?&rdquo;</p>
<p>NiFi allows users to answer these questions by automatically recording everything that happens to your data on a very granular level. Think of &ldquo;data lineage&rdquo; as a family tree where each person has a Ken Burns biography &ndash; a very complete history of everything they did and how it relates to everyone else. Any event (data being created, ingested, routed, modified, tagged, viewed, exported, or deleted) is recorded, along with the time, the identity of the component that acted on it, where it was sent, what was changed, etc.</p>
<p><img class="dialog" style="height: 50%" src="https://alopresto.github.io/assets/provenance_query.png" alt="Provenance records listing" /></p>
<p>Not only does this extensive dataset allow users to query the history of their data, it enables some other powerful features as well.</p>
<p><img class="dialog" style="height: 50%" src="https://alopresto.github.io/assets/lineage_chain.png" alt="Provenance lineage chain" /></p>
<h3 id="explore">Explore</h3>
<p>Users can explore the provenance chain (the directed graph representation of the provenance events) of any flowfile to review the path it followed through the data flow. This allows both broad and focused analysis of things like timing bottlenecks, system performance, and identifying critical paths. Sometimes open-ended exploration of this data will reveal or inspire potential improvements in the flow itself.</p>
<h3 id="replay">Replay</h3>
<p>The provenance data is also crucial to a key feature of NiFi &ndash; allowing the user to <em>replay</em> flowfiles. As long as the provenance data has not been aged off and the referenced content is still available in the content repository, any flowfile can be replayed from any point in the flow. This greatly tightens the flow development lifecycle &ndash; rather than a &ldquo;build and deploy&rdquo; cycle, this encourages rapid refinement of a flow watching live (or slightly stale, but consistent) data proceed through different branches. A metaphor we use frequently in the NiFi community is that the data, much like water, is always flowing. NiFi is less about building permanent water pipes, and more like digging irrigation ditches from a constantly-flowing river.</p>
<p>While replay supports rapid flow refinement, the open secret is that it was developed for a different reason. NiFi connects many disparate systems, and in an enterprise environment, these are often owned and managed by different teams. Sometimes, data coming from System A, managed by Alice&rsquo;s team, flows through a NiFi instance run by Norman, and then is routed to Bob&rsquo;s System B. Saturday at 0300, an urgent alert comes into Norman saying that Bob hasn&rsquo;t received data for the last 3 hours. A quick check of NiFi&rsquo;s stats shows that Alice&rsquo;s app is still producing messages on the correct Kafka topic and NiFi is consuming and delivering the data. After some troubleshooting, the RCA (root cause analysis) is that while the data was being written to a triage directory managed by Bob&rsquo;s team, their application server was down, and the triage directory has aged out half of the delivered data.</p>
<p>In other loosely-coupled systems, the data may be lost forever. Not only does NiFi&rsquo;s provenance allow Norman to prove that the data was delivered (important for the inevitable &ldquo;blame game&rdquo; that will be played by managers and budget people on Monday), but then to reconcile the &ldquo;we&rsquo;re all on the same team&rdquo; mentality and help Bob out by replaying all the data that was lost back through the same flow.</p>
<h3 id="my-cpu-is-a-neural-net-processor-a-learning-computer">&ldquo;My CPU is a neural-net processor, a learning computer&rdquo;</h3>
<p>One of the buzzwords that you can&rsquo;t go 20 minutes without hearing today is &ldquo;machine learning&rdquo; (<em>that&rsquo;s two words</em>). While this means different things to different people (seriously, ask a data scientist sometime if you&rsquo;re having trouble falling asleep), it&rsquo;s generally accepted as &ldquo;the computer learning something without being explicitly programmed&rdquo;. <a href="https://linkedin.com/in/william-song39">Will Song</a> has done some fascinating research on using <a href="https://github.com/song-william/NiFi-Anomaly-Detection">NiFi provenance data for anomaly detection</a>. Rather than examining the actual content of a flowfile (which can inhabit a broad, aschematic domain or be in a format that is not easily consumed or parsed), Will took the approach of examining the provenance metadata, which is tightly defined. By identifying anomalous data in timing, routing, etc., he could build a clustering model and predictor system.</p>
<p>The possibilities for NiFi are impressive &ndash; from early fault detection (think identifying timing increases to predict failing external hardware or network issues), to flow recommender systems (a Markov chain lookup of the most frequent follow-on processors when Processor X is added to the flow or configured with a certain collection of attributes), to flow recovery (self-healing flows that can intelligently re-route data when a specific destination is not available or bypass an expensive enrichment step when the flow volume increases above a threshold rather than delay downstream delivery).</p>
<h2 id="uphill-both-ways">Uphill Both Ways</h2>
<p>The <a href="https://nifi.apache.org/docs/nifi-docs/html/user-guide.html#data_provenance">Provenance Repository</a> was originally developed to provide a basic storage facility and sequential iteration. As noted by <a href="https://twitter.com/dataflowmark">Mark Payne</a> in <a href="https://issues.apache.org/jira/browse/NIFI-3356">NIFI-3356: Provide a newly refactored provenance repository</a></p>
<blockquote>
<p>The Persistent Provenance Repository has been redesigned a few different times over several years. The original design for the repository was to provide storage of events and sequential iteration over those events via a Reporting Task. After that, we added the ability to compress the data so that it could be held longer. We then introduced the notion of indexing and searching via Lucene. We&rsquo;ve since made several more modifications to try to boost performance.</p>
</blockquote>
<blockquote>
<p>At this point, however, the repository is still the bottleneck for many flows that handle large volumes of small FlowFiles. We need a new implementation that is based around the current goals for the repository and that can provide better throughput.</p>
</blockquote>
<p>The <code class="highlighter-rouge">PersistentProvenanceRepository</code> served well for a long time, and while it is still the default implementation for a vanilla installation of NiFi, Mark did amazing work building a completely redesigned backing store called the <code class="highlighter-rouge">WriteAheadProvenanceRepository</code>.</p>
<h2 id="faster-stronger-better">Faster, Stronger, Better</h2>
<p>The <code class="highlighter-rouge">WriteAheadProvenanceRepository</code> (or <code class="highlighter-rouge">WAPR</code> from here on out) uses a write-ahead log for the backing <code class="highlighter-rouge">EventStore</code>, rather than writing directly to <em>journal files</em> as the <code class="highlighter-rouge">PersistentProvenanceRepository</code> did. By combining the <code class="highlighter-rouge">EventStore</code>, which simply reports back an <code class="highlighter-rouge">EventIdentifier</code> to locate the written data, with an <code class="highlighter-rouge">EventIndex</code> (powered by <a href="https://lucene.apache.org/">Apache Lucene</a>), the two components can work in conjunction to provide high throughput and efficient querying and retrieval.</p>
<p>Out of the box, <code class="highlighter-rouge">WAPR</code> is about 3x as fast as <code class="highlighter-rouge">PPR</code>, and this scales better as more disks and CPU resources are made available. In addition, with the Lucene <code class="highlighter-rouge">EventIndex</code>, event records are immediately available for query and retrieval, as opposed to the batch processing and ingesting done by <code class="highlighter-rouge">PPR</code>.</p>
<p>The <code class="highlighter-rouge">WAPR</code> implementation follows the classic Java design pattern philosophy &ldquo;composition over inheritance&rdquo;, so the underlying <code class="highlighter-rouge">EventStore</code> field belonging to the repository contains <code class="highlighter-rouge">RecordWriter</code> and <code class="highlighter-rouge">RecordReader</code> members. By providing a factory for each of these fields during <code class="highlighter-rouge">EventStore</code> construction, the store is responsible for instantiating these objects when necessary, but the repository itself can inject the relevant behavior through DI (<a href="https://en.wikipedia.org/wiki/Dependency_injection">Dependency Injection</a>)/IoC (<a href="https://en.wikipedia.org/wiki/Inversion_of_control">Inversion of Control</a>). This is a crucial decision that made implementing the encrypted version much easier and cleaner.</p>
<h2 id="encrypt-all-the-things">Encrypt All The Things!</h2>
<p>That&rsquo;s 8500 characters to get to the point of this article &ndash; why and how do we encrypt the provenance repository?</p>
<p>The <em>why</em> is pretty straightforward &ndash; why not? In all seriousness, provenance data contains, in addition to generic timing and routing metadata, the value of the attributes at each point in the lineage. Many of these values can be quite sensitive. While NiFi has access controls for the REST API and UI, these details are written in plaintext (or compressed via GZIP) to the backing file system. While NiFi was originally designed with the expectation that it would run on managed hardware, many users are now requesting cloud deployments, or as I call it, &ldquo;storing the crown jewels in someone else&rsquo;s safe&rdquo;. As part of an ongoing effort to harden NiFi for deployment on remote/&rdquo;untrusted&rdquo; hardware, we are continually looking for exposed surfaces. I have plans to provide an encrypted version of all three repositories (<em>content</em> and <em>flowfile</em> are the other two), and provenance seemed like the place to start, as it is the most ephemeral and the <code class="highlighter-rouge">WAPR</code> was most recently written and likely to be the cleanest, learning the most from the continual community feedback.</p>
<p>The <em>how</em> isn&rsquo;t much more complicated. Because of Mark&rsquo;s clean architecture, I was able to extend the <code class="highlighter-rouge">WAPR</code> and intercept the <code class="highlighter-rouge">RecordReaderFactory</code> and <code class="highlighter-rouge">RecordWriterFactory</code> that were injected into the <code class="highlighter-rouge">EventStore</code> constructor and simply replace those with my own implementations. This means the <code class="highlighter-rouge">EncryptedWriteAheadProvenanceRepository</code> file is a grand total of 159 lines (the unit test is 346).</p>
<p>The classes we will actually examine here are the <code class="highlighter-rouge">KeyProvider</code> interface and its two (current) implementations <code class="highlighter-rouge">StaticKeyProvider</code> and <code class="highlighter-rouge">FileBasedKeyProvider</code>, the <code class="highlighter-rouge">ProvenanceEventEncryptor</code> interface and its sole (current) implementation <code class="highlighter-rouge">AESProvenanceEventEncryptor</code>, and the new implementations of <code class="highlighter-rouge">EncryptedSchemaRecordReader</code> and <code class="highlighter-rouge">EncryptedSchemaRecordWriter</code>.</p>
<h3 id="vinz-clortho-keymaster-of-gozer">Vinz Clortho, Keymaster of Gozer</h3>
<p>Providing access to encryption keys is one of the great challenges of data protection. Handling key material protection and availability, as well as key migration, rotation, and expiry are broad and complicated topics. For the initial implementation, I focused on two providers &ndash; <code class="highlighter-rouge">StaticKeyProvider</code> and <code class="highlighter-rouge">FileBasedKeyProvider</code>. The common interface is quite simple, providing four methods (Javadoc elided):</p>
<div class="highlighter-rouge">
<pre class="highlight"><code>public interface KeyProvider {
     SecretKey getKey(String keyId) throws KeyManagementException; 
     boolean keyExists(String keyId);  
     List<String> getAvailableKeyIds();   
     boolean addKey(String keyId, SecretKey key) throws OperationNotSupportedException, KeyManagementException;
}
</code></pre>
</div>
<p>The methods are fairly self-documenting &ndash; by storing the <code class="highlighter-rouge">keyId</code> alongside the encrypted data, we are able to encrypt records with varying keys over time, reducing the risk of key compromise to be more granular.</p>
<p>The static implementation simply contains a <code class="highlighter-rouge">HashMap</code> of key IDs to <code class="highlighter-rouge">SecretKey</code> objects. It is initialized by reading the <code class="highlighter-rouge">$NIFI_HOME/conf/nifi.properties</code> file and reading the provided keys. These keys can be in plaintext or encrypted (along with any other sensitive configuration value) using the <code class="highlighter-rouge">$ ./bin/encrypt-config.sh</code> tool provided by the NiFi Toolkit.</p>
<div class="highlighter-rouge">
<pre class="highlight"><code>if (StaticKeyProvider.class.getName().equals(implementationClassName)) {
    // Get all the keys (map) from config
    if (CryptoUtils.isValidKeyProvider(implementationClassName, config.getKeyProviderLocation(), config.getKeyId(), config.getEncryptionKeys())) {
        Map<String, SecretKey> formedKeys = config.getEncryptionKeys().entrySet().stream()
            .collect(Collectors.toMap(
                Map.Entry::getKey,
                e -> {
                    try {
                        return CryptoUtils.formKeyFromHex(e.getValue());
                    } catch (KeyManagementException e1) {
                        // This should never happen because the hex has already been validated
                        logger.error("Encountered an error: ", e1);
                        return null;
                    }
                }));
        keyProvider = new StaticKeyProvider(formedKeys);
    } else {
        final String msg = "The StaticKeyProvider definition is not valid";
        logger.error(msg);
        throw new KeyManagementException(msg);
    }
}
</code></pre>
</div>
<p>The file-based implementation expands on this by reading from a separate file (which can be located on a remote volume, etc.) and reading a key-value listing of key IDs and keys. The key material is Base64-encoded, encrypted and signed (using <code class="highlighter-rouge">AES G/CM</code>) hexadecimal representations.</p>
<p>Currently neither implementation supports the <code class="highlighter-rouge">addKey</code> method, but in the future, I expect a <code class="highlighter-rouge">PKCS11</code>-compatible HSM (<a href="https://en.wikipedia.org/wiki/Hardware_security_module">Hardware Security Module</a>) provider as well as bridges to sensitive value containers like Square&rsquo;s <a href="https://square.github.io/keywhiz/">KeyWhiz</a> and Hashicorp&rsquo;s <a href="https://www.vaultproject.io/">Vault</a>.</p>
<p>The <code class="highlighter-rouge">KeyProvider</code> interface and the implementations are also contained within the provenance package and module, but I expect to refactor them out to a framework-level service as part of changes in <a href="https://issues.apache.org/jira/browse/NIFI-3890">NIFI-3890: Create Key Management Controller Service</a>.</p>
<h3 id="superfluous-interface-development">Superfluous Interface Development</h3>
<p>I&rsquo;m kind of joking in the section title &ndash; the <code class="highlighter-rouge">ProvenanceEventEncryptor</code> interface has only four methods and only one implementation, but I wanted to ensure it could be cleanly extended in the future.</p>
<div class="highlighter-rouge">
<pre class="highlight"><code>public interface ProvenanceEventEncryptor {
    void initialize(KeyProvider keyProvider) throws KeyManagementException;
    byte[] encrypt(byte[] plainRecord, String recordId, String keyId) throws EncryptionException;
    byte[] decrypt(byte[] encryptedRecord, String recordId) throws EncryptionException;
    String getNextKeyId() throws KeyManagementException;
}
</code></pre>
</div>
<p><em>Warning: Crypto nerd stuff ahead</em></p>
<p>The provided implementation uses <code class="highlighter-rouge">AES</code> (<a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard">Advanced Encryption Standard</a>) in <code class="highlighter-rouge">G/CM</code> (<a href="https://en.wikipedia.org/wiki/Galois/Counter_Mode">Galois/Counter Mode</a>). <code class="highlighter-rouge">AES</code> is a symmetric encryption cipher, a variant of the Rijndael cipher, a substitution-permutation network, with a fixed block size of 128 bits and a key length of 128, 192, or 256 bits. This is in contrast to one of its most-common predecessors, <code class="highlighter-rouge">DES</code> (<a href="https://en.wikipedia.org/wiki/Data_Encryption_Standard">Data Encryption Standard</a>), which used a Feistel network. If you want more detail than this, buy me a beer sometime and sit back. If you absolutely <em>do not</em> want more detail, consider yourself sane, and accept that it is sufficient for what we are covering here. In addition to the cipher selection, <code class="highlighter-rouge">G/CM</code> is an AEAD (<a href="https://en.wikipedia.org/wiki/Authenticated_encryption">Authenticated Encryption with Associated Data</a>) mode of operation, which means that not only does it provide <em>confidentiality</em> (only people with the secret key can decrypt the cipher text), it also provides <em>integrity</em> (the cipher text cannot be undetectably modified by a party without the secret key). This is crucial for authenticity, especially in the application of provenance data, and common modes like <code class="highlighter-rouge">ECB</code>, <code class="highlighter-rouge">CBC</code>, and <code class="highlighter-rouge">CTR</code> do not provide this feature. An alternative construction would be to use a hash-based message authentication code (<code class="highlighter-rouge">HMAC</code>) like <code class="highlighter-rouge">HMAC/SHA-256</code> or <a href="https://research.kudelskisecurity.com/2017/03/06/why-replace-sha-1-with-blake2/"><code class="highlighter-rouge">BLAKE2</code></a> <em>with a separate key</em> <strong>over the cipher text</strong>, but <code class="highlighter-rouge">G/CM</code> satisfies the requirements without a separate key and operation, so it&rsquo;s a personal favorite for scenarios like this.</p>
<p>In the future, there may be <a href="https://blog.cloudflare.com/do-the-chacha-better-mobile-performance-with-cryptography/"><code class="highlighter-rouge">ChaCha20-Poly1305</code></a> implementations for better performance, <a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)">RSA</a> or <a href="https://en.wikipedia.org/wiki/GNU_Privacy_Guard">GPG</a> implementations for asymmetric encryption using public-private key pairs, or even HSM implementations for encryption performed on remote/network-attached encryption appliances with completely contained keys.</p>
<p>Regardless, the actual interface contract is quite straightforward. The <code class="highlighter-rouge">encrypt()</code> and <code class="highlighter-rouge">decrypt()</code> methods accept arbitrary <code class="highlighter-rouge">byte[]</code> messages and some metadata, check that the specified key exists and is valid, and then perform the desired cryptographic operation (including serializing/deserializing the encryption metadata) and return the resulting <code class="highlighter-rouge">byte[]</code>.</p>
<h3 id="how-to-lose-a-super-bowl-and-other-great-interceptions">How to Lose a Super Bowl and Other Great Interceptions</h3>
<p>We now turn to the pieces that actually make the repository work (everything up to this point has been pretty standard point-n&rsquo;-click cryptography). The <code class="highlighter-rouge">EncryptedSchemaRecordWriter</code> and <code class="highlighter-rouge">EncryptedSchemaRecordReader</code> classes that we define will be wrappers extending and adding bonus functionality to the <code class="highlighter-rouge">EventIdFirstSchemaRecord*</code> classes. By providing different factories to the <code class="highlighter-rouge">EventStore</code> constructor as mentioned earlier, we&rsquo;ll provide compatible instances which intercept the <code class="highlighter-rouge">byte[]</code> serialization/deserialization and also encrypt/decrypt the data. This means we don&rsquo;t need any additional work to handle event indexing/querying, compression, etc. This saves us about 800 lines of repetition.</p>
<p>The encryption is easy &ndash; as stated above, the record is already serialized to a <code class="highlighter-rouge">byte[]</code> by the existing record writer, and that <code class="highlighter-rouge">byte[]</code> is handed to the <code class="highlighter-rouge">ProvenanceEventEncryptor</code> already described. The <code class="highlighter-rouge">EncryptionMetadata</code> (key ID, algorithm, IV, version, and cipher text length) is also serialized and prepended to allow on-demand retrieval and decryption (as a good crypto student, you did know we were going to use unique and non-predictable IVs for every event record, right?). This also allows records encrypted by two different keys to reside side-by-side in the repository with no ill effects.</p>
<p>The decryption operation is simply the inverse &ndash; retrieve the blob of data identified by the event ID (and read using random access via stored offset, not sequential reads) from the repository, retrieve the key and decrypt it, and then pass the plaintext serialized form to the delegated <code class="highlighter-rouge">readRecord()</code> method to be rehydrated into an object.</p>
<h3 id="configuring-the-app">Configuring The App</h3>
<p>As described in the <a href="https://nifi.apache.org/docs/nifi-docs/html/user-guide.html#encrypted-provenance">Apache NiFi User Guide</a> and <a href="https://nifi.apache.org/docs/nifi-docs/html/administration-guide.html#encrypted-write-ahead-provenance-repository-properties">Apache NiFi Admin Guide</a> (light reading for insomniacs), the encrypted provenance repository does need a little bit of configuration in <code class="highlighter-rouge">nifi.properties</code>. Every property is verbosely described on that page, but here is the simplest valid configuration:</p>
<div class="highlighter-rouge">
<pre class="highlight"><code>nifi.provenance.repository.implementation=org.apache.nifi.provenance.EncryptedWriteAheadProvenanceRepository
nifi.provenance.repository.debug.frequency=100
nifi.provenance.repository.encryption.key.provider.implementation=org.apache.nifi.provenance.StaticKeyProvider
nifi.provenance.repository.encryption.key.provider.location=
nifi.provenance.repository.encryption.key.id=Key1
nifi.provenance.repository.encryption.key=0123456789ABCDEFFEDCBA98765432100123456789ABCDEFFEDCBA9876543210
</code></pre>
</div>
<h2 id="does-it-actually-work">Does It Actually Work?</h2>
<p>Encryption always adds costs to any software. I ran some basic performance tests to provide some metrics in <a href="https://github.com/apache/nifi/pull/1686">PR 1686</a>, and I found that with low flow volume, using an encrypted provenance repository was more than twice as fast as the old <code class="highlighter-rouge">PPR</code> and almost identical to the new <code class="highlighter-rouge">WAPR</code>. This led me to double-take and question &ldquo;is it actually encrypting anything?&rdquo;</p>
<p><img class="dialog" style="width: 100%" src="https://alopresto.github.io/assets/selv.png" alt="Small Event Low Volume benchmarks" /></p>
<p>I used my handy <a href="http://ridiculousfish.com/hexfiend/">Hex Fiend</a> to examine the actual files on disk to ensure the data was being encrypted. Here you can see the <code class="highlighter-rouge">EncryptionMetadata</code> being serialized via Java serialization and the cipher text of the event record following.</p>
<p><img class="dialog" style="height: 50%" src="https://alopresto.github.io/assets/encrypted_repo_file.png" alt="Encrypted repository file" /></p>
<p>Ok, so once the data is encrypted, is it still useful in the app? Sure enough, a provenance query returns perfectly human-readable records through the REST API and the UI.</p>
<p><img class="dialog" style="height: 50%" src="https://alopresto.github.io/assets/encrypted_prov_query.png" alt="Encrypted provenance repository query" /></p>
<p>With high flow volume, I did &ldquo;luckily&rdquo; see more performance cost. Still, running with an encrypted provenance repository, the flow could handle about 13k events per second. While the flow performance was slower than the original <code class="highlighter-rouge">PPR</code>, the provenance queries were almost identical (and sometimes even faster).</p>
<p><img class="dialog" style="width: 100%" src="https://alopresto.github.io/assets/sehv.png" alt="Small Event High Volume benchmarks" /></p>
<p>Long story short, if you like writing encrypted provenance data to your disk, NiFi&rsquo;s got you covered.</p>
<h2 id="back-to-the-future">Back to the Future</h2>
<p>There is still plenty of work to do here. Repository implementation migration is not as smooth as it could be, tools for migration and key rotation would be nice, the <code class="highlighter-rouge">KeyProvider</code> and other services can be extracted to the framework level, content and flowfile repository implementations are still necessary, and provenance records themselves do not have cryptographic signatures over their content (for chain of custody, governance, and integrity guarantees). The User Guide has <a href="https://nifi.apache.org/docs/nifi-docs/html/user-guide.html#potential-issues">a section</a> devoted just to &ldquo;Potential Issues&rdquo; involved. As provenance data isn&rsquo;t intended to be stored long term in NiFi, but offloaded to a glacial store like <a href="https://atlas.incubator.apache.org/">Apache Atlas</a>, these aren&rsquo;t priority issues. I would recommend you try the encrypted provenance repository on non-business-critical data at first, but in our tests, it has been pretty stable.</p>

  </div><a class="u-url" href="/nifi/entry/encrypted-provenance-repositories-in-apache" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
