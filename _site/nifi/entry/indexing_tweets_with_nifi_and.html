<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Indexing Tweets with NiFi and Solr | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Indexing Tweets with NiFi and Solr" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Indexing Tweets with NiFi and Solr Bryan Bende -&nbsp; bbende@gmail.com This post will cover how to use Apache NiFi to pull in the public stream of tweets from the Twitter API, identify specific tweets of interest, and deliver those tweets to Solr for indexing. The example developed here was built against Apache NiFi 0.3.0 and Apache Solr 5.3. In addition, you will also need to create a Twitter application for accessing their API. A good set of instructions for doing that can be found in this article - How to Create a Twitter App in 8 Easy Steps. Setting up Solr For this example we will start Solr in cloud mode, and create a tweets collection based off the data-driven configuration provided by Solr. The data-driven configuration allows Solr to create fields on the fly based off the incoming data, and is a good place to start for quickly prototyping. The following two commands can be used to get Solr running and create the collection: &lt;/p&gt; ./bin/solr start -c ./bin/solr create_collection -c tweets -d data_driven_schema_configs -shards 1 -replicationFactor 1 &lt;/code&gt; &lt;/div&gt; At this point we should have a running Solr instance on port 8983, with an embedded ZooKeeper on port 9983. Navigate to http://localhost:8983/solr/#/~cloud in your browser to verify Solr is running and the tweets collection was created sucessfully. Building the Dataflow The dataflow we are going build in NiFi will consist of the following processors: GetTwitter for accessing the Twitter API and producing FlowFiles containing JSON Tweets EvaluateJsonPath for extracting values from JSON documents into FlowFile attributes RouteOnAttribute for making decisions about incoming FlowFiles based on their attributes MergeContent for merging together many JSON documents to a single document PutSolrContentStream for streaming JSON documents to Solr The overall flow looks like the following: &lt;/img&gt; Lets walk through the configuration of each processor... GetTwitter &lt;/img&gt; Set the end-point to the Sample Endpoint and fill in the credentials corresponding with the Twitter application you created earlier. For this example we are going to specify a language of &quot;en&quot; to get only English tweets. EvaluateJsonPath &lt;/img&gt; Now we want to extract the text and language of each tweet into FlowFile attributes in order to make decisions on these values later. Set the Destination property to &quot;flowfile-attribute&quot; and add two user-defined properties using the New Property icon on the top right. The name of the property will be the FlowFile attribute name, and the value is the JSON path we would like to extract from the Tweet. This will allow us to access these values later by using expression language, such as ${twitter.text} or ${twitter.lang}. RouteOnAttribute &lt;/img&gt; At this point we would like to ensure we are only indexing tweets we are interested. So we add a user-defined property called &quot;tweet&quot; and specify the following expression language as the value: &lt;/p&gt; ${twitter.text:isEmpty():not():and(${twitter.lang:equals(&quot;en&quot;)})} &lt;/code&gt; &lt;/div&gt; The first part of this expression filters out all the tweets which have an empty message. This occurs frequently as some of the JSON documents coming through the end-point are not actual tweets. The second part of the expression ensures that we are only selecting english tweets. We already set GetTwitter to filter on &quot;en&quot;, but if we hadn&#39;t, the language could be used here to route different languages to different relationships. Any tweets matching the above conditions will be routed to the &quot;tweet&quot; relationship based on the property we defined, and anything else will be routed to the &quot;unmatched&quot; relationship. We can auto-terminate the &quot;unmatched&quot; relationship to have the non-matching FlowFiles discarded. MergeContent &lt;/img&gt; In this example we could send the JSON documents from RouteOnAttribute directly to Solr, but in most real world scenarios accessing the Solr cluster will require network communication, and it will likely produce better performance if we batch together multiple JSON documents into a single request to reduce the amount requests sent over the network. The MergeContent processor is a powerful processor that was built just for this scenario in mind, and is capable of merging FlowFiles based on a number of criteria such as the number of FlowFiles, or their age. MergeContent also performs merges in a streaming manner and as a result is capable of merging a significant number of FlowFiles without worrying about exceeding memory constraints. To configure MergeContent, set a Minimum and Maximum Number of entries, as well as a Max Bin Age to trigger a merge in cases where no new data has come in for a period of time. Also set the Delimiter Strategy to &quot;Text&quot;, and specify the Header, Footer, and Demarcator as [ , ] respectively. This allows us to create a large JSON document composed of many incoming documents. PutSolrContentStream &lt;/img&gt; Configure PutSolrContentStream to point to the Solr instance we started earlier by setting the Solr Type to &quot;Cloud&quot; and the Solr Location to the embedded ZooKeeper that was started earlier (localhost:9983). Also specify a Commit Within of &quot;1000&quot; to commit the incoming documents every second (this may not be needed if you set autoCommit settings in your solrconfig.xml). Any user-defined properties will get sent to Solr on the request as key value pairs, so we need to tell Solr how to transform the incoming JSON document into a Solr document. A good explanation of how this JSON to Solr mapping works can be found here, and an explanation of the user-defined properties can be found here. For this example we will provide the following mappings: split=/ to treat each child JSON document that we merged together as individual Solr documents id:/id to map the id field of the JSON to the id field in the Solr schema (already defined) twitter_text_t:/text to map the text field of the JSON to a dynamic field of type text_general in Solr twitter_username_s:/user/name to map the user of the tweet to a dynamic string field in Solr twitter_created_at_s:/created_at to map the creation date string to a dynamic string field in Solr twitter_timestamp_ms_tl:/timestamp_ms to map the timestamp to a dynamic trie-long field in Solr The naming conventions of the fields allow us to leverage dynamic fields in Solr. Dynamic fields take effect by using a suffix on the field name to indicate the field type, so the &quot;_t&quot; on &quot;twitter_text_t&quot; tells Solr that this field will map to a text_general field type. It is possible to not provide any field mappings and just send the JSON to Solr, and let Solr guess the field types on the fly. However, in that case all of the fields would get added as multiValued fields, and multiValued fields can&#39;t be used in a sorting clause which would prevent us from sorting on the timestamp. So we opt for the dynamic fields here. Also worth noting, this processor has two failure relationships. The regular &quot;failure&quot; relationship is for failures that would generally require some intervention to resolve, such as an invalid document being submitted. The &quot;connection_failure&quot; relationship is for failures related to communication issues between NiFi and Solr. A typical approach in a production scenario would be to route the &quot;connection_failure&quot; back to itself to retry during connection failures, and to route the &quot;failure&quot; relationship to a PutFile processor that could write out failed documents for later inspection. Summary At this point you should be able to hit Start on your processors and start seeing tweets flowing to Solr, happy indexing! A template of the flow created in this example can be found here. We would love to hear any questions, comments, or feedback that you may have! Learn more about Apache NiFi and feel free to leave comments here or e-mail us at dev@nifi.apache.org." />
<meta property="og:description" content="Indexing Tweets with NiFi and Solr Bryan Bende -&nbsp; bbende@gmail.com This post will cover how to use Apache NiFi to pull in the public stream of tweets from the Twitter API, identify specific tweets of interest, and deliver those tweets to Solr for indexing. The example developed here was built against Apache NiFi 0.3.0 and Apache Solr 5.3. In addition, you will also need to create a Twitter application for accessing their API. A good set of instructions for doing that can be found in this article - How to Create a Twitter App in 8 Easy Steps. Setting up Solr For this example we will start Solr in cloud mode, and create a tweets collection based off the data-driven configuration provided by Solr. The data-driven configuration allows Solr to create fields on the fly based off the incoming data, and is a good place to start for quickly prototyping. The following two commands can be used to get Solr running and create the collection: &lt;/p&gt; ./bin/solr start -c ./bin/solr create_collection -c tweets -d data_driven_schema_configs -shards 1 -replicationFactor 1 &lt;/code&gt; &lt;/div&gt; At this point we should have a running Solr instance on port 8983, with an embedded ZooKeeper on port 9983. Navigate to http://localhost:8983/solr/#/~cloud in your browser to verify Solr is running and the tweets collection was created sucessfully. Building the Dataflow The dataflow we are going build in NiFi will consist of the following processors: GetTwitter for accessing the Twitter API and producing FlowFiles containing JSON Tweets EvaluateJsonPath for extracting values from JSON documents into FlowFile attributes RouteOnAttribute for making decisions about incoming FlowFiles based on their attributes MergeContent for merging together many JSON documents to a single document PutSolrContentStream for streaming JSON documents to Solr The overall flow looks like the following: &lt;/img&gt; Lets walk through the configuration of each processor... GetTwitter &lt;/img&gt; Set the end-point to the Sample Endpoint and fill in the credentials corresponding with the Twitter application you created earlier. For this example we are going to specify a language of &quot;en&quot; to get only English tweets. EvaluateJsonPath &lt;/img&gt; Now we want to extract the text and language of each tweet into FlowFile attributes in order to make decisions on these values later. Set the Destination property to &quot;flowfile-attribute&quot; and add two user-defined properties using the New Property icon on the top right. The name of the property will be the FlowFile attribute name, and the value is the JSON path we would like to extract from the Tweet. This will allow us to access these values later by using expression language, such as ${twitter.text} or ${twitter.lang}. RouteOnAttribute &lt;/img&gt; At this point we would like to ensure we are only indexing tweets we are interested. So we add a user-defined property called &quot;tweet&quot; and specify the following expression language as the value: &lt;/p&gt; ${twitter.text:isEmpty():not():and(${twitter.lang:equals(&quot;en&quot;)})} &lt;/code&gt; &lt;/div&gt; The first part of this expression filters out all the tweets which have an empty message. This occurs frequently as some of the JSON documents coming through the end-point are not actual tweets. The second part of the expression ensures that we are only selecting english tweets. We already set GetTwitter to filter on &quot;en&quot;, but if we hadn&#39;t, the language could be used here to route different languages to different relationships. Any tweets matching the above conditions will be routed to the &quot;tweet&quot; relationship based on the property we defined, and anything else will be routed to the &quot;unmatched&quot; relationship. We can auto-terminate the &quot;unmatched&quot; relationship to have the non-matching FlowFiles discarded. MergeContent &lt;/img&gt; In this example we could send the JSON documents from RouteOnAttribute directly to Solr, but in most real world scenarios accessing the Solr cluster will require network communication, and it will likely produce better performance if we batch together multiple JSON documents into a single request to reduce the amount requests sent over the network. The MergeContent processor is a powerful processor that was built just for this scenario in mind, and is capable of merging FlowFiles based on a number of criteria such as the number of FlowFiles, or their age. MergeContent also performs merges in a streaming manner and as a result is capable of merging a significant number of FlowFiles without worrying about exceeding memory constraints. To configure MergeContent, set a Minimum and Maximum Number of entries, as well as a Max Bin Age to trigger a merge in cases where no new data has come in for a period of time. Also set the Delimiter Strategy to &quot;Text&quot;, and specify the Header, Footer, and Demarcator as [ , ] respectively. This allows us to create a large JSON document composed of many incoming documents. PutSolrContentStream &lt;/img&gt; Configure PutSolrContentStream to point to the Solr instance we started earlier by setting the Solr Type to &quot;Cloud&quot; and the Solr Location to the embedded ZooKeeper that was started earlier (localhost:9983). Also specify a Commit Within of &quot;1000&quot; to commit the incoming documents every second (this may not be needed if you set autoCommit settings in your solrconfig.xml). Any user-defined properties will get sent to Solr on the request as key value pairs, so we need to tell Solr how to transform the incoming JSON document into a Solr document. A good explanation of how this JSON to Solr mapping works can be found here, and an explanation of the user-defined properties can be found here. For this example we will provide the following mappings: split=/ to treat each child JSON document that we merged together as individual Solr documents id:/id to map the id field of the JSON to the id field in the Solr schema (already defined) twitter_text_t:/text to map the text field of the JSON to a dynamic field of type text_general in Solr twitter_username_s:/user/name to map the user of the tweet to a dynamic string field in Solr twitter_created_at_s:/created_at to map the creation date string to a dynamic string field in Solr twitter_timestamp_ms_tl:/timestamp_ms to map the timestamp to a dynamic trie-long field in Solr The naming conventions of the fields allow us to leverage dynamic fields in Solr. Dynamic fields take effect by using a suffix on the field name to indicate the field type, so the &quot;_t&quot; on &quot;twitter_text_t&quot; tells Solr that this field will map to a text_general field type. It is possible to not provide any field mappings and just send the JSON to Solr, and let Solr guess the field types on the fly. However, in that case all of the fields would get added as multiValued fields, and multiValued fields can&#39;t be used in a sorting clause which would prevent us from sorting on the timestamp. So we opt for the dynamic fields here. Also worth noting, this processor has two failure relationships. The regular &quot;failure&quot; relationship is for failures that would generally require some intervention to resolve, such as an invalid document being submitted. The &quot;connection_failure&quot; relationship is for failures related to communication issues between NiFi and Solr. A typical approach in a production scenario would be to route the &quot;connection_failure&quot; back to itself to retry during connection failures, and to route the &quot;failure&quot; relationship to a PutFile processor that could write out failed documents for later inspection. Summary At this point you should be able to hit Start on your processors and start seeing tweets flowing to Solr, happy indexing! A template of the flow created in this example can be found here. We would love to hear any questions, comments, or feedback that you may have! Learn more about Apache NiFi and feel free to leave comments here or e-mail us at dev@nifi.apache.org." />
<link rel="canonical" href="http://localhost:4000/nifi/entry/indexing_tweets_with_nifi_and" />
<meta property="og:url" content="http://localhost:4000/nifi/entry/indexing_tweets_with_nifi_and" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2015-09-21T16:35:09-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Indexing Tweets with NiFi and Solr" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2015-09-21T16:35:09-04:00","datePublished":"2015-09-21T16:35:09-04:00","description":"Indexing Tweets with NiFi and Solr Bryan Bende -&nbsp; bbende@gmail.com This post will cover how to use Apache NiFi to pull in the public stream of tweets from the Twitter API, identify specific tweets of interest, and deliver those tweets to Solr for indexing. The example developed here was built against Apache NiFi 0.3.0 and Apache Solr 5.3. In addition, you will also need to create a Twitter application for accessing their API. A good set of instructions for doing that can be found in this article - How to Create a Twitter App in 8 Easy Steps. Setting up Solr For this example we will start Solr in cloud mode, and create a tweets collection based off the data-driven configuration provided by Solr. The data-driven configuration allows Solr to create fields on the fly based off the incoming data, and is a good place to start for quickly prototyping. The following two commands can be used to get Solr running and create the collection: &lt;/p&gt; ./bin/solr start -c ./bin/solr create_collection -c tweets -d data_driven_schema_configs -shards 1 -replicationFactor 1 &lt;/code&gt; &lt;/div&gt; At this point we should have a running Solr instance on port 8983, with an embedded ZooKeeper on port 9983. Navigate to http://localhost:8983/solr/#/~cloud in your browser to verify Solr is running and the tweets collection was created sucessfully. Building the Dataflow The dataflow we are going build in NiFi will consist of the following processors: GetTwitter for accessing the Twitter API and producing FlowFiles containing JSON Tweets EvaluateJsonPath for extracting values from JSON documents into FlowFile attributes RouteOnAttribute for making decisions about incoming FlowFiles based on their attributes MergeContent for merging together many JSON documents to a single document PutSolrContentStream for streaming JSON documents to Solr The overall flow looks like the following: &lt;/img&gt; Lets walk through the configuration of each processor... GetTwitter &lt;/img&gt; Set the end-point to the Sample Endpoint and fill in the credentials corresponding with the Twitter application you created earlier. For this example we are going to specify a language of &quot;en&quot; to get only English tweets. EvaluateJsonPath &lt;/img&gt; Now we want to extract the text and language of each tweet into FlowFile attributes in order to make decisions on these values later. Set the Destination property to &quot;flowfile-attribute&quot; and add two user-defined properties using the New Property icon on the top right. The name of the property will be the FlowFile attribute name, and the value is the JSON path we would like to extract from the Tweet. This will allow us to access these values later by using expression language, such as ${twitter.text} or ${twitter.lang}. RouteOnAttribute &lt;/img&gt; At this point we would like to ensure we are only indexing tweets we are interested. So we add a user-defined property called &quot;tweet&quot; and specify the following expression language as the value: &lt;/p&gt; ${twitter.text:isEmpty():not():and(${twitter.lang:equals(&quot;en&quot;)})} &lt;/code&gt; &lt;/div&gt; The first part of this expression filters out all the tweets which have an empty message. This occurs frequently as some of the JSON documents coming through the end-point are not actual tweets. The second part of the expression ensures that we are only selecting english tweets. We already set GetTwitter to filter on &quot;en&quot;, but if we hadn&#39;t, the language could be used here to route different languages to different relationships. Any tweets matching the above conditions will be routed to the &quot;tweet&quot; relationship based on the property we defined, and anything else will be routed to the &quot;unmatched&quot; relationship. We can auto-terminate the &quot;unmatched&quot; relationship to have the non-matching FlowFiles discarded. MergeContent &lt;/img&gt; In this example we could send the JSON documents from RouteOnAttribute directly to Solr, but in most real world scenarios accessing the Solr cluster will require network communication, and it will likely produce better performance if we batch together multiple JSON documents into a single request to reduce the amount requests sent over the network. The MergeContent processor is a powerful processor that was built just for this scenario in mind, and is capable of merging FlowFiles based on a number of criteria such as the number of FlowFiles, or their age. MergeContent also performs merges in a streaming manner and as a result is capable of merging a significant number of FlowFiles without worrying about exceeding memory constraints. To configure MergeContent, set a Minimum and Maximum Number of entries, as well as a Max Bin Age to trigger a merge in cases where no new data has come in for a period of time. Also set the Delimiter Strategy to &quot;Text&quot;, and specify the Header, Footer, and Demarcator as [ , ] respectively. This allows us to create a large JSON document composed of many incoming documents. PutSolrContentStream &lt;/img&gt; Configure PutSolrContentStream to point to the Solr instance we started earlier by setting the Solr Type to &quot;Cloud&quot; and the Solr Location to the embedded ZooKeeper that was started earlier (localhost:9983). Also specify a Commit Within of &quot;1000&quot; to commit the incoming documents every second (this may not be needed if you set autoCommit settings in your solrconfig.xml). Any user-defined properties will get sent to Solr on the request as key value pairs, so we need to tell Solr how to transform the incoming JSON document into a Solr document. A good explanation of how this JSON to Solr mapping works can be found here, and an explanation of the user-defined properties can be found here. For this example we will provide the following mappings: split=/ to treat each child JSON document that we merged together as individual Solr documents id:/id to map the id field of the JSON to the id field in the Solr schema (already defined) twitter_text_t:/text to map the text field of the JSON to a dynamic field of type text_general in Solr twitter_username_s:/user/name to map the user of the tweet to a dynamic string field in Solr twitter_created_at_s:/created_at to map the creation date string to a dynamic string field in Solr twitter_timestamp_ms_tl:/timestamp_ms to map the timestamp to a dynamic trie-long field in Solr The naming conventions of the fields allow us to leverage dynamic fields in Solr. Dynamic fields take effect by using a suffix on the field name to indicate the field type, so the &quot;_t&quot; on &quot;twitter_text_t&quot; tells Solr that this field will map to a text_general field type. It is possible to not provide any field mappings and just send the JSON to Solr, and let Solr guess the field types on the fly. However, in that case all of the fields would get added as multiValued fields, and multiValued fields can&#39;t be used in a sorting clause which would prevent us from sorting on the timestamp. So we opt for the dynamic fields here. Also worth noting, this processor has two failure relationships. The regular &quot;failure&quot; relationship is for failures that would generally require some intervention to resolve, such as an invalid document being submitted. The &quot;connection_failure&quot; relationship is for failures related to communication issues between NiFi and Solr. A typical approach in a production scenario would be to route the &quot;connection_failure&quot; back to itself to retry during connection failures, and to route the &quot;failure&quot; relationship to a PutFile processor that could write out failed documents for later inspection. Summary At this point you should be able to hit Start on your processors and start seeing tweets flowing to Solr, happy indexing! A template of the flow created in this example can be found here. We would love to hear any questions, comments, or feedback that you may have! Learn more about Apache NiFi and feel free to leave comments here or e-mail us at dev@nifi.apache.org.","headline":"Indexing Tweets with NiFi and Solr","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/nifi/entry/indexing_tweets_with_nifi_and"},"url":"http://localhost:4000/nifi/entry/indexing_tweets_with_nifi_and"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Indexing Tweets with NiFi and Solr</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2015-09-21T16:35:09-04:00" itemprop="datePublished">Sep 21, 2015
      </time>â€¢ <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Bryan Bende", "login"=>"bbende", "email"=>"bbende@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1>Indexing Tweets with NiFi and Solr</h1>
<p>
   <span class="author">Bryan Bende -&nbsp;</span><br />
   <span class="author"><a href="mailto:bbende@gmail.com">bbende@gmail.com</a></span></p>
<hr />
<p>
  This post will cover how to use Apache NiFi to pull in the public stream of tweets from the Twitter API, identify<br />
  specific tweets of interest, and deliver those tweets to Solr for indexing. The example developed here was built against<br />
  <a href="https://nifi.apache.org/download.html">Apache NiFi 0.3.0</a> and <a href="http://lucene.apache.org/solr/downloads.html">Apache Solr 5.3</a>.</p>
<p>
  In addition, you will also need to create a Twitter application for accessing their API. A good set of instructions for<br />
  doing that can be found in this article - <a href="http://iag.me/socialmedia/how-to-create-a-twitter-app-in-8-easy-steps/"><br />
  How to Create a Twitter App in 8 Easy Steps</a>.</p>
<h2>Setting up Solr</h2>
<p>
For this example we will start Solr in cloud mode, and create a tweets collection based off the data-driven configuration provided by Solr.<br />
The data-driven configuration allows Solr to create fields on the fly based off the incoming data, and is a good place to start for quickly prototyping.<br />
The following two commands can be used to get Solr running and create the collection:</p>
<div>
<code></p>
<pre style="background-color: #f1f1f1">
./bin/solr start -c
./bin/solr create_collection -c tweets -d data_driven_schema_configs -shards 1 -replicationFactor 1
</pre>
<p></code>
</div>
<p>
  At this point we should have a running Solr instance on port 8983, with an embedded ZooKeeper on port 9983. Navigate<br />
  to <a href="http://localhost:8983/solr/#/~cloud">http://localhost:8983/solr/#/~cloud</a> in your browser to verify Solr<br />
  is running and the tweets collection was created sucessfully.</p>
<h2>Building the Dataflow</h2>
<p>
  The dataflow we are going build in NiFi will consist of the following processors:</p>
<ul>
<li><b>GetTwitter</b> for accessing the Twitter API and producing FlowFiles containing JSON Tweets</li>
<li><b>EvaluateJsonPath</b> for extracting values from JSON documents into FlowFile attributes</li>
<li><b>RouteOnAttribute</b> for making decisions about incoming FlowFiles based on their attributes</li>
<li><b>MergeContent</b> for merging together many JSON documents to a single document</li>
<li><b>PutSolrContentStream</b> for streaming JSON documents to Solr</li>
</ul>
<p>  The overall flow looks like the following:</p>
<p><img src="https://blogs.apache.org/nifi/mediaresource/467b2cdd-dc71-48f4-be14-159ba3f794bc" alt="nifi-twitter-solr-flow.png"></img></p>
<p>
  Lets walk through the configuration of each processor...</p>
<h3>GetTwitter</h3>
<p><img src="https://blogs.apache.org/nifi/mediaresource/2e77b3e6-8775-4f90-b805-7027a697aca5" alt="config-gettwitter.png"></img></p>
<p>
  Set the end-point to the Sample Endpoint and fill in the credentials corresponding with the Twitter application you created earlier.</p>
<p>
  For this example we are going to specify a language of "en" to get only English tweets.</p>
<h3>EvaluateJsonPath</h3>
<p><img src="https://blogs.apache.org/nifi/mediaresource/4f6bfc11-7eb6-484e-8276-ef5be176c3a6" alt="config-extract-attributes.png"></img></p>
<p>
  Now we want to extract the text and language of each tweet into FlowFile attributes in order to make decisions on these values later. Set the Destination property to "flowfile-attribute" and add two user-defined properties using the New Property icon on the top right. The name of the property will be the FlowFile attribute name, and the value is the JSON path we would like to extract from the Tweet. This will allow us to access these values later by using expression language, such as ${twitter.text} or ${twitter.lang}.</p>
<h3>RouteOnAttribute</h3>
<p><img src="https://blogs.apache.org/nifi/mediaresource/e70dd6c6-67df-4aef-94b4-4589964b66a8" alt="config-route-nonempty.png"></img></p>
<p>
  At this point we would like to ensure we are only indexing tweets we are interested. So we add a user-defined property called "tweet" and specify the following expression language as the value:</p>
<div>
<code></p>
<pre style="background-color: #f1f1f1">
${twitter.text:isEmpty():not():and(${twitter.lang:equals("en")})}
</pre>
<p></code>
</div>
<p>
  The first part of this expression filters out all the tweets which have an empty message.  This occurs frequently as some of the JSON documents coming through the end-point are not actual tweets.<br />
  The second part of the expression ensures that we are only selecting english tweets. We already set GetTwitter to filter on "en", but if we hadn't, the language could be used here to route different languages to different relationships.<br />
  Any tweets matching the above conditions will be routed to the "tweet" relationship based on the property we defined, and anything else will be routed to the "unmatched" relationship. We can auto-terminate the "unmatched" relationship to have the non-matching FlowFiles discarded.</p>
<h3>MergeContent</h3>
<p><img src="https://blogs.apache.org/nifi/mediaresource/d3304c5f-7871-421c-9f49-2f766cf40662" alt="config-merge.png"></img></p>
<p>
 In this example we could send the JSON documents from RouteOnAttribute directly to Solr, but in most real world scenarios accessing the Solr cluster will require network communication, and it will likely produce better performance if we batch together multiple JSON documents into a single request to reduce the amount requests sent over the network. The MergeContent processor is a powerful processor that was built just for this scenario in mind, and is capable of merging FlowFiles based on a number of criteria such as the number of FlowFiles, or their age. MergeContent also performs merges in a streaming manner and as a result is capable of merging a significant number of FlowFiles without worrying about exceeding memory constraints.</p>
<p>
 To configure MergeContent,  set a Minimum and Maximum Number of entries, as well as a Max Bin Age to trigger a merge in cases where no new data has come in for a period of time.  Also set the Delimiter Strategy to "Text", and specify the Header, Footer, and Demarcator as [ , ] respectively. This allows us to create a large JSON document composed of many incoming documents.</p>
<h3>PutSolrContentStream</h3>
<p><img src="https://blogs.apache.org/nifi/mediaresource/5f3c0112-620a-4738-b60c-8087feb2d3db?" alt="config-putsolr.png"></img></p>
<p>
Configure PutSolrContentStream to point to the Solr instance we started earlier by setting the Solr Type to "Cloud" and the Solr Location to the embedded ZooKeeper that was started earlier (localhost:9983). Also specify a Commit Within of "1000" to commit the incoming documents every second (this may not be needed if you set autoCommit settings in your solrconfig.xml).</p>
<p>
 Any user-defined properties will get sent to Solr on the request as key value pairs, so we need to tell Solr how to transform the incoming JSON document into a Solr document. A good explanation of how this JSON to Solr mapping works can be found<br />
<a href="http://lucidworks.com/blog/indexing-custom-json-data/">here</a>, and an explanation of the user-defined<br />
 properties can be found <a href="https://nifi.apache.org/docs/nifi-docs/components/org.apache.nifi.processors.solr.PutSolrContentStream/additionalDetails.html">here</a>.</p>
<p>
  For this example we will provide the following mappings:</p>
<ul>
<li><b>split=/</b> to treat each child JSON document that we merged together as individual Solr documents</li>
<li><b>id:/id</b> to map the id field of the JSON to the id field in the Solr schema (already defined)</li>
<li><b>twitter_text_t:/text</b> to map the text field of the JSON to a dynamic field of type text_general in Solr</li>
<li><b>twitter_username_s:/user/name</b> to map the user of the tweet to a dynamic string field in Solr</li>
<li><b>twitter_created_at_s:/created_at</b> to map the creation date string to a dynamic string field in Solr</li>
<li><b>twitter_timestamp_ms_tl:/timestamp_ms</b> to map the timestamp to a dynamic trie-long field in Solr</li>
</ul>
<p>
The naming conventions of the fields allow us to leverage dynamic fields in Solr. Dynamic fields take effect by using a suffix on the field name to indicate the field type, so the "_t" on "twitter_text_t" tells Solr that this field will map to a text_general field type. It is possible to not provide any field mappings and just send the JSON to Solr, and let Solr guess the field types on the fly. However, in that case all of the fields would get added as multiValued fields, and multiValued fields can't be used in a sorting clause which would prevent us from sorting on the timestamp. So we opt for the dynamic fields here.</p>
<p>
Also worth noting, this processor has two failure relationships. The regular "failure" relationship is for failures that would generally require some intervention to resolve, such as an invalid document being submitted. The "connection_failure" relationship is for failures related to communication issues between NiFi and Solr. A typical approach in a production scenario would be to route the "connection_failure" back to itself to retry during connection failures, and to route the "failure" relationship to a PutFile processor that could write out failed documents for later inspection.</p>
<h2>Summary</h2>
<p>
  At this point you should be able to hit Start on your processors and start seeing tweets flowing to Solr, happy indexing!</p>
<p>
  A template of the flow created in this example can be found <a href="https://cwiki.apache.org/confluence/display/NIFI/Example+Dataflow+Templates">here</a>. We would love to hear any questions, comments, or feedback that you may have!</p>
<p>
  <a href="http://nifi.apache.org">Learn more about Apache NiFi</a> and feel free to leave comments here or e-mail us at dev@nifi.apache.org.</p>

  </div><a class="u-url" href="/nifi/entry/indexing_tweets_with_nifi_and" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
