<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Scaling Accumulo With Multi-Volume Support | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Scaling Accumulo With Multi-Volume Support" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&nbsp;This post was moved to the Accumulo project site. MapReduce is a commonly used approach to querying or analyzing large amounts of data. Typically MapReduce jobs are created using using some set of files in HDFS to produce a result. When new files come in, they get added to the set, and the job gets run again. A common Accumulo approach to this scenario is to load all of the data into a single instance of Accumulo. A single instance of Accumulo can scale quite largely[1,2] to accommodate high levels of ingest and query. The manner in which ingest is performed typically depends on latency requirements. When the desired latency is small, inserts are performed directly into Accumulo. When the desired latency is allowed to be large, then a bulk style of ingest[3] can be used. There are other factors to consider as well, but they are outside the scope of this article. On large clusters using the bulk style of ingest input files are typically batched into MapReduce jobs to create a set of output RFiles for import into Accumulo. The number of files per job is typically determined by the required latency and the number of MapReduce tasks that the cluster can complete in the given time-frame. The resulting RFiles, when imported into Accumulo, are added to the list of files for their associated tablets. Depending on the configuration this will cause Accumulo to major compact these tablets. If the configuration is tweaked to allow more files per tablet, to reduce the major compactions, then more files need to be opened at query time when performing scans on the tablet. Note that no single node is burdened by the file management; but, the number of file operations in aggregate is very large. If each server has several hundred tablets, and there are a thousand tablet servers, and each tablet compacts some files every few imports, we easily have 50,000 file operations (create, allocate a block, rename and delete) every ingest cycle. In addition to the NameNode operations caused by bulk ingest, other Accumulo processes (e.g. master, gc) require interaction with the NameNode. Single processes, like the garbage collector, can be starved of responses from the NameNode as the NameNode is limited on the number of concurrent operations. It is not unusual for an operator&#39;s request for &ldquo;hadoop fs -ls /accumulo&rdquo; to take a minute before returning results during the peak file-management periods. In particular, the file garbage collector can fall behind, not finishing a cycle of unreferenced file removal before the next ingest cycle creates a new batch of files to be deleted. The Hadoop community addressed the NameNode bottleneck issue with HDFS federation[4] which allows a datanode to serve up blocks for multiple namenodes. Additionally, ViewFS allows clients to communicate with multiple namenodes through the use of a client-side mount table. This functionality was insufficient for Accumulo in the 1.6.0 release as ViewFS works at a directory level; as an example, /dirA is mapped to one NameNode and /dirB is mapped to another, and Accumulo uses a single HDFS directory for its storage. Multi-Volume support (MVS), included in 1.6.0, includes the changes that allow Accumulo to work across multiple HDFS clusters (called volumes in Accumulo) while continuing to use a single HDFS directory. A new property, instance.volumes, can be configured with multiple HDFS nameservices and Accumulo will use them all to balance out NameNode operations. The nameservices configured in instance.volumes may optionally use the High Availability NameNode feature as it is transparent to Accumulo. With MVS you have two options to horizontally scale your Accumulo instance. You can use an HDFS cluster with Federation and multiple NameNodes or you can use separate HDFS clusters. By default Accumulo will perform round-robin file allocation for each tablet, spreading the files across the different volumes. The file balancer is pluggable, allowing for custom implementations. For example, if you don&#39;t use Federation and use multiple HDFS clusters, you may want to allocate all files for a particular table to one volume. Comments in the JIRA[5] regarding backups could lead to follow-on work. With the inclusion of snapshots in HDFS, you could easily envision an application that quiesces the database or some set of tables, flushes their entries from memory, and snapshots their directories. These snapshots could then be copied to another HDFS instance either for an on-disk backup, or bulk-imported into another instance of Accumulo for testing or some other use. The example configuration below shows how to set up Accumulo with HA NameNodes and Federation, as it is likely the most complex. We had to reference several web sites, one of the HDFS mailing lists, and the source code to find all of the configuration parameters that were needed. The configuration below includes two sets of HA namenodes, each set servicing an HDFS nameservice in a single HDFS cluster. In the example below, nameserviceA is serviced by name nodes 1 and 2, and nameserviceB is serviced by name nodes 3 and 4. [1] http://ieeexplore.ieee.org/zpl/login.jsp?arnumber=6597155 [2] http://www.pdl.cmu.edu/SDI/2013/slides/big_graph_nsa_rd_2013_56002v1.pdf &lt;/p&gt; [3] http://accumulo.apache.org/1.6/examples/bulkIngest.html &lt;/p&gt; [4] https://issues.apache.org/jira/browse/HDFS-1052 &lt;/p&gt; [5] https://issues.apache.org/jira/browse/ACCUMULO-118 &lt;/p&gt; - By Dave Marion and Eric Newton core-site.xml: &lt;/font&gt; fs.defaultFS viewfs:/// &lt;/property&gt; &lt;/font&gt; fs.viewfs.mounttable.default.link./nameserviceA hdfs://nameserviceA &lt;/property&gt; &lt;/font&gt; fs.viewfs.mounttable.default.link./nameserviceB hdfs://nameserviceB &lt;/property&gt; &lt;/font&gt; fs.viewfs.mounttable.default.link./nameserviceA/accumulo/instance_id hdfs://nameserviceA/accumulo/instance_id Workaround for ACCUMULO-2719 &lt;/property&gt; &lt;/font&gt; dfs.ha.fencing.methods sshfence(hdfs:22)&lt;/font&gt; shell(/bin/true)&lt;/value&gt; &lt;/property&gt; &lt;/font&gt; dfs.ha.fencing.ssh.private-key-files &lt;/value&gt;&lt;/font&gt; &lt;/property&gt; &lt;/font&gt; dfs.ha.fencing.ssh.connect-timeout 30000 &lt;/property&gt; &lt;/font&gt; ha.zookeeper.quorum zkHost1:2181,zkHost2:2181,zkHost3:2181 &lt;/property&gt; &lt;/pre&gt; hdfs-site.xml: &lt;/font&gt; dfs.nameservices nameserviceA,nameserviceB &lt;/property&gt; &lt;/font&gt; dfs.ha.namenodes.nameserviceA nn1,nn2 &lt;/property&gt; &lt;/font&gt; dfs.ha.namenodes.nameserviceB nn3,nn4 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceA.nn1 host1:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceA.nn2 host2:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceA.nn1 host1:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceA.nn2 host2:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceB.nn3 host3:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceB.nn4 host4:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceB.nn3 host3:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceB.nn4 host4:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceA.nn1 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceA &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceA.nn2 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceA &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceB.nn3 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceB &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceB.nn4 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceB &lt;/property&gt; &lt;/font&gt; dfs.client.failover.proxy.provider.nameserviceA org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider &lt;/property&gt; &lt;/font&gt; dfs.client.failover.proxy.provider.nameserviceB org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider &lt;/property&gt; &lt;/font&gt; dfs.ha.automatic-failover.enabled.nameserviceA true &lt;/property&gt; &lt;/font&gt; dfs.ha.automatic-failover.enabled.nameserviceB true &lt;/property&gt; &lt;/pre&gt; accumulo-site.xml: &lt;/font&gt; instance.volumes hdfs://nameserviceA/accumulo,hdfs://nameserviceB/accumulo &lt;/property&gt;&lt;/pre&gt;" />
<meta property="og:description" content="&nbsp;This post was moved to the Accumulo project site. MapReduce is a commonly used approach to querying or analyzing large amounts of data. Typically MapReduce jobs are created using using some set of files in HDFS to produce a result. When new files come in, they get added to the set, and the job gets run again. A common Accumulo approach to this scenario is to load all of the data into a single instance of Accumulo. A single instance of Accumulo can scale quite largely[1,2] to accommodate high levels of ingest and query. The manner in which ingest is performed typically depends on latency requirements. When the desired latency is small, inserts are performed directly into Accumulo. When the desired latency is allowed to be large, then a bulk style of ingest[3] can be used. There are other factors to consider as well, but they are outside the scope of this article. On large clusters using the bulk style of ingest input files are typically batched into MapReduce jobs to create a set of output RFiles for import into Accumulo. The number of files per job is typically determined by the required latency and the number of MapReduce tasks that the cluster can complete in the given time-frame. The resulting RFiles, when imported into Accumulo, are added to the list of files for their associated tablets. Depending on the configuration this will cause Accumulo to major compact these tablets. If the configuration is tweaked to allow more files per tablet, to reduce the major compactions, then more files need to be opened at query time when performing scans on the tablet. Note that no single node is burdened by the file management; but, the number of file operations in aggregate is very large. If each server has several hundred tablets, and there are a thousand tablet servers, and each tablet compacts some files every few imports, we easily have 50,000 file operations (create, allocate a block, rename and delete) every ingest cycle. In addition to the NameNode operations caused by bulk ingest, other Accumulo processes (e.g. master, gc) require interaction with the NameNode. Single processes, like the garbage collector, can be starved of responses from the NameNode as the NameNode is limited on the number of concurrent operations. It is not unusual for an operator&#39;s request for &ldquo;hadoop fs -ls /accumulo&rdquo; to take a minute before returning results during the peak file-management periods. In particular, the file garbage collector can fall behind, not finishing a cycle of unreferenced file removal before the next ingest cycle creates a new batch of files to be deleted. The Hadoop community addressed the NameNode bottleneck issue with HDFS federation[4] which allows a datanode to serve up blocks for multiple namenodes. Additionally, ViewFS allows clients to communicate with multiple namenodes through the use of a client-side mount table. This functionality was insufficient for Accumulo in the 1.6.0 release as ViewFS works at a directory level; as an example, /dirA is mapped to one NameNode and /dirB is mapped to another, and Accumulo uses a single HDFS directory for its storage. Multi-Volume support (MVS), included in 1.6.0, includes the changes that allow Accumulo to work across multiple HDFS clusters (called volumes in Accumulo) while continuing to use a single HDFS directory. A new property, instance.volumes, can be configured with multiple HDFS nameservices and Accumulo will use them all to balance out NameNode operations. The nameservices configured in instance.volumes may optionally use the High Availability NameNode feature as it is transparent to Accumulo. With MVS you have two options to horizontally scale your Accumulo instance. You can use an HDFS cluster with Federation and multiple NameNodes or you can use separate HDFS clusters. By default Accumulo will perform round-robin file allocation for each tablet, spreading the files across the different volumes. The file balancer is pluggable, allowing for custom implementations. For example, if you don&#39;t use Federation and use multiple HDFS clusters, you may want to allocate all files for a particular table to one volume. Comments in the JIRA[5] regarding backups could lead to follow-on work. With the inclusion of snapshots in HDFS, you could easily envision an application that quiesces the database or some set of tables, flushes their entries from memory, and snapshots their directories. These snapshots could then be copied to another HDFS instance either for an on-disk backup, or bulk-imported into another instance of Accumulo for testing or some other use. The example configuration below shows how to set up Accumulo with HA NameNodes and Federation, as it is likely the most complex. We had to reference several web sites, one of the HDFS mailing lists, and the source code to find all of the configuration parameters that were needed. The configuration below includes two sets of HA namenodes, each set servicing an HDFS nameservice in a single HDFS cluster. In the example below, nameserviceA is serviced by name nodes 1 and 2, and nameserviceB is serviced by name nodes 3 and 4. [1] http://ieeexplore.ieee.org/zpl/login.jsp?arnumber=6597155 [2] http://www.pdl.cmu.edu/SDI/2013/slides/big_graph_nsa_rd_2013_56002v1.pdf &lt;/p&gt; [3] http://accumulo.apache.org/1.6/examples/bulkIngest.html &lt;/p&gt; [4] https://issues.apache.org/jira/browse/HDFS-1052 &lt;/p&gt; [5] https://issues.apache.org/jira/browse/ACCUMULO-118 &lt;/p&gt; - By Dave Marion and Eric Newton core-site.xml: &lt;/font&gt; fs.defaultFS viewfs:/// &lt;/property&gt; &lt;/font&gt; fs.viewfs.mounttable.default.link./nameserviceA hdfs://nameserviceA &lt;/property&gt; &lt;/font&gt; fs.viewfs.mounttable.default.link./nameserviceB hdfs://nameserviceB &lt;/property&gt; &lt;/font&gt; fs.viewfs.mounttable.default.link./nameserviceA/accumulo/instance_id hdfs://nameserviceA/accumulo/instance_id Workaround for ACCUMULO-2719 &lt;/property&gt; &lt;/font&gt; dfs.ha.fencing.methods sshfence(hdfs:22)&lt;/font&gt; shell(/bin/true)&lt;/value&gt; &lt;/property&gt; &lt;/font&gt; dfs.ha.fencing.ssh.private-key-files &lt;/value&gt;&lt;/font&gt; &lt;/property&gt; &lt;/font&gt; dfs.ha.fencing.ssh.connect-timeout 30000 &lt;/property&gt; &lt;/font&gt; ha.zookeeper.quorum zkHost1:2181,zkHost2:2181,zkHost3:2181 &lt;/property&gt; &lt;/pre&gt; hdfs-site.xml: &lt;/font&gt; dfs.nameservices nameserviceA,nameserviceB &lt;/property&gt; &lt;/font&gt; dfs.ha.namenodes.nameserviceA nn1,nn2 &lt;/property&gt; &lt;/font&gt; dfs.ha.namenodes.nameserviceB nn3,nn4 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceA.nn1 host1:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceA.nn2 host2:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceA.nn1 host1:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceA.nn2 host2:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceB.nn3 host3:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceB.nn4 host4:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceB.nn3 host3:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceB.nn4 host4:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceA.nn1 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceA &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceA.nn2 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceA &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceB.nn3 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceB &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceB.nn4 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceB &lt;/property&gt; &lt;/font&gt; dfs.client.failover.proxy.provider.nameserviceA org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider &lt;/property&gt; &lt;/font&gt; dfs.client.failover.proxy.provider.nameserviceB org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider &lt;/property&gt; &lt;/font&gt; dfs.ha.automatic-failover.enabled.nameserviceA true &lt;/property&gt; &lt;/font&gt; dfs.ha.automatic-failover.enabled.nameserviceB true &lt;/property&gt; &lt;/pre&gt; accumulo-site.xml: &lt;/font&gt; instance.volumes hdfs://nameserviceA/accumulo,hdfs://nameserviceB/accumulo &lt;/property&gt;&lt;/pre&gt;" />
<link rel="canonical" href="http://localhost:4000/accumulo/entry/scaling_accumulo_with_multi_volume" />
<meta property="og:url" content="http://localhost:4000/accumulo/entry/scaling_accumulo_with_multi_volume" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-12-14T16:22:30-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Scaling Accumulo With Multi-Volume Support" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2018-12-14T16:22:30-05:00","datePublished":"2018-12-14T16:22:30-05:00","description":"&nbsp;This post was moved to the Accumulo project site. MapReduce is a commonly used approach to querying or analyzing large amounts of data. Typically MapReduce jobs are created using using some set of files in HDFS to produce a result. When new files come in, they get added to the set, and the job gets run again. A common Accumulo approach to this scenario is to load all of the data into a single instance of Accumulo. A single instance of Accumulo can scale quite largely[1,2] to accommodate high levels of ingest and query. The manner in which ingest is performed typically depends on latency requirements. When the desired latency is small, inserts are performed directly into Accumulo. When the desired latency is allowed to be large, then a bulk style of ingest[3] can be used. There are other factors to consider as well, but they are outside the scope of this article. On large clusters using the bulk style of ingest input files are typically batched into MapReduce jobs to create a set of output RFiles for import into Accumulo. The number of files per job is typically determined by the required latency and the number of MapReduce tasks that the cluster can complete in the given time-frame. The resulting RFiles, when imported into Accumulo, are added to the list of files for their associated tablets. Depending on the configuration this will cause Accumulo to major compact these tablets. If the configuration is tweaked to allow more files per tablet, to reduce the major compactions, then more files need to be opened at query time when performing scans on the tablet. Note that no single node is burdened by the file management; but, the number of file operations in aggregate is very large. If each server has several hundred tablets, and there are a thousand tablet servers, and each tablet compacts some files every few imports, we easily have 50,000 file operations (create, allocate a block, rename and delete) every ingest cycle. In addition to the NameNode operations caused by bulk ingest, other Accumulo processes (e.g. master, gc) require interaction with the NameNode. Single processes, like the garbage collector, can be starved of responses from the NameNode as the NameNode is limited on the number of concurrent operations. It is not unusual for an operator&#39;s request for &ldquo;hadoop fs -ls /accumulo&rdquo; to take a minute before returning results during the peak file-management periods. In particular, the file garbage collector can fall behind, not finishing a cycle of unreferenced file removal before the next ingest cycle creates a new batch of files to be deleted. The Hadoop community addressed the NameNode bottleneck issue with HDFS federation[4] which allows a datanode to serve up blocks for multiple namenodes. Additionally, ViewFS allows clients to communicate with multiple namenodes through the use of a client-side mount table. This functionality was insufficient for Accumulo in the 1.6.0 release as ViewFS works at a directory level; as an example, /dirA is mapped to one NameNode and /dirB is mapped to another, and Accumulo uses a single HDFS directory for its storage. Multi-Volume support (MVS), included in 1.6.0, includes the changes that allow Accumulo to work across multiple HDFS clusters (called volumes in Accumulo) while continuing to use a single HDFS directory. A new property, instance.volumes, can be configured with multiple HDFS nameservices and Accumulo will use them all to balance out NameNode operations. The nameservices configured in instance.volumes may optionally use the High Availability NameNode feature as it is transparent to Accumulo. With MVS you have two options to horizontally scale your Accumulo instance. You can use an HDFS cluster with Federation and multiple NameNodes or you can use separate HDFS clusters. By default Accumulo will perform round-robin file allocation for each tablet, spreading the files across the different volumes. The file balancer is pluggable, allowing for custom implementations. For example, if you don&#39;t use Federation and use multiple HDFS clusters, you may want to allocate all files for a particular table to one volume. Comments in the JIRA[5] regarding backups could lead to follow-on work. With the inclusion of snapshots in HDFS, you could easily envision an application that quiesces the database or some set of tables, flushes their entries from memory, and snapshots their directories. These snapshots could then be copied to another HDFS instance either for an on-disk backup, or bulk-imported into another instance of Accumulo for testing or some other use. The example configuration below shows how to set up Accumulo with HA NameNodes and Federation, as it is likely the most complex. We had to reference several web sites, one of the HDFS mailing lists, and the source code to find all of the configuration parameters that were needed. The configuration below includes two sets of HA namenodes, each set servicing an HDFS nameservice in a single HDFS cluster. In the example below, nameserviceA is serviced by name nodes 1 and 2, and nameserviceB is serviced by name nodes 3 and 4. [1] http://ieeexplore.ieee.org/zpl/login.jsp?arnumber=6597155 [2] http://www.pdl.cmu.edu/SDI/2013/slides/big_graph_nsa_rd_2013_56002v1.pdf &lt;/p&gt; [3] http://accumulo.apache.org/1.6/examples/bulkIngest.html &lt;/p&gt; [4] https://issues.apache.org/jira/browse/HDFS-1052 &lt;/p&gt; [5] https://issues.apache.org/jira/browse/ACCUMULO-118 &lt;/p&gt; - By Dave Marion and Eric Newton core-site.xml: &lt;/font&gt; fs.defaultFS viewfs:/// &lt;/property&gt; &lt;/font&gt; fs.viewfs.mounttable.default.link./nameserviceA hdfs://nameserviceA &lt;/property&gt; &lt;/font&gt; fs.viewfs.mounttable.default.link./nameserviceB hdfs://nameserviceB &lt;/property&gt; &lt;/font&gt; fs.viewfs.mounttable.default.link./nameserviceA/accumulo/instance_id hdfs://nameserviceA/accumulo/instance_id Workaround for ACCUMULO-2719 &lt;/property&gt; &lt;/font&gt; dfs.ha.fencing.methods sshfence(hdfs:22)&lt;/font&gt; shell(/bin/true)&lt;/value&gt; &lt;/property&gt; &lt;/font&gt; dfs.ha.fencing.ssh.private-key-files &lt;/value&gt;&lt;/font&gt; &lt;/property&gt; &lt;/font&gt; dfs.ha.fencing.ssh.connect-timeout 30000 &lt;/property&gt; &lt;/font&gt; ha.zookeeper.quorum zkHost1:2181,zkHost2:2181,zkHost3:2181 &lt;/property&gt; &lt;/pre&gt; hdfs-site.xml: &lt;/font&gt; dfs.nameservices nameserviceA,nameserviceB &lt;/property&gt; &lt;/font&gt; dfs.ha.namenodes.nameserviceA nn1,nn2 &lt;/property&gt; &lt;/font&gt; dfs.ha.namenodes.nameserviceB nn3,nn4 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceA.nn1 host1:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceA.nn2 host2:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceA.nn1 host1:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceA.nn2 host2:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceB.nn3 host3:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.rpc-address.nameserviceB.nn4 host4:8020 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceB.nn3 host3:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.http-address.nameserviceB.nn4 host4:50070 &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceA.nn1 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceA &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceA.nn2 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceA &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceB.nn3 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceB &lt;/property&gt; &lt;/font&gt; dfs.namenode.shared.edits.dir.nameserviceB.nn4 qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceB &lt;/property&gt; &lt;/font&gt; dfs.client.failover.proxy.provider.nameserviceA org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider &lt;/property&gt; &lt;/font&gt; dfs.client.failover.proxy.provider.nameserviceB org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider &lt;/property&gt; &lt;/font&gt; dfs.ha.automatic-failover.enabled.nameserviceA true &lt;/property&gt; &lt;/font&gt; dfs.ha.automatic-failover.enabled.nameserviceB true &lt;/property&gt; &lt;/pre&gt; accumulo-site.xml: &lt;/font&gt; instance.volumes hdfs://nameserviceA/accumulo,hdfs://nameserviceB/accumulo &lt;/property&gt;&lt;/pre&gt;","headline":"Scaling Accumulo With Multi-Volume Support","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/accumulo/entry/scaling_accumulo_with_multi_volume"},"url":"http://localhost:4000/accumulo/entry/scaling_accumulo_with_multi_volume"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Scaling Accumulo With Multi-Volume Support</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-12-14T16:22:30-05:00" itemprop="datePublished">Dec 14, 2018
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Dave Marion", "login"=>"dlmarion", "email"=>"dlmarion@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>&nbsp;This post was moved <a href="https://accumulo.apache.org/blog/2014/06/25/scaling-accumulo-with-multivolume-support.html" title="Updated location">to the Accumulo project site</a>. </p>
<p class="western">MapReduce is a commonly used approach to<br />
querying or analyzing large amounts of data. Typically MapReduce jobs<br />
are created using using some set of files in HDFS to produce a<br />
result. When new files come in, they get added to the set, and the<br />
job gets run again. A common Accumulo approach to this scenario is to<br />
load all of the data into a single instance of Accumulo.</p>
<p class="western">  A single instance of Accumulo can scale quite<br />
largely[1,2] to accommodate high levels of ingest and query. The manner<br />
in which ingest is performed typically depends on latency<br />
requirements. When the desired latency is small, inserts are<br />
performed directly into Accumulo. When the desired latency is allowed<br />
to be large, then a bulk style of ingest[3] can be used. There are<br />
other factors to consider as well, but they are outside the scope of<br />
this article.</p>
<p class="western">  On large clusters using the bulk style of ingest<br />
input files are typically batched into MapReduce jobs to create a set<br />
of output RFiles for import into Accumulo. The number of files per<br />
job is typically determined by the required latency and the number of<br />
MapReduce tasks that the cluster can complete in the given<br />
time-frame. The resulting RFiles, when imported into Accumulo, are<br />
added to the list of files for their associated tablets. Depending on<br />
the configuration this will cause Accumulo to major compact these<br />
tablets. If the configuration is tweaked to allow more files per<br />
tablet, to reduce the major compactions, then more files need to be<br />
opened at query time when performing scans on the tablet.  Note that<br />
no single node is burdened by the file management; but, the number of<br />
file operations in aggregate is very large.  If each server has<br />
several hundred tablets, and there are a thousand tablet servers, and<br />
each tablet compacts some files every few imports, we easily have<br />
50,000 file operations (create, allocate a block, rename and delete)<br />
every ingest cycle.</p>
<p class="western">  In addition to the NameNode operations caused by<br />
bulk ingest, other Accumulo processes (e.g. master, gc) require<br />
interaction with the NameNode. Single processes, like the garbage<br />
collector, can be starved of responses from the NameNode as the NameNode is<br />
limited on the number of concurrent operations. It is not unusual for<br />
an operator's request for &ldquo;<font face="Courier 10 Pitch, MS Mincho">hadoop<br />
fs -ls /accumulo</font>&rdquo; to take a minute before returning results<br />
during the peak file-management periods. In particular, the file<br />
garbage collector can fall behind, not finishing a cycle of<br />
unreferenced file removal before the next ingest cycle creates a new<br />
batch of files to be deleted.</p>
<p class="western"> The Hadoop community addressed the NameNode<br />
bottleneck issue with HDFS  federation[4] which allows a datanode to<br />
serve up blocks for multiple namenodes. Additionally, ViewFS allows<br />
clients to communicate with multiple namenodes through the use of a<br />
client-side mount table. This functionality was insufficient for<br />
Accumulo in the 1.6.0 release as ViewFS works at a directory level; as an example, /dirA is mapped to<br />
one NameNode and /dirB is mapped to another, and Accumulo uses a<br />
single HDFS directory for its storage.</p>
<p class="western">  Multi-Volume support (MVS), included in 1.6.0,<br />
includes the changes that allow Accumulo to work across multiple HDFS<br />
clusters (called volumes in Accumulo) while continuing to use a<br />
single HDFS directory. A new property, instance.volumes, can be<br />
configured with multiple HDFS nameservices and Accumulo will use them<br />
all to balance out NameNode operations. The nameservices configured<br />
in instance.volumes may optionally use the High Availability NameNode feature as it is transparent<br />
to Accumulo. With MVS you have two options to horizontally scale your<br />
Accumulo instance. You can use an HDFS cluster with Federation and<br />
multiple NameNodes or you can use separate HDFS clusters.</p>
<p class="western"> By default Accumulo will perform round-robin file<br />
allocation for each tablet, spreading the files across the different<br />
volumes. The file balancer is pluggable, allowing for custom<br />
implementations. For example, if you don't use Federation and use<br />
multiple HDFS clusters, you may want to allocate all files for a<br />
particular table to one volume.</p>
<p class="western">  Comments in the JIRA[5] regarding backups could<br />
lead to follow-on work. With the inclusion of snapshots in HDFS, you<br />
could easily envision an application that quiesces the database or<br />
some set of tables, flushes their entries from memory, and snapshots<br />
their directories. These snapshots could then be copied to another<br />
HDFS instance either for an on-disk backup, or bulk-imported into<br />
another instance of Accumulo for testing or some other use.</p>
<p class="western">  The example configuration below shows how to<br />
set up Accumulo with HA NameNodes and Federation, as it is likely the<br />
most complex. We had to reference several web sites, one of the HDFS<br />
mailing lists, and the source code to find all of the configuration<br />
parameters that were needed. The configuration below includes two<br />
sets of HA namenodes, each set servicing an HDFS nameservice in a<br />
single HDFS cluster. In the example below, nameserviceA is serviced<br />
by name nodes 1 and 2, and nameserviceB is serviced by name nodes 3<br />
and 4.</p>
<p class="western">[1]<br />
<font color="#000080"><span lang="zxx"><u><a class="western" href="http://ieeexplore.ieee.org/zpl/login.jsp?arnumber=6597155">http://ieeexplore.ieee.org/zpl/login.jsp?arnumber=6597155</a></u><span style="text-decoration: none;"><span style="color: #000000;"></span></span></span></font></p>
<p class="western"><font color="#000080"><span lang="zxx"><span style="text-decoration: none;"><span style="color: #000000;">[2</span>]<br />
</span></span></font><font color="#000080"><span lang="zxx"><u><a class="western" href="http://www.pdl.cmu.edu/SDI/2013/slides/big_graph_nsa_rd_2013_56002v1.pdf">http://www.pdl.cmu.edu/SDI/2013/slides/big_graph_nsa_rd_2013_56002v1.pdf</a></u></span></font></p></p>
<p class="western">[3]<br />
<font color="#000080"><span lang="zxx"><u><a class="western" href="http://accumulo.apache.org/1.5/examples/bulkIngest.html">http://accumulo.apache.org/1.<font color="#000080">6</font>/examples/bulkIngest.html</a></u></span></font></p></p>
<p class="western">[4]<br />
<font color="#000080"><span lang="zxx"><u><a class="western" href="https://issues.apache.org/jira/browse/HDFS-1052">https://issues.apache.org/jira/browse/HDFS-1052</a></u></span></font></p></p>
<p class="western">[5]<br />
<font color="#000080"><span lang="zxx"><u><a class="western" href="https://issues.apache.org/jira/browse/ACCUMULO-118">https://issues.apache.org/jira/browse/ACCUMULO-118</a></u></span></font></p></p>
<p class="western"><em>- By Dave Marion and Eric Newton</em></p>
<h2>core-site.xml:<br />
</h2>
<pre class="text-body-indent-western">  <font style="font-size: 11pt" size="2"><property></font>
    <font style="font-size: 11pt" size="2"><name>fs.defaultFS</name></font><font style="font-size: 11pt" size="2">
</font>    <font style="font-size: 11pt" size="2"><value>viewfs:///</value></font>
  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font>
<font style="font-size: 11pt" size="2">    <name>fs.viewfs.mounttable.default.link./nameserviceA</name></font>
<font style="font-size: 11pt" size="2">    <value>hdfs://nameserviceA</value></font>
  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font><font style="font-size: 11pt" size="2">
</font><font style="font-size: 11pt" size="2">    <name>fs.viewfs.mounttable.default.link./nameserviceB</name></font><font style="font-size: 11pt" size="2">
</font><font size="2">      </font><font style="font-size: 11pt" size="2"><value>hdfs://nameserviceB</value></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font><font style="font-size: 11pt" size="2">
</font><font style="font-size: 11pt" size="2">    <name>fs.viewfs.mounttable.default.link./nameserviceA/accumulo/instance_id</name></font><font style="font-size: 11pt" size="2">
</font><font size="2">      </font><font style="font-size: 11pt" size="2"><value>hdfs://nameserviceA/accumulo/instance_id</value></font>
<font style="font-size: 11pt" size="2">    <description>Workaround for ACCUMULO-2719</description></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font><font style="font-size: 11pt" size="2">
</font><font style="font-size: 11pt" size="2">    <name>dfs.ha.fencing.methods</name></font><font style="font-size: 11pt" size="2">
</font><font style="font-size: 11pt" size="2">    <value>sshfence(hdfs:22)</font>      
<font style="font-size: 11pt" size="2">           shell(/bin/true)</value></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font>   
<font style="font-size: 11pt" size="2">    <name>dfs.ha.fencing.ssh.private-key-files</name></font><font style="font-size: 11pt" size="2">
</font><font size="2">      </font><font style="font-size: 11pt" size="2"><value><PRIVATE_KEY_LOCATION></value></font>
  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.ha.fencing.ssh.connect-timeout</name></font>
<font style="font-size: 11pt" size="2">    <value>30000</value></font><font style="font-size: 11pt" size="2">
</font><font style="font-size: 11pt" size="2">  </property></font>
<font style="font-size: 11pt" size="2">  <property></font><font style="font-size: 11pt" size="2">
</font><font style="font-size: 11pt" size="2">    <name>ha.zookeeper.quorum</name></font>   
<font style="font-size: 11pt" size="2">    <value>zkHost1:2181,zkHost2:2181,zkHost3:2181</value></font>
<font style="font-size: 11pt" size="2">  </property></font><font style="font-size: 11pt" size="2">
</font>
</pre>
<h2>hdfs-site.xml:<br />
</h2>
<pre class="text-body-indent-western"><font style="font-size: 11pt" size="2">  <property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.nameservices</name></font>
<font style="font-size: 11pt" size="2">    <value>nameserviceA,nameserviceB</value></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font><font style="font-size: 11pt" size="2">
</font><font style="font-size: 11pt" size="2">    <name>dfs.ha.namenodes.nameserviceA</name></font>
<font style="font-size: 11pt" size="2">    <value>nn1,nn2</value></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font> 
<font style="font-size: 11pt" size="2">    <name>dfs.ha.namenodes.nameserviceB</name></font><font style="font-size: 11pt" size="2"></font>
<font style="font-size: 11pt" size="2">    <value>nn3,nn4</value></font>
  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.rpc-address.nameserviceA.nn1</name></font><font style="font-size: 11pt" size="2"></font>
<font style="font-size: 11pt" size="2">    <value>host1:8020</value></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"></property></font>
  <font style="font-size: 11pt" size="2"><property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.rpc-address.nameserviceA.nn2</name></font>
<font style="font-size: 11pt" size="2">    <value>host2:8020</value></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.http-address.nameserviceA.nn1</name></font>
<font style="font-size: 11pt" size="2">    <value>host1:50070</value></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.http-address.nameserviceA.nn2</name></font>
<font style="font-size: 11pt" size="2">    <value>host2:50070</value></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.rpc-address.nameserviceB.nn3</name></font>
<font style="font-size: 11pt" size="2">    <value>host3:8020</value></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.rpc-address.nameserviceB.nn4</name></font>
<font style="font-size: 11pt" size="2">    <value>host4:8020</value></font>
<font style="font-size: 11pt" size="2">  </property></font>
<font style="font-size: 11pt" size="2">  <property></font> 
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.http-address.nameserviceB.nn3</name></font>
<font style="font-size: 11pt" size="2">    <value>host3:50070</value></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"></property></font><font style="font-size: 11pt" size="2">
</font>  <font style="font-size: 11pt" size="2"><property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.http-address.nameserviceB.nn4</name></font>
<font style="font-size: 11pt" size="2">    <value>host4:50070</value></font>
<font style="font-size: 11pt" size="2">  </property></font>
<font style="font-size: 11pt" size="2">  <property></font> 
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.shared.edits.dir.nameserviceA.nn1</name></font>
<font style="font-size: 11pt" size="2">    <value>qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceA</value></font>
<font style="font-size: 11pt" size="2">  </property></font>
<font style="font-size: 11pt" size="2">  <property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.shared.edits.dir.nameserviceA.nn2</name></font>   
<font style="font-size: 11pt" size="2">    <value>qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceA</value></font>
<font style="font-size: 11pt" size="2">  </property></font>
<font style="font-size: 11pt" size="2">  <property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.shared.edits.dir.nameserviceB.nn3</name></font>
<font style="font-size: 11pt" size="2">    <value>qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceB</value></font>
<font style="font-size: 11pt" size="2">  </property></font>
<font style="font-size: 11pt" size="2">  <property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.namenode.shared.edits.dir.nameserviceB.nn4</name></font>
<font style="font-size: 11pt" size="2">    <value>qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceB</value></font>
<font style="font-size: 11pt" size="2">  </property></font>
<font style="font-size: 11pt" size="2">  <property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.client.failover.proxy.provider.nameserviceA</name></font>
<font style="font-size: 11pt" size="2">    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value></font>
<font style="font-size: 11pt" size="2">  </property></font>
<font style="font-size: 11pt" size="2">  <property></font> 
<font style="font-size: 11pt" size="2">    <name>dfs.client.failover.proxy.provider.nameserviceB</name></font>
<font style="font-size: 11pt" size="2">    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value></font>
<font style="font-size: 11pt" size="2">  </property></font>
<font style="font-size: 11pt" size="2">  <property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.ha.automatic-failover.enabled.nameserviceA</name></font>
<font style="font-size: 11pt" size="2">    <value>true</value></font>
<font style="font-size: 11pt" size="2">  </property></font>
  <font style="font-size: 11pt" size="2"><property></font>
<font style="font-size: 11pt" size="2">    <name>dfs.ha.automatic-failover.enabled.nameserviceB</name></font>
<font style="font-size: 11pt" size="2">    <value>true</value></font>
<font style="font-size: 11pt" size="2">  </property></font><font style="font-size: 11pt" size="2">
</font>
</pre>
<pre></pre>
<h2 class="text-body-indent-western">accumulo-site.xml:</h2>
<pre class="text-body-indent-western"><font style="font-size: 11pt" size="2">  <property></font>
<font style="font-size: 11pt" size="2">    <name>instance.volumes</name></font>   
<font style="font-size: 11pt" size="2">    <value>hdfs://nameserviceA/accumulo,hdfs://nameserviceB/accumulo</value></font>
<font style="font-size: 11pt" size="2">  </property></font></pre>

  </div><a class="u-url" href="/accumulo/entry/scaling_accumulo_with_multi_volume" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
