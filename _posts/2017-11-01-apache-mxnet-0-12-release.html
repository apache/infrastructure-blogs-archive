---
layout: post
status: PUBLISHED
published: true
title: Apache MXNet 0.12 Release Adds Support for New NVIDIA Volta GPUs and Sparse
  Tensor
author:
  display_name: Sandeep Krishnamurthy
  login: skm
  email: sandeep.krishna98@gmail.com
author_login: skm
author_email: sandeep.krishna98@gmail.com
id: 4149fff5-506b-4be5-9443-2a3f40d6d6e4
date: '2017-11-01 20:11:04 -0400'
categories:
- Technology
tags:
- apachemxnet
comments:
- id: 0
  author: Nathan
  author_email: nativecasinos.ca@gmail.com
  author_url: http://nativecasinos.ca
  date: '2019-06-25 13:03:50 -0400'
  content: Hi Jessica. I have did it too - static elements from the site from apache
    and dynamic from nginx and the site works fine. Thanks alot
- id: 0
  author: Vao
  author_email: lazzzko@gmail.com
  author_url: https://vao.com.ua/gel-laki
  date: '2019-06-25 13:08:18 -0400'
  content: "Hi guys. I have made it too. \r\nstatic elements from the site from apache
    and dynamic from nginx and the site works fine. This seems to be a going to a
    new mainstream. Thanks"
- id: 0
  author: nolvadex
  author_email: fourweremi1976@gmail.com
  author_url: https://pharmachemicalsonline.com/cancer/nolvadex.html
  date: '2019-07-19 18:26:52 -0400'
  content: cool. thanks for sharing I love the attention to detail. You are just amazing.
    And I really loved your work.
- id: 0
  author: lizmcconel
  author_email: lizmcconel@gmail.com
  author_url: https://mindepcasinos.com/
  date: '2019-11-28 13:59:06 -0500'
  content: Thank you for the info! I will use it for my project http://mindepcasinos.com/
- id: 3
  author: Jessica
  author_email: jessica.walkerr90@gmail.com
  author_url: https://www.hookupgeek.com
  date: '2019-04-18 07:28:52 -0400'
  content: Hi! I have installed apache + nginx on my website and I surpised about
    the speed of my website! Static = apache and php dynamic = nginx!
- id: 4
  author: MyFreeCams
  author_email: GoodmanAmia040@yandex.com
  author_url: https://www.similarcams.com/myfreecams
  date: '2019-10-25 14:49:53 -0400'
  content: I think these parameters will fit my blog.
- id: 28
  author: John David
  author_email: johnsonericblog@gmail.com
  author_url: ''
  date: '2020-02-03 00:29:16 -0500'
  content: Thanks for the info, but I have a question. Can I use Apache MXNet 0.12
    or higher with Amazon AWS? I read in one article ( https://www.sysnettechsolutions.com/en/install-wordpress/
    ) that older versions of MXNet did not fully work on AWS. For example, when I
    run the new version of MXNet on AWS, can I install WordPress with MXNet described
    in the article here?
- id: 343
  author: Kate Kross
  author_email: kate.kross90@gmail.com
  author_url: ''
  date: '2019-12-04 19:03:32 -0500'
  content: Conversations and dates I've had with men on https://wizzlove.com/reviews/wellhello-review
    wellhello were very good to excellent matches for me. I felt safe with my dates
    and they were respectful of my boundaries. Being my favorite, this dating site
    is no different than the rest in that we have to be very careful about who and
    how much we choose to trust and who we choose to date.
- id: 30954
  author: Apkdrod
  author_email: gregorywillett265@gmail.com
  author_url: https://apkdrod.com/
  date: '2019-07-29 17:02:05 -0400'
  content: "In fact, I have been using your service for a long time, especially I
    like the way you write an article about updates in detail, thank you for that.\r\nBen,
    Web Dev at https://apkdrod.com/"
permalink: mxnet/entry/apache-mxnet-0-12-release
---
<p>
We are excited about the availability of Apache MXNet version 0.12. With this release, MXNet adds two new important features: support for NVIDIA Volta GPUs and support for Sparse Tensors<br />
<br/></p>
<p><b> Support for NVIDIA Volta GPU Architecture </b></p>
<p>
The MXNet v0.12 release adds support for NVIDIA Volta V100 GPUs, enabling users to train convolutional neural networks up to 3.5 times faster than on the Pascal GPUs. Trillions of floating-point (FP) multiplications and additions for training a neural network have typically been done using single precision (FP32) to achieve high accuracy. However, recent research has shown that the same accuracy can be achieved using half-precision (FP16) data types.</p>
<p>
The Volta GPU architecture introduces Tensor Cores. Each Tensor Core can execute 64 fuse-multiply-add ops per clock, which roughly quadruples the CUDA core FLOPS per clock per core. Each Tensor Core performs D = A x B + C, where A and B are half-precision matrices, while C and D can be either half or single-precision matrices, thereby performing mixed precision training. The new mixed-precision training allows users to achieve optimal training performance without sacrificing accuracy by using FP16 for most of the layers of a network, and higher precision data types only when necessary.</p>
<p>
You can take advantage of Volta Tensor Cores to enable FP16 training in MXNet by passing a simple command, "--dtype float16" to the MXNet training script. For example, you can invoke <a href="https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/train_imagenet.py"> imagenet training script </a> with command:<br />
 train_imagenet.py --dtype float16<br />
<br/></p>
<p><b> Sparse Tensor Support </b></p>
<p>
MXNet v0.12 adds support for sparse tensors to efficiently store and compute tensors allowing developers to perform sparse matrix operations in a storage and compute-efficient manner and train deep learning models faster. MXNet v0.12 supports two major sparse data formats: Compressed Sparse Row (CSR) and Row Sparse (RSP). The CSR format is optimized to represent matrices with a large number of columns where each row has only a few non-zero elements. The RSP format is optimized to represent matrices with a huge number of rows where most of the row slices are complete zeros. For example, the CSR format can be used to encode the feature vectors of input data for a recommendation engine, whereas the RSP format can be used to perform the sparse gradient updates during training. This release enables sparse support on CPU for most commonly used operators such as matrix dot product and element-wise operators. Sparse support for more operators will be added in future releases.</p>
<p>
Follow <a href="https://mxnet.incubator.apache.org/versions/master/tutorials/#sparse-ndarray">these tutorials</a> to learn how to use the new sparse operators in MXNet.</p>
<p>
Get Apache MXNet 0.12 from <a href="http://www.apache.org/dist/incubator/mxnet/"> downloads page </a>. Read more about this release in <a href="https://github.com/apache/incubator-mxnet/releases/tag/0.12.0"> Release Notes </a>.</p>
<p>
Or, You can download and play with MXNet easily using one of the options below:</p>
<ul>
<li>
The Pip package can be found here: <a href="https://pypi.python.org/pypi/mxnet">https://pypi.python.org/pypi/mxnet</a>
</li>
<li>
The Docker Images can be found here: <a href="https://hub.docker.com/u/mxnet/">https://hub.docker.com/u/mxnet/ </a>
</li></p>
<p>
If you want to learn more about MXNet visit <a href="https://mxnet.incubator.apache.org/">https://mxnet.incubator.apache.org/</a>.<br />
Finally, you are welcome to join and also invite your friends to the dynamic and growing MXNet community by subscribing to dev@mxnet.incubator.apache.org</p>
