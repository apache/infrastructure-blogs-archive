<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Natural Language Processing with Groovy, OpenNLP, CoreNLP, Nlp4j, Datumbox, Smile, Spark NLP, DJL and TensorFlow | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Natural Language Processing with Groovy, OpenNLP, CoreNLP, Nlp4j, Datumbox, Smile, Spark NLP, DJL and TensorFlow" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Natural Language Processing is certainly a large and sometimes complex topic with many aspects. Some of those aspects deserve entire blogs in their own right. For this blog, we will briefly look at a few simple use cases illustrating where you might be able to use NLP technology in your own project. Language Detection Knowing what language some text represents can be a critical first step to subsequent processing. Let&#39;s look at how to predict the language using a pre-built model and Apache OpenNLP. Here, ResourceHelper is a utility class used to download and cache the model. The first run may take a little while as it downloads the model. Subsequent runs should be fast. Here we are using a well-known model referenced in the OpenNLP documentation. def helper = new ResourceHelper(&#39;https://dlcdn.apache.org/opennlp/models/langdetect/1.8.3/&#39;)def model = new LanguageDetectorModel(helper.load(&#39;langdetect-183&#39;))def detector = new LanguageDetectorME(model)[ spa: &#39;Bienvenido a Madrid&#39;, fra: &#39;Bienvenue &agrave; Paris&#39;, dan: &#39;Velkommen til K&oslash;benhavn&#39;, bul: &#39;Добре дошли в София&#39;].each { k, v -&gt; assert detector.predictLanguage(v).lang == k} The LanguageDetectorME class lets us predict the language. In general, the predictor may not be accurate on small samples of text but it was good enough for our example. We&#39;ve used the language code as the key in our map and we check that against the predicted language. A more complex scenario is training your own model. Let&#39;s look at how to do that with Datumbox. Datumbox has a pre-trained models zoo but its language detection model didn&#39;t seem to work well for the small snippets in the next example, so we&#39;ll train our own model. First, we&#39;ll define our datasets: def datasets = [ English: getClass().classLoader.getResource(&quot;training.language.en.txt&quot;).toURI(), French: getClass().classLoader.getResource(&quot;training.language.fr.txt&quot;).toURI(), German: getClass().classLoader.getResource(&quot;training.language.de.txt&quot;).toURI(), Spanish: getClass().classLoader.getResource(&quot;training.language.es.txt&quot;).toURI(), Indonesian: getClass().classLoader.getResource(&quot;training.language.id.txt&quot;).toURI()] The de training dataset comes from the Datumbox examples. The training datasets for the other languages are from Kaggle. We set up the training parameters needed by our algorithm: def trainingParams = new TextClassifier.TrainingParameters( numericalScalerTrainingParameters: null, featureSelectorTrainingParametersList: [new ChisquareSelect.TrainingParameters()], textExtractorParameters: new NgramsExtractor.Parameters(), modelerTrainingParameters: new MultinomialNaiveBayes.TrainingParameters()) We&#39;ll use a Na&iuml;ve Bayes model with Chisquare feature selection. Next we create our algorithm, train it with our training dataset, and then validate it against the training dataset. We&#39;d normally want to split the data into training and testing datasets, to give us a more accurate statistic of the accuracy of our model. But for simplicity, while still illustrating the API, we&#39;ll train and validate with our entire dataset: def config = Configuration.configurationdef classifier = MLBuilder.create(trainingParams, config)classifier.fit(datasets)def metrics = classifier.validate(datasets)println &quot;Classifier Accuracy (using training data): $metrics.accuracy&quot; When run, we see the following output: Classifier Accuracy (using training data): 0.9975609756097561 Our test dataset will consist of some hard-coded illustrative phrases. Let&#39;s use our model to predict the language for each phrase: [ &#39;Bienvenido a Madrid&#39;, &#39;Bienvenue &agrave; Paris&#39;, &#39;Welcome to London&#39;, &#39;Willkommen in Berlin&#39;, &#39;Selamat Datang di Jakarta&#39;].each { txt -&gt; def r = classifier.predict(txt) def predicted = r.YPredicted def probability = sprintf &#39;%4.2f&#39;, r.YPredictedProbabilities.get(predicted) println &quot;Classifying: &#39;$txt&#39;, Predicted: $predicted, Probability: $probability&quot;} When run, it has this output: Classifying: &#39;Bienvenido a Madrid&#39;,&nbsp; Predicted: Spanish,&nbsp; Probability: 0.83 Classifying: &#39;Bienvenue &agrave; Paris&#39;,&nbsp; Predicted: French,&nbsp; Probability: 0.71 Classifying: &#39;Welcome to London&#39;,&nbsp; Predicted: English,&nbsp; Probability: 1.00 Classifying: &#39;Willkommen in Berlin&#39;,&nbsp; Predicted: German,&nbsp; Probability: 0.84 Classifying: &#39;Selamat Datang di Jakarta&#39;,&nbsp; Predicted: Indonesian,&nbsp; Probability: 1.00 Given these phrases are very short, it is nice to get them all correct, and the probabilities all seem reasonable for this scenario. Parts of Speech Parts of speech (POS) analysers examine each part of a sentence (the words and potentially punctuation) in terms of the role they play in a sentence. A typical analyser will assign or annotate words with their role like identifying nouns, verbs, adjectives and so forth. This can be a key early step for tools like the voice assistants from Amazon, Apple and Google. We&#39;ll start by looking at a perhaps lesser known library Nlp4j before looking at some others. In fact, there are multiple Nlp4j libraries. We&#39;ll use the one from nlp4j.org,&nbsp;which seems to be the most active and recently updated. This library uses the Stanford CoreNLP library under the covers for its English POS functionality. The library has the concept of documents, and annotators that work on documents. Once annotated, we can print out all of the discovered words and their annotations: var doc = new DefaultDocument()doc.putAttribute(&#39;text&#39;, &#39;I eat sushi with chopsticks.&#39;)var ann = new StanfordPosAnnotator()ann.setProperty(&#39;target&#39;, &#39;text&#39;)ann.annotate(doc)println doc.keywords.collect{ k -&gt; &quot;${k.facet - &#39;word.&#39;}(${k.str})&quot; }.join(&#39; &#39;) When run, we see the following output: PRP(I) VBP(eat) NN(sushi) IN(with) NNS(chopsticks) .(.) The annotations, also known as tags or facets, for this example are as follows: PRP Personal pronoun VBP Present tense verb NN Noun, singular IN Preposition NNS Noun, plural The documentation for the libraries we are using give a more complete list of such annotations. A nice aspect of this library is support for other languages, in particular, Japanese. The code is very similar but uses a different annotator: doc = new DefaultDocument()doc.putAttribute(&#39;text&#39;, &#39;私は学校に行きました。&#39;)ann = new KuromojiAnnotator()ann.setProperty(&#39;target&#39;, &#39;text&#39;)ann.annotate(doc)println doc.keywords.collect{ k -&gt; &quot;${k.facet}(${k.str})&quot; }.join(&#39; &#39;) When run, we see the following output: 名詞(私) 助詞(は) 名詞(学校) 助詞(に) 動詞(行き) 助動詞(まし) 助動詞(た) 記号(。) Before progressing, we&#39;ll highlight the result visualization capabilities of the GroovyConsole. This feature lets us write a small Groovy script which converts results to any swing component. In our case we&#39;ll convert lists of annotated strings to a JLabel component containing HTML including colored annotation boxes. The details aren&#39;t included here but can be found in the repo. We need to copy that file into our ~/.groovy folder and then enable script visualization as shown here: Then we should see the following when running the script: The visualization is purely optional but adds a nice touch. If using Groovy in notebook environments like Jupyter/BeakerX, there might be visualization tools in those environments too. Let&#39;s look at a larger example using the Smile library. First, the sentences that we&#39;ll examine: def sentences = [ &#39;Paul has two sisters, Maree and Christine.&#39;, &#39;No wise fish would go anywhere without a porpoise&#39;, &#39;His bark was much worse than his bite&#39;, &#39;Turn on the lights to the main bedroom&#39;, &quot;Light &#39;em all up&quot;, &#39;Make it dark downstairs&#39;] A couple of those sentences might seem a little strange but they are selected to show off quite a few of the different POS tags. Smile has a tokenizer class which splits a sentence into words. It handles numerous cases like contractions and abbreviations (&quot;e.g.&quot;, &quot;&#39;tis&quot;, &quot;won&#39;t&quot;). Smile also has a POS class based on the&nbsp;hidden Markov model and a built-in model is used for that class. Here is our code using those classes: def tokenizer = new SimpleTokenizer(true)sentences.each { def tokens = Arrays.stream(tokenizer.split(it)).toArray(String[]::new) def tags = HMMPOSTagger.default.tag(tokens)*.toString() println tokens.indices.collect{tags[it] == tokens[it] ? tags[it] : &quot;${tags[it]}(${tokens[it]})&quot; }.join(&#39; &#39;)} We run the tokenizer for each sentence. Each token is then displayed directly or with its tag if it has one. Running the script gives this visualization: &lt;/p&gt; Paul NNP has VBZ two CD sisters NNS , Maree NNP and CC Christine NNP . No DT wise JJ fish NN would MD go VB anywhere RB without IN a DT porpoise NN His PRP$ bark NN was VBD much RB worse JJR than IN his PRP$ bite NN Turn VB on IN the DT lights NNS to TO the DT main JJ bedroom NN Light NNP &#39;em PRP all RB up RB Make VB it PRP dark JJ downstairs NN [Note: the scripts in the repo just print to stdout which is perfect when using the command-line or IDEs. The visualization in the GoovyConsole kicks in only for the actual result. So, if you are following along at home and wanting to use the GroovyConsole, you&#39;d change the each to collect and remove the println, and you should be good for visualization.] The OpenNLP code is very similar: def tokenizer = SimpleTokenizer.INSTANCEsentences.each { String[] tokens = tokenizer.tokenize(it) def posTagger = new POSTaggerME(&#39;en&#39;) String[] tags = posTagger.tag(tokens) println tokens.indices.collect{tags[it] == tokens[it] ? tags[it] : &quot;${tags[it]}(${tokens[it]})&quot; }.join(&#39; &#39;)} OpenNLP allows you to supply your own POS model but downloads a default one if none is specified. When the script is run, it has this visualization: &lt;/p&gt; Paul PROPN has VERB two NUM sisters NOUN , PUNCT Maree PROPN and CCONJ Christine PROPN . PUNCT No DET wise ADJ fish NOUN would AUX go VERB anywhere ADV without ADP a DET porpoise NOUN His PRON bark NOUN was AUX much ADV worse ADJ than ADP his PRON bite NOUN Turn VERB on ADP the DET lights NOUN to ADP the DET main ADJ bedroom NOUN Light NOUN &#39; PUNCT em NOUN all ADV up ADP Make VERB it PRON dark ADJ downstairs NOUN The observant reader may have noticed some slight differences in the tags used in this library. They are essentially the same but using slightly different names. This is something to be aware of when swapping between POS libraries or models. Make sure you look up the documentation for the library/model you are using to understand the available tag types. Entity Detection Named entity recognition (NER), seeks to identity and classify named entities in text. Categories of interest might be persons, organizations, locations dates, etc. It is another technology used in many fields of NLP. We&#39;ll start with our sentences to analyse: String[] sentences = [ &quot;A commit by Daniel Sun on December 6, 2020 improved Groovy 4&#39;s language integrated query.&quot;, &quot;A commit by Daniel on Sun., December 6, 2020 improved Groovy 4&#39;s language integrated query.&quot;, &#39;The Groovy in Action book by Dierk Koenig et. al. is a bargain at $50, or indeed any price.&#39;, &#39;The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark.&#39;, &#39;I saw Ms. May Smith waving to June Jones.&#39;, &#39;The parcel was passed from May to June.&#39;, &#39;The Mona Lisa by Leonardo da Vinci has been on display in the Louvre, Paris since 1797.&#39;] We&#39;ll use some well-known models, we&#39;ll focus on the&nbsp;person, money, date, time, and location&nbsp;models: def base = &#39;http://opennlp.sourceforge.net/models-1.5&#39;def modelNames = [&#39;person&#39;, &#39;money&#39;, &#39;date&#39;, &#39;time&#39;, &#39;location&#39;]def finders = modelNames.collect { model -&gt; new NameFinderME(DownloadUtil.downloadModel(new URL(&quot;$base/en-ner-${model}.bin&quot;), TokenNameFinderModel))} We&#39;ll now tokenize our sentences: def tokenizer = SimpleTokenizer.INSTANCEsentences.each { sentence -&gt; String[] tokens = tokenizer.tokenize(sentence) Span[] tokenSpans = tokenizer.tokenizePos(sentence) def entityText = [:] def entityPos = [:] finders.indices.each {fi -&gt; // could be made smarter by looking at probabilities and overlapping spans Span[] spans = finders[fi].find(tokens) spans.each{span -&gt; def se = span.start..&lt;span.end def pos = (tokenSpans[se.from].start)..&lt;(tokenSpans[se.to].end) entityPos[span.start] = pos entityText[span.start] = &quot;$span.type(${sentence[pos]})&quot; } } entityPos.keySet().sort().reverseEach { def pos = entityPos[it] def (from, to) = [pos.from, pos.to + 1] sentence = sentence[0..&lt;from] + entityText[it] + sentence[to..-1] } println sentence} And when visualized, shows this: &lt;/p&gt; A commit by Daniel Sun person on December 6, 2020 date improved Groovy 4&#39;s language integrated query. A commit by Daniel person on Sun., December 6, 2020 date improved Groovy 4&#39;s language integrated query. The Groovy in Action book by Dierk Koenig person et. al. is a bargain at $50 money , or indeed any price. The conference wrapped up yesterday date at 5:30 p.m. time in Copenhagen location , Denmark location . I saw Ms. May Smith person waving to June Jones person . The parcel was passed from May to June date . The Mona Lisa by Leonardo da Vinci person has been on display in the Louvre, Paris location since 1797 date . We can see here that most examples have been categorized as we might expect. We&#39;d have to improve our model for it to do a better job on the &quot;May to June&quot; example. Scaling Entity Detection We can also run our named entity detection algorithms on platforms like Spark NLP which adds NLP functionality to Apache Spark. We&#39;ll use glove_100d embeddings and the onto_100 NER model. var assembler = new DocumentAssembler(inputCol: &#39;text&#39;, outputCol: &#39;document&#39;, cleanupMode: &#39;disabled&#39;)var tokenizer = new Tokenizer(inputCols: [&#39;document&#39;] as String[], outputCol: &#39;token&#39;)var embeddings = WordEmbeddingsModel.pretrained(&#39;glove_100d&#39;).tap { inputCols = [&#39;document&#39;, &#39;token&#39;] as String[] outputCol = &#39;embeddings&#39;}var model = NerDLModel.pretrained(&#39;onto_100&#39;, &#39;en&#39;).tap { inputCols = [&#39;document&#39;, &#39;token&#39;, &#39;embeddings&#39;] as String[] outputCol =&#39;ner&#39;}var converter = new NerConverter(inputCols: [&#39;document&#39;, &#39;token&#39;, &#39;ner&#39;] as String[], outputCol: &#39;ner_chunk&#39;)var pipeline = new Pipeline(stages: [assembler, tokenizer, embeddings, model, converter] as PipelineStage[])var spark = SparkNLP.start(false, false, &#39;16G&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;)var text = [ &quot;The Mona Lisa is a 16th century oil painting created by Leonardo. It&#39;s held at the Louvre in Paris.&quot;]var data = spark.createDataset(text, Encoders.STRING()).toDF(&#39;text&#39;)var pipelineModel = pipeline.fit(data)var transformed = pipelineModel.transform(data)transformed.show()use(SparkCategory) { transformed.collectAsList().each { row -&gt; def res = row.text def chunks = row.ner_chunk.reverseIterator() while (chunks.hasNext()) { def chunk = chunks.next() int begin = chunk.begin int end = chunk.end def entity = chunk.metadata.get(&#39;entity&#39;).get() res = res[0..&lt;begin] + &quot;$entity($chunk.result)&quot; + res[end&lt;..-1] } println res }} We won&#39;t go into all of the details here. In summary, the code sets up a pipeline that transforms our input sentences, via a series of steps, into chunks, where each chunk corresponds to a detected entity. Each chunk has a start and ending position, and an associated tag type. This may not seem like it is much different to our earlier examples, but if we had large volumes of data and we were running in a large cluster, the work could be spread across worker nodes within the cluster. Here we have used a utility SparkCategory class which makes accessing the information in Spark Row instances a little nicer in terms of Groovy shorthand syntax. We can use row.text instead of row.get(row.fieldIndex(&#39;text&#39;)). Here is the code for this utility class: class SparkCategory { static get(Row r, String field) { r.get(r.fieldIndex(field)) }} If doing more than this simple example, the use of SparkCategory could be made implicit through various standard Groovy techniques. When we run our script, we see the following output: 22/08/07 12:31:39 INFO SparkContext: Running Spark version 3.3.0 ... glove_100d download started this may take some time. Approximate size to download 145.3 MB ... onto_100 download started this may take some time. Approximate size to download 13.5 MB ... +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+ | text| document| token| embeddings| ner| ner_chunk| +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+ |The Mona Lisa is ...|[{document, 0, 98...|[{token, 0, 2, Th...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 0, 12, T...| +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+ PERSON(The Mona Lisa) is a DATE(16th century) oil painting created by PERSON(Leonardo). It&#39;s held at the FAC(Louvre) in GPE(Paris). The result has the following visualization: &lt;/p&gt; The Mona Lisa PERSON is a 16th century DATE oil painting created by Leonardo PERSON . It&#39;s held at the Louvre FAC in Paris GPE . Here FAC is facility (buildings, airports, highways, bridges, etc.) and GPE is Geo-Political Entity (countries, cities, states, etc.). Sentence Detection Detecting sentences in text might seem a simple concept at first but there are numerous special cases. Consider the following text: def text = &#39;&#39;&#39;The most referenced scientific paper of all time is &quot;Protein measurement with theFolin phenol reagent&quot; by Lowry, O. H., Rosebrough, N. J., Farr, A. L. &amp; Randall,R. J. and was published in the J. BioChem. in 1951. It describes a method formeasuring the amount of protein (even as small as 0.2 &gamma;, were &gamma; is the specificweight) in solutions and has been cited over 300,000 times and can be found here:https://www.jbc.org/content/193/1/265.full.pdf. Dr. Lowry completedtwo doctoral degrees under an M.D.-Ph.D. program from the University of Chicagobefore moving to Harvard under A. Baird Hastings. He was also the H.O.D ofPharmacology at Washington University in St. Louis for 29 years.&#39;&#39;&#39; There are full stops at the end of each sentence (though in general, it could also be other punctuation like exclamation marks and question marks). There are also full stops and decimal points in abbreviations, URLs, decimal numbers and so forth. Sentence detection algorithms might have some special hard-coded cases, like &quot;Dr.&quot;, &quot;Ms.&quot;, or in an emoticon, and may also use some heuristics. In general, they might also be trained with examples like above. Here is some code for OpenNLP for detecting sentences in the above: def helper = new ResourceHelper(&#39;http://opennlp.sourceforge.net/models-1.5&#39;)def model = new SentenceModel(helper.load(&#39;en-sent&#39;))def detector = new SentenceDetectorME(model)def sentences = detector.sentDetect(text)assert text.count(&#39;.&#39;) == 28assert sentences.size() == 4println &quot;Found ${sentences.size()} sentences:\n&quot; + sentences.join(&#39;\n\n&#39;) It has the following output: Downloading en-sent Found 4 sentences: The most referenced scientific paper of all time is &quot;Protein measurement with the Folin phenol reagent&quot; by Lowry, O. H., Rosebrough, N. J., Farr, A. L. &amp; Randall, R. J. and was published in the J. BioChem. in 1951. It describes a method for measuring the amount of protein (even as small as 0.2 &gamma;, were &gamma; is the specific weight) in solutions and has been cited over 300,000 times and can be found here: https://www.jbc.org/content/193/1/265.full.pdf. Dr. Lowry completed two doctoral degrees under an M.D.-Ph.D. program from the University of Chicago before moving to Harvard under A. Baird Hastings. He was also the H.O.D of Pharmacology at Washington University in St. Louis for 29 years. We can see here, it handled all of the tricky cases in the example. Relationship Extraction with Triples The next step after detecting named entities and the various parts of speech of certain words is to explore relationships between them. This is often done in the form of subject-predicate-object triplets. In our earlier NER example, for the sentence &quot;&lt;span style=&quot;background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, &quot;Courier New&quot;, monospace; font-size: 13px;&quot;&gt;The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark.&lt;/span&gt;&quot;, we found various date, time and location named entities. We can extract triples using the MinIE library (which in turns uses the Standford CoreNLP library) with the following code: def parser = CoreNLPUtils.StanfordDepNNParser()sentences.each { sentence -&gt; def minie = new MinIE(sentence, parser, MinIE.Mode.SAFE) println &quot;\nInput sentence: $sentence&quot; println &#39;=============================&#39; println &#39;Extractions:&#39; for (ap in minie.propositions) { println &quot;\tTriple: $ap.tripleAsString&quot; def attr = ap.attribution.attributionPhrase ? ap.attribution.toStringCompact() : &#39;NONE&#39; println &quot;\tFactuality: $ap.factualityAsString\tAttribution: $attr&quot; println &#39;\t----------&#39; }} The output for the previously mentioned sentence is shown below: Input sentence: The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark. ============================= Extractions: Triple: &quot;conference&quot; &quot;wrapped up yesterday at&quot; &quot;5:30 p.m.&quot; Factuality: (+,CT) Attribution: NONE ---------- Triple: &quot;conference&quot; &quot;wrapped up yesterday in&quot; &quot;Copenhagen&quot; Factuality: (+,CT) Attribution: NONE ---------- Triple: &quot;conference&quot; &quot;wrapped up&quot; &quot;yesterday&quot; Factuality: (+,CT) Attribution: NONE We can now piece together the relationships between the earlier entities we detected. There was also a problematic case amongst the earlier NER examples, &quot;&lt;span style=&quot;background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, &quot;Courier New&quot;, monospace; font-size: 13px;&quot;&gt;The parcel was passed from May to June.&lt;/span&gt;&quot;. Using the previous model, detected &quot;&lt;span style=&quot;background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, &quot;Courier New&quot;, monospace; font-size: 13px;&quot;&gt;May to June&lt;/span&gt;&quot; as a date. Let&#39;s explore that using CoreNLP&#39;s triple extraction directly. We won&#39;t show the source code here but CoreNLP supports simple and more powerful approaches to solving this problem. The output for the sentence in question using the more powerful technique is: Sentence #7: The parcel was passed from May to June. root(ROOT-0, passed-4) det(parcel-2, The-1) nsubj:pass(passed-4, parcel-2) aux:pass(passed-4, was-3) case(May-6, from-5) obl:from(passed-4, May-6) case(June-8, to-7) obl:to(passed-4, June-8) punct(passed-4, .-9) Triples: 1.0 parcel was passed 1.0 parcel was passed to June 1.0 parcel was passed from May to June 1.0 parcel was passed from May We can see that this has done a better job of piecing together what entities we have and their relationships. Sentiment Analysis Sentiment analysis is a NLP technique used to determine whether data is positive, negative, or neutral. Standford CoreNLP has default models it uses for this purpose: def doc = new Document(&#39;&#39;&#39;StanfordNLP is fantastic!Groovy is great fun!Math can be hard!&#39;&#39;&#39;)for (sent in doc.sentences()) { println &quot;${sent.toString().padRight(40)} ${sent.sentiment()}&quot;} Which has the following output: [main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.6 sec]. [main] INFO edu.stanford.nlp.sentiment.SentimentModel - Loading sentiment model edu/stanford/nlp/models/sentiment/sentiment.ser.gz ... done [0.1 sec]. StanfordNLP is fantastic!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; POSITIVE Groovy is great fun!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;VERY_POSITIVE Math can be hard!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; NEUTRAL We can also train our own. Let&#39;s start with two datasets: def datasets = [ positive: getClass().classLoader.getResource(&quot;rt-polarity.pos&quot;).toURI(), negative: getClass().classLoader.getResource(&quot;rt-polarity.neg&quot;).toURI()] We&#39;ll first use Datumbox which, as we saw earlier, requires training parameters for our algorithm: def trainingParams = new TextClassifier.TrainingParameters( numericalScalerTrainingParameters: null, featureSelectorTrainingParametersList: [new ChisquareSelect.TrainingParameters()], textExtractorParameters: new NgramsExtractor.Parameters(), modelerTrainingParameters: new MultinomialNaiveBayes.TrainingParameters()) We now create our algorithm, train it with or training dataset, and for illustrative purposes validate against the training dataset: def config = Configuration.configurationTextClassifier classifier = MLBuilder.create(trainingParams, config)classifier.fit(datasets)def metrics = classifier.validate(datasets)println &quot;Classifier Accuracy (using training data): $metrics.accuracy&quot; The output is shown here: [main] INFO com.datumbox.framework.core.common.dataobjects.Dataframe$Builder - Dataset Parsing positive class [main] INFO com.datumbox.framework.core.common.dataobjects.Dataframe$Builder - Dataset Parsing negative class ... Classifier Accuracy (using training data): 0.8275959103273615 Now we can test our model against several sentences: [&#39;Datumbox is divine!&#39;, &#39;Groovy is great fun!&#39;, &#39;Math can be hard!&#39;].each { def r = classifier.predict(it) def predicted = r.YPredicted def probability = sprintf &#39;%4.2f&#39;, r.YPredictedProbabilities.get(predicted) println &quot;Classifing: &#39;$it&#39;, Predicted: $predicted, Probability: $probability&quot;} Which has this output: ... [main] INFO com.datumbox.framework.applications.nlp.TextClassifier - predict() ... Classifing: &#39;Datumbox is divine!&#39;, Predicted: positive, Probability: 0.83 Classifing: &#39;Groovy is great fun!&#39;, Predicted: positive, Probability: 0.80 Classifing: &#39;Math can be hard!&#39;, Predicted: negative, Probability: 0.95 We can do the same thing but with OpenNLP. First, we collect our input data. OpenNLP is expecting it in a single dataset with tagged examples: def trainingCollection = datasets.collect { k, v -&gt; new File(v).readLines().collect{&quot;$k $it&quot;.toString() }}.sum() Now, we&#39;ll train two models. One uses na&iuml;ve bayes, the other maxent. We train up both variants. def variants = [ Maxent : new TrainingParameters(), NaiveBayes: new TrainingParameters((CUTOFF_PARAM): &#39;0&#39;, (ALGORITHM_PARAM): NAIVE_BAYES_VALUE)]def models = [:]variants.each{ key, trainingParams -&gt; def trainingStream = new CollectionObjectStream(trainingCollection) def sampleStream = new DocumentSampleStream(trainingStream) println &quot;\nTraining using $key&quot; models[key] = DocumentCategorizerME.train(&#39;en&#39;, sampleStream, trainingParams, new DoccatFactory())} Now we run sentiment predictions on our sample sentences using both variants: def w = sentences*.size().max()variants.each { key, params -&gt; def categorizer = new DocumentCategorizerME(models[key]) println &quot;\nAnalyzing using $key&quot; sentences.each { def result = categorizer.categorize(it.split(&#39;[ !]&#39;)) def category = categorizer.getBestCategory(result) def prob = sprintf &#39;%4.2f&#39;, result[categorizer.getIndex(category)] println &quot;${it.padRight(w)} $category ($prob)}&quot; }} When we run this we get: Training using Maxent ...done. ... Training using NaiveBayes ...done. ... Analyzing using Maxent OpenNLP is fantastic! positive (0.64)} Groovy is great fun! positive (0.74)} Math can be hard! negative (0.61)} Analyzing using NaiveBayes OpenNLP is fantastic! positive (0.72)} Groovy is great fun! positive (0.81)} Math can be hard! negative (0.72)} The models here appear to have lower probability levels compared to the model we trained for Datumbox. We could try tweaking the training parameters further if this was a problem. We&#39;d probably also need a bigger testing set to convince ourselves of the relative merits of each model. Some models can be over-trained on small datasets and perform very well with data similar to their training datasets but perform much worse for other data. Universal Sentence Encoding This example is inspired from the UniversalSentenceEncoder example in the DJL examples module. It looks at using the universal sentence encoder model from TensorFlow Hub via the DeepJavaLibrary (DJL) api. First we define a translator. The Translator interface allow us to specify pre and post processing functionality. class MyTranslator implements NoBatchifyTranslator&lt;String[], double[][]&gt; { @Override NDList processInput(TranslatorContext ctx, String[] raw) { var factory = ctx.NDManager var inputs = new NDList(raw.collect(factory::create)) new NDList(NDArrays.stack(inputs)) } @Override double[][] processOutput(TranslatorContext ctx, NDList list) { long numOutputs = list.singletonOrThrow().shape.get(0) NDList result = [] for (i in 0..&lt;numOutputs) { result &lt;&lt; list.singletonOrThrow().get(i) } result*.toFloatArray() as double[][] }} Here, we manually pack our input sentences into the required n-dimensional data types, and extract our output calculations into a 2D double array. Next, we create our predict method by first defining the criteria for our prediction algorithm. We are going to use our translator, use the TensorFlow engine, use a predefined sentence encoder model from the TensorFlow Hub, and indicate that we are creating a text embedding application: def predict(String[] inputs) { String modelUrl = &quot;https://storage.googleapis.com/tfhub-modules/google/universal-sentence-encoder/4.tar.gz&quot; Criteria&lt;String[], double[][]&gt; criteria = Criteria.builder() .optApplication(Application.NLP.TEXT_EMBEDDING) .setTypes(String[], double[][]) .optModelUrls(modelUrl) .optTranslator(new MyTranslator()) .optEngine(&quot;TensorFlow&quot;) .optProgress(new ProgressBar()) .build() try (var model = criteria.loadModel() var predictor = model.newPredictor()) { predictor.predict(inputs) }} Next, let&#39;s define our input strings: String[] inputs = [ &quot;Cycling is low impact and great for cardio&quot;, &quot;Swimming is low impact and good for fitness&quot;, &quot;Palates is good for fitness and flexibility&quot;, &quot;Weights are good for strength and fitness&quot;, &quot;Orchids can be tricky to grow&quot;, &quot;Sunflowers are fun to grow&quot;, &quot;Radishes are easy to grow&quot;, &quot;The taste of radishes grows on you after a while&quot;,]var k = inputs.size() Now, we&#39;ll use our predictor method to calculate the embeddings for each sentence. We&#39;ll print out the embeddings and also calculate the dot product of the embeddings. The dot product (the same as the inner product for this case) reveals how related the sentences are. var embeddings = predict(inputs)var z = new double[k][k]for (i in 0..&lt;k) { println &quot;Embedding for: ${inputs[i]}\n${embeddings[i]}&quot; for (j in 0..&lt;k) { z[i][j] = dot(embeddings[i], embeddings[j]) }} Finally, we&#39;ll use the Heatmap class from Smile to present a nice display highlighting what the data reveals: new Heatmap(inputs, inputs, z, Palette.heat(20).reverse()).canvas().with { title = &#39;Semantic textual similarity&#39; setAxisLabels(&#39;&#39;, &#39;&#39;) window()} The output shows us the embeddings: Loading: 100% |========================================| 2022-08-07 17:10:43.212697: ... This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 ... 2022-08-07 17:10:52.589396: ... SavedModel load for tags { serve }; Status: success: OK... ... Embedding for: Cycling is low impact and great for cardio [-0.02865048497915268, 0.02069241739809513, 0.010843578726053238, -0.04450441896915436, ...] ... Embedding for: The taste of radishes grows on you after a while [0.015841705724596977, -0.03129228577017784, 0.01183396577835083, 0.022753292694687843, ...] The embeddings are an indication of similarity. Two sentences with similar meaning typically have similar embeddings. The displayed graphic is shown below: This graphic shows that our first four sentences are somewhat related, as are the last four sentences, but that there is minimal relationship between those two groups. More information Further examples can be found in the related repos: https://github.com/paulk-asert/groovy-data-science/blob/master/subprojects/LanguageProcessing https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingSparkNLP https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingDjl Conclusion We have look at a range of NLP examples using various NLP libraries. Hopefully you can see some cases where you could use additional NLP technologies in some of your own applications." />
<meta property="og:description" content="Natural Language Processing is certainly a large and sometimes complex topic with many aspects. Some of those aspects deserve entire blogs in their own right. For this blog, we will briefly look at a few simple use cases illustrating where you might be able to use NLP technology in your own project. Language Detection Knowing what language some text represents can be a critical first step to subsequent processing. Let&#39;s look at how to predict the language using a pre-built model and Apache OpenNLP. Here, ResourceHelper is a utility class used to download and cache the model. The first run may take a little while as it downloads the model. Subsequent runs should be fast. Here we are using a well-known model referenced in the OpenNLP documentation. def helper = new ResourceHelper(&#39;https://dlcdn.apache.org/opennlp/models/langdetect/1.8.3/&#39;)def model = new LanguageDetectorModel(helper.load(&#39;langdetect-183&#39;))def detector = new LanguageDetectorME(model)[ spa: &#39;Bienvenido a Madrid&#39;, fra: &#39;Bienvenue &agrave; Paris&#39;, dan: &#39;Velkommen til K&oslash;benhavn&#39;, bul: &#39;Добре дошли в София&#39;].each { k, v -&gt; assert detector.predictLanguage(v).lang == k} The LanguageDetectorME class lets us predict the language. In general, the predictor may not be accurate on small samples of text but it was good enough for our example. We&#39;ve used the language code as the key in our map and we check that against the predicted language. A more complex scenario is training your own model. Let&#39;s look at how to do that with Datumbox. Datumbox has a pre-trained models zoo but its language detection model didn&#39;t seem to work well for the small snippets in the next example, so we&#39;ll train our own model. First, we&#39;ll define our datasets: def datasets = [ English: getClass().classLoader.getResource(&quot;training.language.en.txt&quot;).toURI(), French: getClass().classLoader.getResource(&quot;training.language.fr.txt&quot;).toURI(), German: getClass().classLoader.getResource(&quot;training.language.de.txt&quot;).toURI(), Spanish: getClass().classLoader.getResource(&quot;training.language.es.txt&quot;).toURI(), Indonesian: getClass().classLoader.getResource(&quot;training.language.id.txt&quot;).toURI()] The de training dataset comes from the Datumbox examples. The training datasets for the other languages are from Kaggle. We set up the training parameters needed by our algorithm: def trainingParams = new TextClassifier.TrainingParameters( numericalScalerTrainingParameters: null, featureSelectorTrainingParametersList: [new ChisquareSelect.TrainingParameters()], textExtractorParameters: new NgramsExtractor.Parameters(), modelerTrainingParameters: new MultinomialNaiveBayes.TrainingParameters()) We&#39;ll use a Na&iuml;ve Bayes model with Chisquare feature selection. Next we create our algorithm, train it with our training dataset, and then validate it against the training dataset. We&#39;d normally want to split the data into training and testing datasets, to give us a more accurate statistic of the accuracy of our model. But for simplicity, while still illustrating the API, we&#39;ll train and validate with our entire dataset: def config = Configuration.configurationdef classifier = MLBuilder.create(trainingParams, config)classifier.fit(datasets)def metrics = classifier.validate(datasets)println &quot;Classifier Accuracy (using training data): $metrics.accuracy&quot; When run, we see the following output: Classifier Accuracy (using training data): 0.9975609756097561 Our test dataset will consist of some hard-coded illustrative phrases. Let&#39;s use our model to predict the language for each phrase: [ &#39;Bienvenido a Madrid&#39;, &#39;Bienvenue &agrave; Paris&#39;, &#39;Welcome to London&#39;, &#39;Willkommen in Berlin&#39;, &#39;Selamat Datang di Jakarta&#39;].each { txt -&gt; def r = classifier.predict(txt) def predicted = r.YPredicted def probability = sprintf &#39;%4.2f&#39;, r.YPredictedProbabilities.get(predicted) println &quot;Classifying: &#39;$txt&#39;, Predicted: $predicted, Probability: $probability&quot;} When run, it has this output: Classifying: &#39;Bienvenido a Madrid&#39;,&nbsp; Predicted: Spanish,&nbsp; Probability: 0.83 Classifying: &#39;Bienvenue &agrave; Paris&#39;,&nbsp; Predicted: French,&nbsp; Probability: 0.71 Classifying: &#39;Welcome to London&#39;,&nbsp; Predicted: English,&nbsp; Probability: 1.00 Classifying: &#39;Willkommen in Berlin&#39;,&nbsp; Predicted: German,&nbsp; Probability: 0.84 Classifying: &#39;Selamat Datang di Jakarta&#39;,&nbsp; Predicted: Indonesian,&nbsp; Probability: 1.00 Given these phrases are very short, it is nice to get them all correct, and the probabilities all seem reasonable for this scenario. Parts of Speech Parts of speech (POS) analysers examine each part of a sentence (the words and potentially punctuation) in terms of the role they play in a sentence. A typical analyser will assign or annotate words with their role like identifying nouns, verbs, adjectives and so forth. This can be a key early step for tools like the voice assistants from Amazon, Apple and Google. We&#39;ll start by looking at a perhaps lesser known library Nlp4j before looking at some others. In fact, there are multiple Nlp4j libraries. We&#39;ll use the one from nlp4j.org,&nbsp;which seems to be the most active and recently updated. This library uses the Stanford CoreNLP library under the covers for its English POS functionality. The library has the concept of documents, and annotators that work on documents. Once annotated, we can print out all of the discovered words and their annotations: var doc = new DefaultDocument()doc.putAttribute(&#39;text&#39;, &#39;I eat sushi with chopsticks.&#39;)var ann = new StanfordPosAnnotator()ann.setProperty(&#39;target&#39;, &#39;text&#39;)ann.annotate(doc)println doc.keywords.collect{ k -&gt; &quot;${k.facet - &#39;word.&#39;}(${k.str})&quot; }.join(&#39; &#39;) When run, we see the following output: PRP(I) VBP(eat) NN(sushi) IN(with) NNS(chopsticks) .(.) The annotations, also known as tags or facets, for this example are as follows: PRP Personal pronoun VBP Present tense verb NN Noun, singular IN Preposition NNS Noun, plural The documentation for the libraries we are using give a more complete list of such annotations. A nice aspect of this library is support for other languages, in particular, Japanese. The code is very similar but uses a different annotator: doc = new DefaultDocument()doc.putAttribute(&#39;text&#39;, &#39;私は学校に行きました。&#39;)ann = new KuromojiAnnotator()ann.setProperty(&#39;target&#39;, &#39;text&#39;)ann.annotate(doc)println doc.keywords.collect{ k -&gt; &quot;${k.facet}(${k.str})&quot; }.join(&#39; &#39;) When run, we see the following output: 名詞(私) 助詞(は) 名詞(学校) 助詞(に) 動詞(行き) 助動詞(まし) 助動詞(た) 記号(。) Before progressing, we&#39;ll highlight the result visualization capabilities of the GroovyConsole. This feature lets us write a small Groovy script which converts results to any swing component. In our case we&#39;ll convert lists of annotated strings to a JLabel component containing HTML including colored annotation boxes. The details aren&#39;t included here but can be found in the repo. We need to copy that file into our ~/.groovy folder and then enable script visualization as shown here: Then we should see the following when running the script: The visualization is purely optional but adds a nice touch. If using Groovy in notebook environments like Jupyter/BeakerX, there might be visualization tools in those environments too. Let&#39;s look at a larger example using the Smile library. First, the sentences that we&#39;ll examine: def sentences = [ &#39;Paul has two sisters, Maree and Christine.&#39;, &#39;No wise fish would go anywhere without a porpoise&#39;, &#39;His bark was much worse than his bite&#39;, &#39;Turn on the lights to the main bedroom&#39;, &quot;Light &#39;em all up&quot;, &#39;Make it dark downstairs&#39;] A couple of those sentences might seem a little strange but they are selected to show off quite a few of the different POS tags. Smile has a tokenizer class which splits a sentence into words. It handles numerous cases like contractions and abbreviations (&quot;e.g.&quot;, &quot;&#39;tis&quot;, &quot;won&#39;t&quot;). Smile also has a POS class based on the&nbsp;hidden Markov model and a built-in model is used for that class. Here is our code using those classes: def tokenizer = new SimpleTokenizer(true)sentences.each { def tokens = Arrays.stream(tokenizer.split(it)).toArray(String[]::new) def tags = HMMPOSTagger.default.tag(tokens)*.toString() println tokens.indices.collect{tags[it] == tokens[it] ? tags[it] : &quot;${tags[it]}(${tokens[it]})&quot; }.join(&#39; &#39;)} We run the tokenizer for each sentence. Each token is then displayed directly or with its tag if it has one. Running the script gives this visualization: &lt;/p&gt; Paul NNP has VBZ two CD sisters NNS , Maree NNP and CC Christine NNP . No DT wise JJ fish NN would MD go VB anywhere RB without IN a DT porpoise NN His PRP$ bark NN was VBD much RB worse JJR than IN his PRP$ bite NN Turn VB on IN the DT lights NNS to TO the DT main JJ bedroom NN Light NNP &#39;em PRP all RB up RB Make VB it PRP dark JJ downstairs NN [Note: the scripts in the repo just print to stdout which is perfect when using the command-line or IDEs. The visualization in the GoovyConsole kicks in only for the actual result. So, if you are following along at home and wanting to use the GroovyConsole, you&#39;d change the each to collect and remove the println, and you should be good for visualization.] The OpenNLP code is very similar: def tokenizer = SimpleTokenizer.INSTANCEsentences.each { String[] tokens = tokenizer.tokenize(it) def posTagger = new POSTaggerME(&#39;en&#39;) String[] tags = posTagger.tag(tokens) println tokens.indices.collect{tags[it] == tokens[it] ? tags[it] : &quot;${tags[it]}(${tokens[it]})&quot; }.join(&#39; &#39;)} OpenNLP allows you to supply your own POS model but downloads a default one if none is specified. When the script is run, it has this visualization: &lt;/p&gt; Paul PROPN has VERB two NUM sisters NOUN , PUNCT Maree PROPN and CCONJ Christine PROPN . PUNCT No DET wise ADJ fish NOUN would AUX go VERB anywhere ADV without ADP a DET porpoise NOUN His PRON bark NOUN was AUX much ADV worse ADJ than ADP his PRON bite NOUN Turn VERB on ADP the DET lights NOUN to ADP the DET main ADJ bedroom NOUN Light NOUN &#39; PUNCT em NOUN all ADV up ADP Make VERB it PRON dark ADJ downstairs NOUN The observant reader may have noticed some slight differences in the tags used in this library. They are essentially the same but using slightly different names. This is something to be aware of when swapping between POS libraries or models. Make sure you look up the documentation for the library/model you are using to understand the available tag types. Entity Detection Named entity recognition (NER), seeks to identity and classify named entities in text. Categories of interest might be persons, organizations, locations dates, etc. It is another technology used in many fields of NLP. We&#39;ll start with our sentences to analyse: String[] sentences = [ &quot;A commit by Daniel Sun on December 6, 2020 improved Groovy 4&#39;s language integrated query.&quot;, &quot;A commit by Daniel on Sun., December 6, 2020 improved Groovy 4&#39;s language integrated query.&quot;, &#39;The Groovy in Action book by Dierk Koenig et. al. is a bargain at $50, or indeed any price.&#39;, &#39;The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark.&#39;, &#39;I saw Ms. May Smith waving to June Jones.&#39;, &#39;The parcel was passed from May to June.&#39;, &#39;The Mona Lisa by Leonardo da Vinci has been on display in the Louvre, Paris since 1797.&#39;] We&#39;ll use some well-known models, we&#39;ll focus on the&nbsp;person, money, date, time, and location&nbsp;models: def base = &#39;http://opennlp.sourceforge.net/models-1.5&#39;def modelNames = [&#39;person&#39;, &#39;money&#39;, &#39;date&#39;, &#39;time&#39;, &#39;location&#39;]def finders = modelNames.collect { model -&gt; new NameFinderME(DownloadUtil.downloadModel(new URL(&quot;$base/en-ner-${model}.bin&quot;), TokenNameFinderModel))} We&#39;ll now tokenize our sentences: def tokenizer = SimpleTokenizer.INSTANCEsentences.each { sentence -&gt; String[] tokens = tokenizer.tokenize(sentence) Span[] tokenSpans = tokenizer.tokenizePos(sentence) def entityText = [:] def entityPos = [:] finders.indices.each {fi -&gt; // could be made smarter by looking at probabilities and overlapping spans Span[] spans = finders[fi].find(tokens) spans.each{span -&gt; def se = span.start..&lt;span.end def pos = (tokenSpans[se.from].start)..&lt;(tokenSpans[se.to].end) entityPos[span.start] = pos entityText[span.start] = &quot;$span.type(${sentence[pos]})&quot; } } entityPos.keySet().sort().reverseEach { def pos = entityPos[it] def (from, to) = [pos.from, pos.to + 1] sentence = sentence[0..&lt;from] + entityText[it] + sentence[to..-1] } println sentence} And when visualized, shows this: &lt;/p&gt; A commit by Daniel Sun person on December 6, 2020 date improved Groovy 4&#39;s language integrated query. A commit by Daniel person on Sun., December 6, 2020 date improved Groovy 4&#39;s language integrated query. The Groovy in Action book by Dierk Koenig person et. al. is a bargain at $50 money , or indeed any price. The conference wrapped up yesterday date at 5:30 p.m. time in Copenhagen location , Denmark location . I saw Ms. May Smith person waving to June Jones person . The parcel was passed from May to June date . The Mona Lisa by Leonardo da Vinci person has been on display in the Louvre, Paris location since 1797 date . We can see here that most examples have been categorized as we might expect. We&#39;d have to improve our model for it to do a better job on the &quot;May to June&quot; example. Scaling Entity Detection We can also run our named entity detection algorithms on platforms like Spark NLP which adds NLP functionality to Apache Spark. We&#39;ll use glove_100d embeddings and the onto_100 NER model. var assembler = new DocumentAssembler(inputCol: &#39;text&#39;, outputCol: &#39;document&#39;, cleanupMode: &#39;disabled&#39;)var tokenizer = new Tokenizer(inputCols: [&#39;document&#39;] as String[], outputCol: &#39;token&#39;)var embeddings = WordEmbeddingsModel.pretrained(&#39;glove_100d&#39;).tap { inputCols = [&#39;document&#39;, &#39;token&#39;] as String[] outputCol = &#39;embeddings&#39;}var model = NerDLModel.pretrained(&#39;onto_100&#39;, &#39;en&#39;).tap { inputCols = [&#39;document&#39;, &#39;token&#39;, &#39;embeddings&#39;] as String[] outputCol =&#39;ner&#39;}var converter = new NerConverter(inputCols: [&#39;document&#39;, &#39;token&#39;, &#39;ner&#39;] as String[], outputCol: &#39;ner_chunk&#39;)var pipeline = new Pipeline(stages: [assembler, tokenizer, embeddings, model, converter] as PipelineStage[])var spark = SparkNLP.start(false, false, &#39;16G&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;)var text = [ &quot;The Mona Lisa is a 16th century oil painting created by Leonardo. It&#39;s held at the Louvre in Paris.&quot;]var data = spark.createDataset(text, Encoders.STRING()).toDF(&#39;text&#39;)var pipelineModel = pipeline.fit(data)var transformed = pipelineModel.transform(data)transformed.show()use(SparkCategory) { transformed.collectAsList().each { row -&gt; def res = row.text def chunks = row.ner_chunk.reverseIterator() while (chunks.hasNext()) { def chunk = chunks.next() int begin = chunk.begin int end = chunk.end def entity = chunk.metadata.get(&#39;entity&#39;).get() res = res[0..&lt;begin] + &quot;$entity($chunk.result)&quot; + res[end&lt;..-1] } println res }} We won&#39;t go into all of the details here. In summary, the code sets up a pipeline that transforms our input sentences, via a series of steps, into chunks, where each chunk corresponds to a detected entity. Each chunk has a start and ending position, and an associated tag type. This may not seem like it is much different to our earlier examples, but if we had large volumes of data and we were running in a large cluster, the work could be spread across worker nodes within the cluster. Here we have used a utility SparkCategory class which makes accessing the information in Spark Row instances a little nicer in terms of Groovy shorthand syntax. We can use row.text instead of row.get(row.fieldIndex(&#39;text&#39;)). Here is the code for this utility class: class SparkCategory { static get(Row r, String field) { r.get(r.fieldIndex(field)) }} If doing more than this simple example, the use of SparkCategory could be made implicit through various standard Groovy techniques. When we run our script, we see the following output: 22/08/07 12:31:39 INFO SparkContext: Running Spark version 3.3.0 ... glove_100d download started this may take some time. Approximate size to download 145.3 MB ... onto_100 download started this may take some time. Approximate size to download 13.5 MB ... +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+ | text| document| token| embeddings| ner| ner_chunk| +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+ |The Mona Lisa is ...|[{document, 0, 98...|[{token, 0, 2, Th...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 0, 12, T...| +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+ PERSON(The Mona Lisa) is a DATE(16th century) oil painting created by PERSON(Leonardo). It&#39;s held at the FAC(Louvre) in GPE(Paris). The result has the following visualization: &lt;/p&gt; The Mona Lisa PERSON is a 16th century DATE oil painting created by Leonardo PERSON . It&#39;s held at the Louvre FAC in Paris GPE . Here FAC is facility (buildings, airports, highways, bridges, etc.) and GPE is Geo-Political Entity (countries, cities, states, etc.). Sentence Detection Detecting sentences in text might seem a simple concept at first but there are numerous special cases. Consider the following text: def text = &#39;&#39;&#39;The most referenced scientific paper of all time is &quot;Protein measurement with theFolin phenol reagent&quot; by Lowry, O. H., Rosebrough, N. J., Farr, A. L. &amp; Randall,R. J. and was published in the J. BioChem. in 1951. It describes a method formeasuring the amount of protein (even as small as 0.2 &gamma;, were &gamma; is the specificweight) in solutions and has been cited over 300,000 times and can be found here:https://www.jbc.org/content/193/1/265.full.pdf. Dr. Lowry completedtwo doctoral degrees under an M.D.-Ph.D. program from the University of Chicagobefore moving to Harvard under A. Baird Hastings. He was also the H.O.D ofPharmacology at Washington University in St. Louis for 29 years.&#39;&#39;&#39; There are full stops at the end of each sentence (though in general, it could also be other punctuation like exclamation marks and question marks). There are also full stops and decimal points in abbreviations, URLs, decimal numbers and so forth. Sentence detection algorithms might have some special hard-coded cases, like &quot;Dr.&quot;, &quot;Ms.&quot;, or in an emoticon, and may also use some heuristics. In general, they might also be trained with examples like above. Here is some code for OpenNLP for detecting sentences in the above: def helper = new ResourceHelper(&#39;http://opennlp.sourceforge.net/models-1.5&#39;)def model = new SentenceModel(helper.load(&#39;en-sent&#39;))def detector = new SentenceDetectorME(model)def sentences = detector.sentDetect(text)assert text.count(&#39;.&#39;) == 28assert sentences.size() == 4println &quot;Found ${sentences.size()} sentences:\n&quot; + sentences.join(&#39;\n\n&#39;) It has the following output: Downloading en-sent Found 4 sentences: The most referenced scientific paper of all time is &quot;Protein measurement with the Folin phenol reagent&quot; by Lowry, O. H., Rosebrough, N. J., Farr, A. L. &amp; Randall, R. J. and was published in the J. BioChem. in 1951. It describes a method for measuring the amount of protein (even as small as 0.2 &gamma;, were &gamma; is the specific weight) in solutions and has been cited over 300,000 times and can be found here: https://www.jbc.org/content/193/1/265.full.pdf. Dr. Lowry completed two doctoral degrees under an M.D.-Ph.D. program from the University of Chicago before moving to Harvard under A. Baird Hastings. He was also the H.O.D of Pharmacology at Washington University in St. Louis for 29 years. We can see here, it handled all of the tricky cases in the example. Relationship Extraction with Triples The next step after detecting named entities and the various parts of speech of certain words is to explore relationships between them. This is often done in the form of subject-predicate-object triplets. In our earlier NER example, for the sentence &quot;&lt;span style=&quot;background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, &quot;Courier New&quot;, monospace; font-size: 13px;&quot;&gt;The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark.&lt;/span&gt;&quot;, we found various date, time and location named entities. We can extract triples using the MinIE library (which in turns uses the Standford CoreNLP library) with the following code: def parser = CoreNLPUtils.StanfordDepNNParser()sentences.each { sentence -&gt; def minie = new MinIE(sentence, parser, MinIE.Mode.SAFE) println &quot;\nInput sentence: $sentence&quot; println &#39;=============================&#39; println &#39;Extractions:&#39; for (ap in minie.propositions) { println &quot;\tTriple: $ap.tripleAsString&quot; def attr = ap.attribution.attributionPhrase ? ap.attribution.toStringCompact() : &#39;NONE&#39; println &quot;\tFactuality: $ap.factualityAsString\tAttribution: $attr&quot; println &#39;\t----------&#39; }} The output for the previously mentioned sentence is shown below: Input sentence: The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark. ============================= Extractions: Triple: &quot;conference&quot; &quot;wrapped up yesterday at&quot; &quot;5:30 p.m.&quot; Factuality: (+,CT) Attribution: NONE ---------- Triple: &quot;conference&quot; &quot;wrapped up yesterday in&quot; &quot;Copenhagen&quot; Factuality: (+,CT) Attribution: NONE ---------- Triple: &quot;conference&quot; &quot;wrapped up&quot; &quot;yesterday&quot; Factuality: (+,CT) Attribution: NONE We can now piece together the relationships between the earlier entities we detected. There was also a problematic case amongst the earlier NER examples, &quot;&lt;span style=&quot;background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, &quot;Courier New&quot;, monospace; font-size: 13px;&quot;&gt;The parcel was passed from May to June.&lt;/span&gt;&quot;. Using the previous model, detected &quot;&lt;span style=&quot;background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, &quot;Courier New&quot;, monospace; font-size: 13px;&quot;&gt;May to June&lt;/span&gt;&quot; as a date. Let&#39;s explore that using CoreNLP&#39;s triple extraction directly. We won&#39;t show the source code here but CoreNLP supports simple and more powerful approaches to solving this problem. The output for the sentence in question using the more powerful technique is: Sentence #7: The parcel was passed from May to June. root(ROOT-0, passed-4) det(parcel-2, The-1) nsubj:pass(passed-4, parcel-2) aux:pass(passed-4, was-3) case(May-6, from-5) obl:from(passed-4, May-6) case(June-8, to-7) obl:to(passed-4, June-8) punct(passed-4, .-9) Triples: 1.0 parcel was passed 1.0 parcel was passed to June 1.0 parcel was passed from May to June 1.0 parcel was passed from May We can see that this has done a better job of piecing together what entities we have and their relationships. Sentiment Analysis Sentiment analysis is a NLP technique used to determine whether data is positive, negative, or neutral. Standford CoreNLP has default models it uses for this purpose: def doc = new Document(&#39;&#39;&#39;StanfordNLP is fantastic!Groovy is great fun!Math can be hard!&#39;&#39;&#39;)for (sent in doc.sentences()) { println &quot;${sent.toString().padRight(40)} ${sent.sentiment()}&quot;} Which has the following output: [main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.6 sec]. [main] INFO edu.stanford.nlp.sentiment.SentimentModel - Loading sentiment model edu/stanford/nlp/models/sentiment/sentiment.ser.gz ... done [0.1 sec]. StanfordNLP is fantastic!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; POSITIVE Groovy is great fun!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;VERY_POSITIVE Math can be hard!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; NEUTRAL We can also train our own. Let&#39;s start with two datasets: def datasets = [ positive: getClass().classLoader.getResource(&quot;rt-polarity.pos&quot;).toURI(), negative: getClass().classLoader.getResource(&quot;rt-polarity.neg&quot;).toURI()] We&#39;ll first use Datumbox which, as we saw earlier, requires training parameters for our algorithm: def trainingParams = new TextClassifier.TrainingParameters( numericalScalerTrainingParameters: null, featureSelectorTrainingParametersList: [new ChisquareSelect.TrainingParameters()], textExtractorParameters: new NgramsExtractor.Parameters(), modelerTrainingParameters: new MultinomialNaiveBayes.TrainingParameters()) We now create our algorithm, train it with or training dataset, and for illustrative purposes validate against the training dataset: def config = Configuration.configurationTextClassifier classifier = MLBuilder.create(trainingParams, config)classifier.fit(datasets)def metrics = classifier.validate(datasets)println &quot;Classifier Accuracy (using training data): $metrics.accuracy&quot; The output is shown here: [main] INFO com.datumbox.framework.core.common.dataobjects.Dataframe$Builder - Dataset Parsing positive class [main] INFO com.datumbox.framework.core.common.dataobjects.Dataframe$Builder - Dataset Parsing negative class ... Classifier Accuracy (using training data): 0.8275959103273615 Now we can test our model against several sentences: [&#39;Datumbox is divine!&#39;, &#39;Groovy is great fun!&#39;, &#39;Math can be hard!&#39;].each { def r = classifier.predict(it) def predicted = r.YPredicted def probability = sprintf &#39;%4.2f&#39;, r.YPredictedProbabilities.get(predicted) println &quot;Classifing: &#39;$it&#39;, Predicted: $predicted, Probability: $probability&quot;} Which has this output: ... [main] INFO com.datumbox.framework.applications.nlp.TextClassifier - predict() ... Classifing: &#39;Datumbox is divine!&#39;, Predicted: positive, Probability: 0.83 Classifing: &#39;Groovy is great fun!&#39;, Predicted: positive, Probability: 0.80 Classifing: &#39;Math can be hard!&#39;, Predicted: negative, Probability: 0.95 We can do the same thing but with OpenNLP. First, we collect our input data. OpenNLP is expecting it in a single dataset with tagged examples: def trainingCollection = datasets.collect { k, v -&gt; new File(v).readLines().collect{&quot;$k $it&quot;.toString() }}.sum() Now, we&#39;ll train two models. One uses na&iuml;ve bayes, the other maxent. We train up both variants. def variants = [ Maxent : new TrainingParameters(), NaiveBayes: new TrainingParameters((CUTOFF_PARAM): &#39;0&#39;, (ALGORITHM_PARAM): NAIVE_BAYES_VALUE)]def models = [:]variants.each{ key, trainingParams -&gt; def trainingStream = new CollectionObjectStream(trainingCollection) def sampleStream = new DocumentSampleStream(trainingStream) println &quot;\nTraining using $key&quot; models[key] = DocumentCategorizerME.train(&#39;en&#39;, sampleStream, trainingParams, new DoccatFactory())} Now we run sentiment predictions on our sample sentences using both variants: def w = sentences*.size().max()variants.each { key, params -&gt; def categorizer = new DocumentCategorizerME(models[key]) println &quot;\nAnalyzing using $key&quot; sentences.each { def result = categorizer.categorize(it.split(&#39;[ !]&#39;)) def category = categorizer.getBestCategory(result) def prob = sprintf &#39;%4.2f&#39;, result[categorizer.getIndex(category)] println &quot;${it.padRight(w)} $category ($prob)}&quot; }} When we run this we get: Training using Maxent ...done. ... Training using NaiveBayes ...done. ... Analyzing using Maxent OpenNLP is fantastic! positive (0.64)} Groovy is great fun! positive (0.74)} Math can be hard! negative (0.61)} Analyzing using NaiveBayes OpenNLP is fantastic! positive (0.72)} Groovy is great fun! positive (0.81)} Math can be hard! negative (0.72)} The models here appear to have lower probability levels compared to the model we trained for Datumbox. We could try tweaking the training parameters further if this was a problem. We&#39;d probably also need a bigger testing set to convince ourselves of the relative merits of each model. Some models can be over-trained on small datasets and perform very well with data similar to their training datasets but perform much worse for other data. Universal Sentence Encoding This example is inspired from the UniversalSentenceEncoder example in the DJL examples module. It looks at using the universal sentence encoder model from TensorFlow Hub via the DeepJavaLibrary (DJL) api. First we define a translator. The Translator interface allow us to specify pre and post processing functionality. class MyTranslator implements NoBatchifyTranslator&lt;String[], double[][]&gt; { @Override NDList processInput(TranslatorContext ctx, String[] raw) { var factory = ctx.NDManager var inputs = new NDList(raw.collect(factory::create)) new NDList(NDArrays.stack(inputs)) } @Override double[][] processOutput(TranslatorContext ctx, NDList list) { long numOutputs = list.singletonOrThrow().shape.get(0) NDList result = [] for (i in 0..&lt;numOutputs) { result &lt;&lt; list.singletonOrThrow().get(i) } result*.toFloatArray() as double[][] }} Here, we manually pack our input sentences into the required n-dimensional data types, and extract our output calculations into a 2D double array. Next, we create our predict method by first defining the criteria for our prediction algorithm. We are going to use our translator, use the TensorFlow engine, use a predefined sentence encoder model from the TensorFlow Hub, and indicate that we are creating a text embedding application: def predict(String[] inputs) { String modelUrl = &quot;https://storage.googleapis.com/tfhub-modules/google/universal-sentence-encoder/4.tar.gz&quot; Criteria&lt;String[], double[][]&gt; criteria = Criteria.builder() .optApplication(Application.NLP.TEXT_EMBEDDING) .setTypes(String[], double[][]) .optModelUrls(modelUrl) .optTranslator(new MyTranslator()) .optEngine(&quot;TensorFlow&quot;) .optProgress(new ProgressBar()) .build() try (var model = criteria.loadModel() var predictor = model.newPredictor()) { predictor.predict(inputs) }} Next, let&#39;s define our input strings: String[] inputs = [ &quot;Cycling is low impact and great for cardio&quot;, &quot;Swimming is low impact and good for fitness&quot;, &quot;Palates is good for fitness and flexibility&quot;, &quot;Weights are good for strength and fitness&quot;, &quot;Orchids can be tricky to grow&quot;, &quot;Sunflowers are fun to grow&quot;, &quot;Radishes are easy to grow&quot;, &quot;The taste of radishes grows on you after a while&quot;,]var k = inputs.size() Now, we&#39;ll use our predictor method to calculate the embeddings for each sentence. We&#39;ll print out the embeddings and also calculate the dot product of the embeddings. The dot product (the same as the inner product for this case) reveals how related the sentences are. var embeddings = predict(inputs)var z = new double[k][k]for (i in 0..&lt;k) { println &quot;Embedding for: ${inputs[i]}\n${embeddings[i]}&quot; for (j in 0..&lt;k) { z[i][j] = dot(embeddings[i], embeddings[j]) }} Finally, we&#39;ll use the Heatmap class from Smile to present a nice display highlighting what the data reveals: new Heatmap(inputs, inputs, z, Palette.heat(20).reverse()).canvas().with { title = &#39;Semantic textual similarity&#39; setAxisLabels(&#39;&#39;, &#39;&#39;) window()} The output shows us the embeddings: Loading: 100% |========================================| 2022-08-07 17:10:43.212697: ... This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 ... 2022-08-07 17:10:52.589396: ... SavedModel load for tags { serve }; Status: success: OK... ... Embedding for: Cycling is low impact and great for cardio [-0.02865048497915268, 0.02069241739809513, 0.010843578726053238, -0.04450441896915436, ...] ... Embedding for: The taste of radishes grows on you after a while [0.015841705724596977, -0.03129228577017784, 0.01183396577835083, 0.022753292694687843, ...] The embeddings are an indication of similarity. Two sentences with similar meaning typically have similar embeddings. The displayed graphic is shown below: This graphic shows that our first four sentences are somewhat related, as are the last four sentences, but that there is minimal relationship between those two groups. More information Further examples can be found in the related repos: https://github.com/paulk-asert/groovy-data-science/blob/master/subprojects/LanguageProcessing https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingSparkNLP https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingDjl Conclusion We have look at a range of NLP examples using various NLP libraries. Hopefully you can see some cases where you could use additional NLP technologies in some of your own applications." />
<link rel="canonical" href="http://localhost:4000/groovy/entry/natural-language-processing-with-groovy" />
<meta property="og:url" content="http://localhost:4000/groovy/entry/natural-language-processing-with-groovy" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-07T12:39:03-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Natural Language Processing with Groovy, OpenNLP, CoreNLP, Nlp4j, Datumbox, Smile, Spark NLP, DJL and TensorFlow" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-08-07T12:39:03-04:00","datePublished":"2022-08-07T12:39:03-04:00","description":"Natural Language Processing is certainly a large and sometimes complex topic with many aspects. Some of those aspects deserve entire blogs in their own right. For this blog, we will briefly look at a few simple use cases illustrating where you might be able to use NLP technology in your own project. Language Detection Knowing what language some text represents can be a critical first step to subsequent processing. Let&#39;s look at how to predict the language using a pre-built model and Apache OpenNLP. Here, ResourceHelper is a utility class used to download and cache the model. The first run may take a little while as it downloads the model. Subsequent runs should be fast. Here we are using a well-known model referenced in the OpenNLP documentation. def helper = new ResourceHelper(&#39;https://dlcdn.apache.org/opennlp/models/langdetect/1.8.3/&#39;)def model = new LanguageDetectorModel(helper.load(&#39;langdetect-183&#39;))def detector = new LanguageDetectorME(model)[ spa: &#39;Bienvenido a Madrid&#39;, fra: &#39;Bienvenue &agrave; Paris&#39;, dan: &#39;Velkommen til K&oslash;benhavn&#39;, bul: &#39;Добре дошли в София&#39;].each { k, v -&gt; assert detector.predictLanguage(v).lang == k} The LanguageDetectorME class lets us predict the language. In general, the predictor may not be accurate on small samples of text but it was good enough for our example. We&#39;ve used the language code as the key in our map and we check that against the predicted language. A more complex scenario is training your own model. Let&#39;s look at how to do that with Datumbox. Datumbox has a pre-trained models zoo but its language detection model didn&#39;t seem to work well for the small snippets in the next example, so we&#39;ll train our own model. First, we&#39;ll define our datasets: def datasets = [ English: getClass().classLoader.getResource(&quot;training.language.en.txt&quot;).toURI(), French: getClass().classLoader.getResource(&quot;training.language.fr.txt&quot;).toURI(), German: getClass().classLoader.getResource(&quot;training.language.de.txt&quot;).toURI(), Spanish: getClass().classLoader.getResource(&quot;training.language.es.txt&quot;).toURI(), Indonesian: getClass().classLoader.getResource(&quot;training.language.id.txt&quot;).toURI()] The de training dataset comes from the Datumbox examples. The training datasets for the other languages are from Kaggle. We set up the training parameters needed by our algorithm: def trainingParams = new TextClassifier.TrainingParameters( numericalScalerTrainingParameters: null, featureSelectorTrainingParametersList: [new ChisquareSelect.TrainingParameters()], textExtractorParameters: new NgramsExtractor.Parameters(), modelerTrainingParameters: new MultinomialNaiveBayes.TrainingParameters()) We&#39;ll use a Na&iuml;ve Bayes model with Chisquare feature selection. Next we create our algorithm, train it with our training dataset, and then validate it against the training dataset. We&#39;d normally want to split the data into training and testing datasets, to give us a more accurate statistic of the accuracy of our model. But for simplicity, while still illustrating the API, we&#39;ll train and validate with our entire dataset: def config = Configuration.configurationdef classifier = MLBuilder.create(trainingParams, config)classifier.fit(datasets)def metrics = classifier.validate(datasets)println &quot;Classifier Accuracy (using training data): $metrics.accuracy&quot; When run, we see the following output: Classifier Accuracy (using training data): 0.9975609756097561 Our test dataset will consist of some hard-coded illustrative phrases. Let&#39;s use our model to predict the language for each phrase: [ &#39;Bienvenido a Madrid&#39;, &#39;Bienvenue &agrave; Paris&#39;, &#39;Welcome to London&#39;, &#39;Willkommen in Berlin&#39;, &#39;Selamat Datang di Jakarta&#39;].each { txt -&gt; def r = classifier.predict(txt) def predicted = r.YPredicted def probability = sprintf &#39;%4.2f&#39;, r.YPredictedProbabilities.get(predicted) println &quot;Classifying: &#39;$txt&#39;, Predicted: $predicted, Probability: $probability&quot;} When run, it has this output: Classifying: &#39;Bienvenido a Madrid&#39;,&nbsp; Predicted: Spanish,&nbsp; Probability: 0.83 Classifying: &#39;Bienvenue &agrave; Paris&#39;,&nbsp; Predicted: French,&nbsp; Probability: 0.71 Classifying: &#39;Welcome to London&#39;,&nbsp; Predicted: English,&nbsp; Probability: 1.00 Classifying: &#39;Willkommen in Berlin&#39;,&nbsp; Predicted: German,&nbsp; Probability: 0.84 Classifying: &#39;Selamat Datang di Jakarta&#39;,&nbsp; Predicted: Indonesian,&nbsp; Probability: 1.00 Given these phrases are very short, it is nice to get them all correct, and the probabilities all seem reasonable for this scenario. Parts of Speech Parts of speech (POS) analysers examine each part of a sentence (the words and potentially punctuation) in terms of the role they play in a sentence. A typical analyser will assign or annotate words with their role like identifying nouns, verbs, adjectives and so forth. This can be a key early step for tools like the voice assistants from Amazon, Apple and Google. We&#39;ll start by looking at a perhaps lesser known library Nlp4j before looking at some others. In fact, there are multiple Nlp4j libraries. We&#39;ll use the one from nlp4j.org,&nbsp;which seems to be the most active and recently updated. This library uses the Stanford CoreNLP library under the covers for its English POS functionality. The library has the concept of documents, and annotators that work on documents. Once annotated, we can print out all of the discovered words and their annotations: var doc = new DefaultDocument()doc.putAttribute(&#39;text&#39;, &#39;I eat sushi with chopsticks.&#39;)var ann = new StanfordPosAnnotator()ann.setProperty(&#39;target&#39;, &#39;text&#39;)ann.annotate(doc)println doc.keywords.collect{ k -&gt; &quot;${k.facet - &#39;word.&#39;}(${k.str})&quot; }.join(&#39; &#39;) When run, we see the following output: PRP(I) VBP(eat) NN(sushi) IN(with) NNS(chopsticks) .(.) The annotations, also known as tags or facets, for this example are as follows: PRP Personal pronoun VBP Present tense verb NN Noun, singular IN Preposition NNS Noun, plural The documentation for the libraries we are using give a more complete list of such annotations. A nice aspect of this library is support for other languages, in particular, Japanese. The code is very similar but uses a different annotator: doc = new DefaultDocument()doc.putAttribute(&#39;text&#39;, &#39;私は学校に行きました。&#39;)ann = new KuromojiAnnotator()ann.setProperty(&#39;target&#39;, &#39;text&#39;)ann.annotate(doc)println doc.keywords.collect{ k -&gt; &quot;${k.facet}(${k.str})&quot; }.join(&#39; &#39;) When run, we see the following output: 名詞(私) 助詞(は) 名詞(学校) 助詞(に) 動詞(行き) 助動詞(まし) 助動詞(た) 記号(。) Before progressing, we&#39;ll highlight the result visualization capabilities of the GroovyConsole. This feature lets us write a small Groovy script which converts results to any swing component. In our case we&#39;ll convert lists of annotated strings to a JLabel component containing HTML including colored annotation boxes. The details aren&#39;t included here but can be found in the repo. We need to copy that file into our ~/.groovy folder and then enable script visualization as shown here: Then we should see the following when running the script: The visualization is purely optional but adds a nice touch. If using Groovy in notebook environments like Jupyter/BeakerX, there might be visualization tools in those environments too. Let&#39;s look at a larger example using the Smile library. First, the sentences that we&#39;ll examine: def sentences = [ &#39;Paul has two sisters, Maree and Christine.&#39;, &#39;No wise fish would go anywhere without a porpoise&#39;, &#39;His bark was much worse than his bite&#39;, &#39;Turn on the lights to the main bedroom&#39;, &quot;Light &#39;em all up&quot;, &#39;Make it dark downstairs&#39;] A couple of those sentences might seem a little strange but they are selected to show off quite a few of the different POS tags. Smile has a tokenizer class which splits a sentence into words. It handles numerous cases like contractions and abbreviations (&quot;e.g.&quot;, &quot;&#39;tis&quot;, &quot;won&#39;t&quot;). Smile also has a POS class based on the&nbsp;hidden Markov model and a built-in model is used for that class. Here is our code using those classes: def tokenizer = new SimpleTokenizer(true)sentences.each { def tokens = Arrays.stream(tokenizer.split(it)).toArray(String[]::new) def tags = HMMPOSTagger.default.tag(tokens)*.toString() println tokens.indices.collect{tags[it] == tokens[it] ? tags[it] : &quot;${tags[it]}(${tokens[it]})&quot; }.join(&#39; &#39;)} We run the tokenizer for each sentence. Each token is then displayed directly or with its tag if it has one. Running the script gives this visualization: &lt;/p&gt; Paul NNP has VBZ two CD sisters NNS , Maree NNP and CC Christine NNP . No DT wise JJ fish NN would MD go VB anywhere RB without IN a DT porpoise NN His PRP$ bark NN was VBD much RB worse JJR than IN his PRP$ bite NN Turn VB on IN the DT lights NNS to TO the DT main JJ bedroom NN Light NNP &#39;em PRP all RB up RB Make VB it PRP dark JJ downstairs NN [Note: the scripts in the repo just print to stdout which is perfect when using the command-line or IDEs. The visualization in the GoovyConsole kicks in only for the actual result. So, if you are following along at home and wanting to use the GroovyConsole, you&#39;d change the each to collect and remove the println, and you should be good for visualization.] The OpenNLP code is very similar: def tokenizer = SimpleTokenizer.INSTANCEsentences.each { String[] tokens = tokenizer.tokenize(it) def posTagger = new POSTaggerME(&#39;en&#39;) String[] tags = posTagger.tag(tokens) println tokens.indices.collect{tags[it] == tokens[it] ? tags[it] : &quot;${tags[it]}(${tokens[it]})&quot; }.join(&#39; &#39;)} OpenNLP allows you to supply your own POS model but downloads a default one if none is specified. When the script is run, it has this visualization: &lt;/p&gt; Paul PROPN has VERB two NUM sisters NOUN , PUNCT Maree PROPN and CCONJ Christine PROPN . PUNCT No DET wise ADJ fish NOUN would AUX go VERB anywhere ADV without ADP a DET porpoise NOUN His PRON bark NOUN was AUX much ADV worse ADJ than ADP his PRON bite NOUN Turn VERB on ADP the DET lights NOUN to ADP the DET main ADJ bedroom NOUN Light NOUN &#39; PUNCT em NOUN all ADV up ADP Make VERB it PRON dark ADJ downstairs NOUN The observant reader may have noticed some slight differences in the tags used in this library. They are essentially the same but using slightly different names. This is something to be aware of when swapping between POS libraries or models. Make sure you look up the documentation for the library/model you are using to understand the available tag types. Entity Detection Named entity recognition (NER), seeks to identity and classify named entities in text. Categories of interest might be persons, organizations, locations dates, etc. It is another technology used in many fields of NLP. We&#39;ll start with our sentences to analyse: String[] sentences = [ &quot;A commit by Daniel Sun on December 6, 2020 improved Groovy 4&#39;s language integrated query.&quot;, &quot;A commit by Daniel on Sun., December 6, 2020 improved Groovy 4&#39;s language integrated query.&quot;, &#39;The Groovy in Action book by Dierk Koenig et. al. is a bargain at $50, or indeed any price.&#39;, &#39;The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark.&#39;, &#39;I saw Ms. May Smith waving to June Jones.&#39;, &#39;The parcel was passed from May to June.&#39;, &#39;The Mona Lisa by Leonardo da Vinci has been on display in the Louvre, Paris since 1797.&#39;] We&#39;ll use some well-known models, we&#39;ll focus on the&nbsp;person, money, date, time, and location&nbsp;models: def base = &#39;http://opennlp.sourceforge.net/models-1.5&#39;def modelNames = [&#39;person&#39;, &#39;money&#39;, &#39;date&#39;, &#39;time&#39;, &#39;location&#39;]def finders = modelNames.collect { model -&gt; new NameFinderME(DownloadUtil.downloadModel(new URL(&quot;$base/en-ner-${model}.bin&quot;), TokenNameFinderModel))} We&#39;ll now tokenize our sentences: def tokenizer = SimpleTokenizer.INSTANCEsentences.each { sentence -&gt; String[] tokens = tokenizer.tokenize(sentence) Span[] tokenSpans = tokenizer.tokenizePos(sentence) def entityText = [:] def entityPos = [:] finders.indices.each {fi -&gt; // could be made smarter by looking at probabilities and overlapping spans Span[] spans = finders[fi].find(tokens) spans.each{span -&gt; def se = span.start..&lt;span.end def pos = (tokenSpans[se.from].start)..&lt;(tokenSpans[se.to].end) entityPos[span.start] = pos entityText[span.start] = &quot;$span.type(${sentence[pos]})&quot; } } entityPos.keySet().sort().reverseEach { def pos = entityPos[it] def (from, to) = [pos.from, pos.to + 1] sentence = sentence[0..&lt;from] + entityText[it] + sentence[to..-1] } println sentence} And when visualized, shows this: &lt;/p&gt; A commit by Daniel Sun person on December 6, 2020 date improved Groovy 4&#39;s language integrated query. A commit by Daniel person on Sun., December 6, 2020 date improved Groovy 4&#39;s language integrated query. The Groovy in Action book by Dierk Koenig person et. al. is a bargain at $50 money , or indeed any price. The conference wrapped up yesterday date at 5:30 p.m. time in Copenhagen location , Denmark location . I saw Ms. May Smith person waving to June Jones person . The parcel was passed from May to June date . The Mona Lisa by Leonardo da Vinci person has been on display in the Louvre, Paris location since 1797 date . We can see here that most examples have been categorized as we might expect. We&#39;d have to improve our model for it to do a better job on the &quot;May to June&quot; example. Scaling Entity Detection We can also run our named entity detection algorithms on platforms like Spark NLP which adds NLP functionality to Apache Spark. We&#39;ll use glove_100d embeddings and the onto_100 NER model. var assembler = new DocumentAssembler(inputCol: &#39;text&#39;, outputCol: &#39;document&#39;, cleanupMode: &#39;disabled&#39;)var tokenizer = new Tokenizer(inputCols: [&#39;document&#39;] as String[], outputCol: &#39;token&#39;)var embeddings = WordEmbeddingsModel.pretrained(&#39;glove_100d&#39;).tap { inputCols = [&#39;document&#39;, &#39;token&#39;] as String[] outputCol = &#39;embeddings&#39;}var model = NerDLModel.pretrained(&#39;onto_100&#39;, &#39;en&#39;).tap { inputCols = [&#39;document&#39;, &#39;token&#39;, &#39;embeddings&#39;] as String[] outputCol =&#39;ner&#39;}var converter = new NerConverter(inputCols: [&#39;document&#39;, &#39;token&#39;, &#39;ner&#39;] as String[], outputCol: &#39;ner_chunk&#39;)var pipeline = new Pipeline(stages: [assembler, tokenizer, embeddings, model, converter] as PipelineStage[])var spark = SparkNLP.start(false, false, &#39;16G&#39;, &#39;&#39;, &#39;&#39;, &#39;&#39;)var text = [ &quot;The Mona Lisa is a 16th century oil painting created by Leonardo. It&#39;s held at the Louvre in Paris.&quot;]var data = spark.createDataset(text, Encoders.STRING()).toDF(&#39;text&#39;)var pipelineModel = pipeline.fit(data)var transformed = pipelineModel.transform(data)transformed.show()use(SparkCategory) { transformed.collectAsList().each { row -&gt; def res = row.text def chunks = row.ner_chunk.reverseIterator() while (chunks.hasNext()) { def chunk = chunks.next() int begin = chunk.begin int end = chunk.end def entity = chunk.metadata.get(&#39;entity&#39;).get() res = res[0..&lt;begin] + &quot;$entity($chunk.result)&quot; + res[end&lt;..-1] } println res }} We won&#39;t go into all of the details here. In summary, the code sets up a pipeline that transforms our input sentences, via a series of steps, into chunks, where each chunk corresponds to a detected entity. Each chunk has a start and ending position, and an associated tag type. This may not seem like it is much different to our earlier examples, but if we had large volumes of data and we were running in a large cluster, the work could be spread across worker nodes within the cluster. Here we have used a utility SparkCategory class which makes accessing the information in Spark Row instances a little nicer in terms of Groovy shorthand syntax. We can use row.text instead of row.get(row.fieldIndex(&#39;text&#39;)). Here is the code for this utility class: class SparkCategory { static get(Row r, String field) { r.get(r.fieldIndex(field)) }} If doing more than this simple example, the use of SparkCategory could be made implicit through various standard Groovy techniques. When we run our script, we see the following output: 22/08/07 12:31:39 INFO SparkContext: Running Spark version 3.3.0 ... glove_100d download started this may take some time. Approximate size to download 145.3 MB ... onto_100 download started this may take some time. Approximate size to download 13.5 MB ... +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+ | text| document| token| embeddings| ner| ner_chunk| +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+ |The Mona Lisa is ...|[{document, 0, 98...|[{token, 0, 2, Th...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 0, 12, T...| +--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+ PERSON(The Mona Lisa) is a DATE(16th century) oil painting created by PERSON(Leonardo). It&#39;s held at the FAC(Louvre) in GPE(Paris). The result has the following visualization: &lt;/p&gt; The Mona Lisa PERSON is a 16th century DATE oil painting created by Leonardo PERSON . It&#39;s held at the Louvre FAC in Paris GPE . Here FAC is facility (buildings, airports, highways, bridges, etc.) and GPE is Geo-Political Entity (countries, cities, states, etc.). Sentence Detection Detecting sentences in text might seem a simple concept at first but there are numerous special cases. Consider the following text: def text = &#39;&#39;&#39;The most referenced scientific paper of all time is &quot;Protein measurement with theFolin phenol reagent&quot; by Lowry, O. H., Rosebrough, N. J., Farr, A. L. &amp; Randall,R. J. and was published in the J. BioChem. in 1951. It describes a method formeasuring the amount of protein (even as small as 0.2 &gamma;, were &gamma; is the specificweight) in solutions and has been cited over 300,000 times and can be found here:https://www.jbc.org/content/193/1/265.full.pdf. Dr. Lowry completedtwo doctoral degrees under an M.D.-Ph.D. program from the University of Chicagobefore moving to Harvard under A. Baird Hastings. He was also the H.O.D ofPharmacology at Washington University in St. Louis for 29 years.&#39;&#39;&#39; There are full stops at the end of each sentence (though in general, it could also be other punctuation like exclamation marks and question marks). There are also full stops and decimal points in abbreviations, URLs, decimal numbers and so forth. Sentence detection algorithms might have some special hard-coded cases, like &quot;Dr.&quot;, &quot;Ms.&quot;, or in an emoticon, and may also use some heuristics. In general, they might also be trained with examples like above. Here is some code for OpenNLP for detecting sentences in the above: def helper = new ResourceHelper(&#39;http://opennlp.sourceforge.net/models-1.5&#39;)def model = new SentenceModel(helper.load(&#39;en-sent&#39;))def detector = new SentenceDetectorME(model)def sentences = detector.sentDetect(text)assert text.count(&#39;.&#39;) == 28assert sentences.size() == 4println &quot;Found ${sentences.size()} sentences:\\n&quot; + sentences.join(&#39;\\n\\n&#39;) It has the following output: Downloading en-sent Found 4 sentences: The most referenced scientific paper of all time is &quot;Protein measurement with the Folin phenol reagent&quot; by Lowry, O. H., Rosebrough, N. J., Farr, A. L. &amp; Randall, R. J. and was published in the J. BioChem. in 1951. It describes a method for measuring the amount of protein (even as small as 0.2 &gamma;, were &gamma; is the specific weight) in solutions and has been cited over 300,000 times and can be found here: https://www.jbc.org/content/193/1/265.full.pdf. Dr. Lowry completed two doctoral degrees under an M.D.-Ph.D. program from the University of Chicago before moving to Harvard under A. Baird Hastings. He was also the H.O.D of Pharmacology at Washington University in St. Louis for 29 years. We can see here, it handled all of the tricky cases in the example. Relationship Extraction with Triples The next step after detecting named entities and the various parts of speech of certain words is to explore relationships between them. This is often done in the form of subject-predicate-object triplets. In our earlier NER example, for the sentence &quot;&lt;span style=&quot;background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, &quot;Courier New&quot;, monospace; font-size: 13px;&quot;&gt;The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark.&lt;/span&gt;&quot;, we found various date, time and location named entities. We can extract triples using the MinIE library (which in turns uses the Standford CoreNLP library) with the following code: def parser = CoreNLPUtils.StanfordDepNNParser()sentences.each { sentence -&gt; def minie = new MinIE(sentence, parser, MinIE.Mode.SAFE) println &quot;\\nInput sentence: $sentence&quot; println &#39;=============================&#39; println &#39;Extractions:&#39; for (ap in minie.propositions) { println &quot;\\tTriple: $ap.tripleAsString&quot; def attr = ap.attribution.attributionPhrase ? ap.attribution.toStringCompact() : &#39;NONE&#39; println &quot;\\tFactuality: $ap.factualityAsString\\tAttribution: $attr&quot; println &#39;\\t----------&#39; }} The output for the previously mentioned sentence is shown below: Input sentence: The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark. ============================= Extractions: Triple: &quot;conference&quot; &quot;wrapped up yesterday at&quot; &quot;5:30 p.m.&quot; Factuality: (+,CT) Attribution: NONE ---------- Triple: &quot;conference&quot; &quot;wrapped up yesterday in&quot; &quot;Copenhagen&quot; Factuality: (+,CT) Attribution: NONE ---------- Triple: &quot;conference&quot; &quot;wrapped up&quot; &quot;yesterday&quot; Factuality: (+,CT) Attribution: NONE We can now piece together the relationships between the earlier entities we detected. There was also a problematic case amongst the earlier NER examples, &quot;&lt;span style=&quot;background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, &quot;Courier New&quot;, monospace; font-size: 13px;&quot;&gt;The parcel was passed from May to June.&lt;/span&gt;&quot;. Using the previous model, detected &quot;&lt;span style=&quot;background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, &quot;Courier New&quot;, monospace; font-size: 13px;&quot;&gt;May to June&lt;/span&gt;&quot; as a date. Let&#39;s explore that using CoreNLP&#39;s triple extraction directly. We won&#39;t show the source code here but CoreNLP supports simple and more powerful approaches to solving this problem. The output for the sentence in question using the more powerful technique is: Sentence #7: The parcel was passed from May to June. root(ROOT-0, passed-4) det(parcel-2, The-1) nsubj:pass(passed-4, parcel-2) aux:pass(passed-4, was-3) case(May-6, from-5) obl:from(passed-4, May-6) case(June-8, to-7) obl:to(passed-4, June-8) punct(passed-4, .-9) Triples: 1.0 parcel was passed 1.0 parcel was passed to June 1.0 parcel was passed from May to June 1.0 parcel was passed from May We can see that this has done a better job of piecing together what entities we have and their relationships. Sentiment Analysis Sentiment analysis is a NLP technique used to determine whether data is positive, negative, or neutral. Standford CoreNLP has default models it uses for this purpose: def doc = new Document(&#39;&#39;&#39;StanfordNLP is fantastic!Groovy is great fun!Math can be hard!&#39;&#39;&#39;)for (sent in doc.sentences()) { println &quot;${sent.toString().padRight(40)} ${sent.sentiment()}&quot;} Which has the following output: [main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.6 sec]. [main] INFO edu.stanford.nlp.sentiment.SentimentModel - Loading sentiment model edu/stanford/nlp/models/sentiment/sentiment.ser.gz ... done [0.1 sec]. StanfordNLP is fantastic!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; POSITIVE Groovy is great fun!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;VERY_POSITIVE Math can be hard!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; NEUTRAL We can also train our own. Let&#39;s start with two datasets: def datasets = [ positive: getClass().classLoader.getResource(&quot;rt-polarity.pos&quot;).toURI(), negative: getClass().classLoader.getResource(&quot;rt-polarity.neg&quot;).toURI()] We&#39;ll first use Datumbox which, as we saw earlier, requires training parameters for our algorithm: def trainingParams = new TextClassifier.TrainingParameters( numericalScalerTrainingParameters: null, featureSelectorTrainingParametersList: [new ChisquareSelect.TrainingParameters()], textExtractorParameters: new NgramsExtractor.Parameters(), modelerTrainingParameters: new MultinomialNaiveBayes.TrainingParameters()) We now create our algorithm, train it with or training dataset, and for illustrative purposes validate against the training dataset: def config = Configuration.configurationTextClassifier classifier = MLBuilder.create(trainingParams, config)classifier.fit(datasets)def metrics = classifier.validate(datasets)println &quot;Classifier Accuracy (using training data): $metrics.accuracy&quot; The output is shown here: [main] INFO com.datumbox.framework.core.common.dataobjects.Dataframe$Builder - Dataset Parsing positive class [main] INFO com.datumbox.framework.core.common.dataobjects.Dataframe$Builder - Dataset Parsing negative class ... Classifier Accuracy (using training data): 0.8275959103273615 Now we can test our model against several sentences: [&#39;Datumbox is divine!&#39;, &#39;Groovy is great fun!&#39;, &#39;Math can be hard!&#39;].each { def r = classifier.predict(it) def predicted = r.YPredicted def probability = sprintf &#39;%4.2f&#39;, r.YPredictedProbabilities.get(predicted) println &quot;Classifing: &#39;$it&#39;, Predicted: $predicted, Probability: $probability&quot;} Which has this output: ... [main] INFO com.datumbox.framework.applications.nlp.TextClassifier - predict() ... Classifing: &#39;Datumbox is divine!&#39;, Predicted: positive, Probability: 0.83 Classifing: &#39;Groovy is great fun!&#39;, Predicted: positive, Probability: 0.80 Classifing: &#39;Math can be hard!&#39;, Predicted: negative, Probability: 0.95 We can do the same thing but with OpenNLP. First, we collect our input data. OpenNLP is expecting it in a single dataset with tagged examples: def trainingCollection = datasets.collect { k, v -&gt; new File(v).readLines().collect{&quot;$k $it&quot;.toString() }}.sum() Now, we&#39;ll train two models. One uses na&iuml;ve bayes, the other maxent. We train up both variants. def variants = [ Maxent : new TrainingParameters(), NaiveBayes: new TrainingParameters((CUTOFF_PARAM): &#39;0&#39;, (ALGORITHM_PARAM): NAIVE_BAYES_VALUE)]def models = [:]variants.each{ key, trainingParams -&gt; def trainingStream = new CollectionObjectStream(trainingCollection) def sampleStream = new DocumentSampleStream(trainingStream) println &quot;\\nTraining using $key&quot; models[key] = DocumentCategorizerME.train(&#39;en&#39;, sampleStream, trainingParams, new DoccatFactory())} Now we run sentiment predictions on our sample sentences using both variants: def w = sentences*.size().max()variants.each { key, params -&gt; def categorizer = new DocumentCategorizerME(models[key]) println &quot;\\nAnalyzing using $key&quot; sentences.each { def result = categorizer.categorize(it.split(&#39;[ !]&#39;)) def category = categorizer.getBestCategory(result) def prob = sprintf &#39;%4.2f&#39;, result[categorizer.getIndex(category)] println &quot;${it.padRight(w)} $category ($prob)}&quot; }} When we run this we get: Training using Maxent ...done. ... Training using NaiveBayes ...done. ... Analyzing using Maxent OpenNLP is fantastic! positive (0.64)} Groovy is great fun! positive (0.74)} Math can be hard! negative (0.61)} Analyzing using NaiveBayes OpenNLP is fantastic! positive (0.72)} Groovy is great fun! positive (0.81)} Math can be hard! negative (0.72)} The models here appear to have lower probability levels compared to the model we trained for Datumbox. We could try tweaking the training parameters further if this was a problem. We&#39;d probably also need a bigger testing set to convince ourselves of the relative merits of each model. Some models can be over-trained on small datasets and perform very well with data similar to their training datasets but perform much worse for other data. Universal Sentence Encoding This example is inspired from the UniversalSentenceEncoder example in the DJL examples module. It looks at using the universal sentence encoder model from TensorFlow Hub via the DeepJavaLibrary (DJL) api. First we define a translator. The Translator interface allow us to specify pre and post processing functionality. class MyTranslator implements NoBatchifyTranslator&lt;String[], double[][]&gt; { @Override NDList processInput(TranslatorContext ctx, String[] raw) { var factory = ctx.NDManager var inputs = new NDList(raw.collect(factory::create)) new NDList(NDArrays.stack(inputs)) } @Override double[][] processOutput(TranslatorContext ctx, NDList list) { long numOutputs = list.singletonOrThrow().shape.get(0) NDList result = [] for (i in 0..&lt;numOutputs) { result &lt;&lt; list.singletonOrThrow().get(i) } result*.toFloatArray() as double[][] }} Here, we manually pack our input sentences into the required n-dimensional data types, and extract our output calculations into a 2D double array. Next, we create our predict method by first defining the criteria for our prediction algorithm. We are going to use our translator, use the TensorFlow engine, use a predefined sentence encoder model from the TensorFlow Hub, and indicate that we are creating a text embedding application: def predict(String[] inputs) { String modelUrl = &quot;https://storage.googleapis.com/tfhub-modules/google/universal-sentence-encoder/4.tar.gz&quot; Criteria&lt;String[], double[][]&gt; criteria = Criteria.builder() .optApplication(Application.NLP.TEXT_EMBEDDING) .setTypes(String[], double[][]) .optModelUrls(modelUrl) .optTranslator(new MyTranslator()) .optEngine(&quot;TensorFlow&quot;) .optProgress(new ProgressBar()) .build() try (var model = criteria.loadModel() var predictor = model.newPredictor()) { predictor.predict(inputs) }} Next, let&#39;s define our input strings: String[] inputs = [ &quot;Cycling is low impact and great for cardio&quot;, &quot;Swimming is low impact and good for fitness&quot;, &quot;Palates is good for fitness and flexibility&quot;, &quot;Weights are good for strength and fitness&quot;, &quot;Orchids can be tricky to grow&quot;, &quot;Sunflowers are fun to grow&quot;, &quot;Radishes are easy to grow&quot;, &quot;The taste of radishes grows on you after a while&quot;,]var k = inputs.size() Now, we&#39;ll use our predictor method to calculate the embeddings for each sentence. We&#39;ll print out the embeddings and also calculate the dot product of the embeddings. The dot product (the same as the inner product for this case) reveals how related the sentences are. var embeddings = predict(inputs)var z = new double[k][k]for (i in 0..&lt;k) { println &quot;Embedding for: ${inputs[i]}\\n${embeddings[i]}&quot; for (j in 0..&lt;k) { z[i][j] = dot(embeddings[i], embeddings[j]) }} Finally, we&#39;ll use the Heatmap class from Smile to present a nice display highlighting what the data reveals: new Heatmap(inputs, inputs, z, Palette.heat(20).reverse()).canvas().with { title = &#39;Semantic textual similarity&#39; setAxisLabels(&#39;&#39;, &#39;&#39;) window()} The output shows us the embeddings: Loading: 100% |========================================| 2022-08-07 17:10:43.212697: ... This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 ... 2022-08-07 17:10:52.589396: ... SavedModel load for tags { serve }; Status: success: OK... ... Embedding for: Cycling is low impact and great for cardio [-0.02865048497915268, 0.02069241739809513, 0.010843578726053238, -0.04450441896915436, ...] ... Embedding for: The taste of radishes grows on you after a while [0.015841705724596977, -0.03129228577017784, 0.01183396577835083, 0.022753292694687843, ...] The embeddings are an indication of similarity. Two sentences with similar meaning typically have similar embeddings. The displayed graphic is shown below: This graphic shows that our first four sentences are somewhat related, as are the last four sentences, but that there is minimal relationship between those two groups. More information Further examples can be found in the related repos: https://github.com/paulk-asert/groovy-data-science/blob/master/subprojects/LanguageProcessing https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingSparkNLP https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingDjl Conclusion We have look at a range of NLP examples using various NLP libraries. Hopefully you can see some cases where you could use additional NLP technologies in some of your own applications.","headline":"Natural Language Processing with Groovy, OpenNLP, CoreNLP, Nlp4j, Datumbox, Smile, Spark NLP, DJL and TensorFlow","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/groovy/entry/natural-language-processing-with-groovy"},"url":"http://localhost:4000/groovy/entry/natural-language-processing-with-groovy"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Natural Language Processing with Groovy, OpenNLP, CoreNLP, Nlp4j, Datumbox, Smile, Spark NLP, DJL and TensorFlow</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-08-07T12:39:03-04:00" itemprop="datePublished">Aug 7, 2022
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Paul King", "login"=>"paulk", "email"=>"paulk@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Natural Language Processing is certainly a large and sometimes complex topic with many aspects. Some of those aspects deserve entire blogs in their own right. For this blog, we will briefly look at a few simple use cases illustrating where you might be able to use NLP technology in your own project.</p>
<h3>Language Detection</h3>
<p>Knowing what language some text represents can be a critical first step to subsequent processing. Let's look at how to predict the language using a pre-built model and <a href="https://opennlp.apache.org/" target="_blank">Apache OpenNLP</a>. Here, <code>ResourceHelper</code> is a utility class used to download and cache the model. The first run may take a little while as it downloads the model. Subsequent runs should be fast. Here we are using a well-known model referenced in the OpenNLP documentation.</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>helper = <span style="color:#cc7832;">new </span>ResourceHelper(<span style="color:#6a8759;">'https://dlcdn.apache.org/opennlp/models/langdetect/1.8.3/'</span>)<br><span style="color:#cc7832;">def </span>model = <span style="color:#cc7832;">new </span>LanguageDetectorModel(helper.load(<span style="color:#6a8759;">'langdetect-183'</span>))<br><span style="color:#cc7832;">def </span>detector = <span style="color:#cc7832;">new </span>LanguageDetectorME(model)<br><br>[ <span style="color:#6a8759;">spa</span>: <span style="color:#6a8759;">'Bienvenido a Madrid'</span>, <span style="color:#6a8759;">fra</span>: <span style="color:#6a8759;">'Bienvenue &agrave; Paris'</span>,<br>  <span style="color:#6a8759;">dan</span>: <span style="color:#6a8759;">'Velkommen til K&oslash;benhavn'</span>, <span style="color:#6a8759;">bul</span>: <span style="color:#6a8759;">'Добре дошли в София'<br></span>].each <span style="font-weight:bold;">{ </span>k, v <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">    </span><span style="color:#cc7832;">assert </span>detector.predictLanguage(v).<span style="color:#9876aa;">lang </span>== k<br><span style="font-weight:bold;">}</span></pre>
<p>The <code>LanguageDetectorME</code> class lets us predict the language. In general, the predictor may not be accurate on small samples of text but it was good enough for our example. We've used the language code as the key in our map and we check that against the predicted language.</p>
<p>A more complex scenario is training your own model. Let's look at how to do that with <a href="https://www.datumbox.com/machine-learning-framework/" target="_blank">Datumbox</a>. Datumbox has a <a href="https://github.com/datumbox/datumbox-framework-zoo" target="_blank">pre-trained models zoo</a> but its language detection model didn't seem to work well for the small snippets in the next example, so we'll train our own model. First, we'll define our datasets:<br></p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>datasets = [<br>    <span style="color:#6a8759;">English</span>: getClass().<span style="color:#9876aa;">classLoader</span>.getResource(<span style="color:#6a8759;">"training.language.en.txt"</span>).toURI(),<br>    <span style="color:#6a8759;">French</span>: getClass().<span style="color:#9876aa;">classLoader</span>.getResource(<span style="color:#6a8759;">"training.language.fr.txt"</span>).toURI(),<br>    <span style="color:#6a8759;">German</span>: getClass().<span style="color:#9876aa;">classLoader</span>.getResource(<span style="color:#6a8759;">"training.language.de.txt"</span>).toURI(),<br>    <span style="color:#6a8759;">Spanish</span>: getClass().<span style="color:#9876aa;">classLoader</span>.getResource(<span style="color:#6a8759;">"training.language.es.txt"</span>).toURI(),<br>    <span style="color:#6a8759;">Indonesian</span>: getClass().<span style="color:#9876aa;">classLoader</span>.getResource(<span style="color:#6a8759;">"training.language.id.txt"</span>).toURI()<br>]</pre>
<p style="">The <code>de</code> training dataset comes from the <a href="https://github.com/datumbox/NaiveBayesClassifier/tree/master/resources/datasets/training.language.de.txt" target="_blank">Datumbox examples</a>. The training datasets for the other languages are from <a href="https://www.kaggle.com/zarajamshaid/language-identification-datasst" target="_blank">Kaggle</a>.</p>
<p style="">We set up the training parameters needed by our algorithm:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>trainingParams = <span style="color:#cc7832;">new </span>TextClassifier.TrainingParameters(<br>    <span style="color:#6a8759;">numericalScalerTrainingParameters</span>: <span style="color:#cc7832;">null</span>,<br>    <span style="color:#6a8759;">featureSelectorTrainingParametersList</span>: [<span style="color:#cc7832;">new </span>ChisquareSelect.TrainingParameters()],<br>    <span style="color:#6a8759;">textExtractorParameters</span>: <span style="color:#cc7832;">new </span>NgramsExtractor.Parameters(),<br>    <span style="color:#6a8759;">modelerTrainingParameters</span>: <span style="color:#cc7832;">new </span>MultinomialNaiveBayes.TrainingParameters()<br>)<br></pre>
<p style="">We'll use a Na&iuml;ve Bayes model with Chisquare feature selection.</p>
<p style="">Next we create our algorithm, train it with our training dataset, and then validate it against the training dataset. We'd normally want to split the data into training and testing datasets, to give us a more accurate statistic of the accuracy of our model. But for simplicity, while still illustrating the API, we'll train and validate with our entire dataset:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>config = Configuration.<span style="color:#9876aa;font-style:italic;">configuration<br></span><span style="color:#cc7832;">def </span>classifier = MLBuilder.<span style="color:#9876aa;font-style:italic;">create</span>(trainingParams, config)<br>classifier.fit(datasets)<br><span style="color:#cc7832;">def </span>metrics = classifier.validate(datasets)<br>println <span style="color:#6a8759;">"Classifier Accuracy (using training data): </span>$metrics.accuracy<span style="color:#6a8759;">"<br></span></pre>
<p style="">When run, we see the following output:</p>
<pre style="line-height: 1.42857;">Classifier Accuracy (using training data): 0.9975609756097561
</pre>
<p>Our test dataset will consist of some hard-coded illustrative phrases. Let's use our model to predict the language for each phrase:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;">[   <span style="color:#6a8759;">'Bienvenido a Madrid'</span>, <span style="color:#6a8759;">'Bienvenue &agrave; Paris'</span>, <span style="color:#6a8759;">'Welcome to London'</span>,<br>    <span style="color:#6a8759;">'Willkommen in Berlin'</span>, <span style="color:#6a8759;">'Selamat Datang di Jakarta'<br></span>].each <span style="font-weight:bold;">{ </span>txt <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">    </span><span style="color:#cc7832;">def </span>r = classifier.predict(txt)<br>    <span style="color:#cc7832;">def </span>predicted = r.YPredicted<br>    <span style="color:#cc7832;">def </span>probability = sprintf <span style="color:#6a8759;">'%4.2f'</span>, r.YPredictedProbabilities.get(predicted)<br>    println <span style="color:#6a8759;">"Classifying: '</span>$txt<span style="color:#6a8759;">',  Predicted: </span>$predicted<span style="color:#6a8759;">,  Probability: </span>$probability<span style="color:#6a8759;">"<br></span><span style="font-weight:bold;">}<br></span></pre>
<p>When run, it has this output:</p>
<pre>Classifying: 'Bienvenido a Madrid',&nbsp; Predicted: Spanish,&nbsp; Probability: 0.83
Classifying: 'Bienvenue &agrave; Paris',&nbsp; Predicted: French,&nbsp; Probability: 0.71
Classifying: 'Welcome to London',&nbsp; Predicted: English,&nbsp; Probability: 1.00
Classifying: 'Willkommen in Berlin',&nbsp; Predicted: German,&nbsp; Probability: 0.84
Classifying: 'Selamat Datang di Jakarta',&nbsp; Predicted: Indonesian,&nbsp; Probability: 1.00
</pre>
<div>Given these phrases are very short, it is nice to get them all correct, and the probabilities all seem reasonable for this scenario.</div>
<h3>Parts of Speech</h3>
<p>Parts of speech (POS) analysers examine each part of a sentence (the words and potentially punctuation) in terms of the role they play in a sentence. A typical analyser will assign or annotate words with their role like identifying nouns, verbs, adjectives and so forth. This can be a key early step for tools like the voice assistants from Amazon, Apple and Google.</p>
<p> We'll start by looking at a perhaps lesser known library Nlp4j before looking at some others. In fact, there are multiple Nlp4j libraries. We'll use the one from <a href="https://nlp4j.org/" target="_blank">nlp4j.org</a>,&nbsp;which seems to be the most active and recently updated.</p>
<p>This library uses the <a href="https://stanfordnlp.github.io/CoreNLP/" target="_blank">Stanford CoreNLP</a> library under the covers for its English POS functionality. The library has the concept of documents, and annotators that work on documents. Once annotated, we can print out all of the discovered words and their annotations:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">var </span>doc = <span style="color:#cc7832;">new </span>DefaultDocument()<br>doc.putAttribute(<span style="color:#6a8759;">'text'</span>, <span style="color:#6a8759;">'I eat sushi with chopsticks.'</span>)<br><span style="color:#cc7832;">var </span>ann = <span style="color:#cc7832;">new </span>StanfordPosAnnotator()<br>ann.setProperty(<span style="color:#6a8759;">'target'</span>, <span style="color:#6a8759;">'text'</span>)<br>ann.annotate(doc)<br>println doc.<span style="color:#9876aa;">keywords</span>.collect<span style="font-weight:bold;">{  </span>k <span style="font-weight:bold;">-> </span><span style="color:#6a8759;">"</span>$<span style="font-weight:bold;">{</span>k.<span style="color:#9876aa;">facet </span>- <span style="color:#6a8759;">'word.'</span><span style="font-weight:bold;">}</span><span style="color:#6a8759;">(</span>$<span style="font-weight:bold;">{</span>k.<span style="color:#9876aa;">str</span><span style="font-weight:bold;">}</span><span style="color:#6a8759;">)" </span><span style="font-weight:bold;">}</span>.join(<span style="color:#6a8759;">' '</span>)<br></pre>
<p>When run, we see the following output:</p>
<pre>PRP(I) VBP(eat) NN(sushi) IN(with) NNS(chopsticks) .(.)</pre>
<p>The annotations, also known as tags or facets, for this example are as follows:</p>
<table style="border:1px solid gray; margin:5px;">
<tbody>
<tr style="color:#9876aa;">
<td style="padding:5px;">PRP</td>
<td style="padding:5px;">Personal pronoun</td>
</tr>
<tr>
<td style="padding:5px;">VBP</td>
<td style="padding:5px;">Present tense verb</td>
</tr>
<tr style="color:#9876aa;">
<td style="padding:5px;">NN</td>
<td style="padding:5px;">Noun, singular</td>
</tr>
<tr>
<td style="padding:5px;">IN</td>
<td style="padding:5px;">Preposition</td>
</tr>
<tr style="color:#9876aa;">
<td style="padding:5px;">NNS</td>
<td style="padding:5px;">Noun, plural</td>
</tr>
</tbody>
</table>
<p>The documentation for the libraries we are using give a more complete list of such annotations.</p>
<p>A nice aspect of this library is support for other languages, in particular, Japanese. The code is very similar but uses a different annotator:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;">doc = <span style="color:#cc7832;">new </span>DefaultDocument()<br>doc.putAttribute(<span style="color:#6a8759;">'text'</span>, <span style="color:#6a8759;">'</span><span style="color:#6a8759;font-family:'Courier New',monospace;">私は学校に行きました。</span><span style="color:#6a8759;">'</span>)<br>ann = <span style="color:#cc7832;">new </span>KuromojiAnnotator()<br>ann.setProperty(<span style="color:#6a8759;">'target'</span>, <span style="color:#6a8759;">'text'</span>)<br>ann.annotate(doc)<br>println doc.<span style="color:#9876aa;">keywords</span>.collect<span style="font-weight:bold;">{ </span>k <span style="font-weight:bold;">-> </span><span style="color:#6a8759;">"</span>$<span style="font-weight:bold;">{</span>k.<span style="color:#9876aa;">facet</span><span style="font-weight:bold;">}</span><span style="color:#6a8759;">(</span>$<span style="font-weight:bold;">{</span>k.<span style="color:#9876aa;">str</span><span style="font-weight:bold;">}</span><span style="color:#6a8759;">)" </span><span style="font-weight:bold;">}</span>.join(<span style="color:#6a8759;">' '</span>)<br></pre>
<p>When run, we see the following output:</p>
<pre>名詞(私) 助詞(は) 名詞(学校) 助詞(に) 動詞(行き) 助動詞(まし) 助動詞(た) 記号(。)</pre>
<p>Before progressing, we'll highlight the result visualization capabilities of the GroovyConsole. This feature lets us write a small Groovy script which converts results to any swing component. In our case we'll convert lists of annotated strings to a <code>JLabel</code> component containing HTML including colored annotation boxes. The details aren't included here but can be found in the <a href="https://github.com/paulk-asert/groovy-data-science/blob/master/subprojects/LanguageProcessing/src/main/resources/OutputTransforms.groovy" target="_blank">repo</a>. We need to copy that file into our <code>~/.groovy</code> folder and then enable script visualization as shown here:</p>
<p><img src="https://blogs.apache.org/groovy/mediaresource/dab9114e-95d6-4dd6-a294-76be3d2e3a47" style="width:80%;" alt="Screenshot from 2022-08-04 21-57-35.png"></p>
<p>Then we should see the following when running the script:</p>
<p><img src="https://blogs.apache.org/groovy/mediaresource/8ed6c774-f2a5-40d9-94ac-89ecbf56132d" style="width:100%;" alt="Screenshot from 2022-08-04 21-59-47.png"></p>
<p>The visualization is purely optional but adds a nice touch. If using Groovy in notebook environments like Jupyter/BeakerX, there might be visualization tools in those environments too.</p>
<p>Let's look at a larger example using the <a href="https://haifengl.github.io/" target="_blank">Smile</a> library.</p>
<p>First, the sentences that we'll examine:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>sentences = [<br>    <span style="color:#6a8759;">'Paul has two sisters, Maree and Christine.'</span>,<br>    <span style="color:#6a8759;">'No wise fish would go anywhere without a porpoise'</span>,<br>    <span style="color:#6a8759;">'His bark was much worse than his bite'</span>,<br>    <span style="color:#6a8759;">'Turn on the lights to the main bedroom'</span>,<br>    <span style="color:#6a8759;">"Light 'em all up"</span>,<br>    <span style="color:#6a8759;">'Make it dark downstairs'<br></span>]<br></pre>
<p>A couple of those sentences might seem a little strange but they are selected to show off quite a few of the different POS tags.</p>
<p>Smile has a tokenizer class which splits a sentence into words. It handles numerous cases like contractions and abbreviations ("e.g.", "'tis", "won't"). Smile also has a POS class based on the&nbsp;hidden Markov model and a built-in model is used for that class. Here is our code using those classes:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>tokenizer = <span style="color:#cc7832;">new </span>SimpleTokenizer(<span style="color:#cc7832;">true</span>)<br>sentences.each <span style="font-weight:bold;">{<br></span><span style="font-weight:bold;">    </span><span style="color:#cc7832;">def </span>tokens = Arrays.<span style="color:#9876aa;font-style:italic;">stream</span>(tokenizer.split(it)).toArray(String[]::<span style="color:#cc7832;">new</span>)<br>    <span style="color:#cc7832;">def </span>tags = HMMPOSTagger.<span style="color:#9876aa;font-style:italic;">default</span>.tag(tokens)*.toString()<br>    println tokens.<span style="color:#9876aa;">indices</span>.collect<span style="font-weight:bold;">{</span>tags[it] == tokens[it] ? tags[it] : <span style="color:#6a8759;">"</span>$<span style="font-weight:bold;">{</span>tags[it]<span style="font-weight:bold;">}</span><span style="color:#6a8759;">(</span>$<span style="font-weight:bold;">{</span>tokens[it]<span style="font-weight:bold;">}</span><span style="color:#6a8759;">)" </span><span style="font-weight:bold;">}</span>.join(<span style="color:#6a8759;">' '</span>)<br><span style="font-weight:bold;">}<br></span></pre>
<p>We run the tokenizer for each sentence. Each token is then displayed directly or with its tag if it has one.</p>
<p>Running the script gives this visualization:</p></p>
<table style="background-color: white; margin: 5px; border: 1px solid gray">
<tbody>
<tr>
<td style="padding: 5px;">
<table>
<tbody>
<tr>
<td style="padding: 5px; text-align: center; ">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Paul</span><br><br />
        <span style="color:white;">NNP</span></div>
</td>
<td style="padding: 5px; text-align: center;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">has</span><br><br />
        <span style="color:white;">VBZ</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#DF401C;">
        <span style="background-color:white; color:#DF401C;">two</span><br><br />
        <span style="color:white;">CD</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">sisters</span><br><br />
        <span style="color:white;">NNS</span></div>
</td>
<td style="text-align: center; padding: 5px;">, </td>
<td style="padding: 5px;">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Maree</span><br><br />
        <span style="color:white;">NNP</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#C54AA8;">
        <span style="background-color:white; color:#C54AA8;">and</span><br><br />
        <span style="color:white;">CC</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Christine</span><br><br />
        <span style="color:white;">NNP</span></div>
</td>
<td style="text-align: center; padding: 5px;">.</td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#895C9F;">
        <span style="background-color:white; color:#895C9F;">No</span><br><br />
        <span style="color:white;">DT</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6AA4;">
        <span style="background-color:white; color:#5B6AA4;">wise</span><br><br />
        <span style="color:white;">JJ</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6633;">
        <span style="background-color:white; color:#5B6633;">fish</span><br><br />
        <span style="color:white;">NN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#FC5F00;">
        <span style="background-color:white; color:#FC5F00;">would</span><br><br />
        <span style="color:white;">MD</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#561B06;">
        <span style="background-color:white; color:#561B06;">go</span><br><br />
        <span style="color:white;">VB</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#32CD32;">
        <span style="background-color:white; color:#32CD32;">anywhere</span><br><br />
        <span style="color:white;">RB</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0000CD;">
        <span style="background-color:white; color:#0000CD;">without</span><br><br />
        <span style="color:white;">IN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#895C9F;">
        <span style="background-color:white; color:#895C9F;">a</span><br><br />
        <span style="color:white;">DT</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6633;">
        <span style="background-color:white; color:#5B6633;">porpoise</span><br><br />
        <span style="color:white;">NN</span></div>
</td>
<td style="text-align: center; padding: 5px;"></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#CD853F;">
        <span style="background-color:white; color:#CD853F;">His</span><br><br />
        <span style="color:white;">PRP$</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6633;">
        <span style="background-color:white; color:#5B6633;">bark</span><br><br />
        <span style="color:white;">NN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#8B4513;">
        <span style="background-color:white; color:#8B4513;">was</span><br><br />
        <span style="color:white;">VBD</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#32CD32;">
        <span style="background-color:white; color:#32CD32;">much</span><br><br />
        <span style="color:white;">RB</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#57411B;">
        <span style="background-color:white; color:#57411B;">worse</span><br><br />
        <span style="color:white;">JJR</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0000CD;">
        <span style="background-color:white; color:#0000CD;">than</span><br><br />
        <span style="color:white;">IN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#CD853F;">
        <span style="background-color:white; color:#CD853F;">his</span><br><br />
        <span style="color:white;">PRP$</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6633;">
        <span style="background-color:white; color:#5B6633;">bite</span><br><br />
        <span style="color:white;">NN</span></div>
</td>
<td style="text-align: center; padding: 5px;"></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#561B06;">
        <span style="background-color:white; color:#561B06;">Turn</span><br><br />
        <span style="color:white;">VB</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0000CD;">
        <span style="background-color:white; color:#0000CD;">on</span><br><br />
        <span style="color:white;">IN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#895C9F;">
        <span style="background-color:white; color:#895C9F;">the</span><br><br />
        <span style="color:white;">DT</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">lights</span><br><br />
        <span style="color:white;">NNS</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">to</span><br><br />
        <span style="color:white;">TO</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#895C9F;">
        <span style="background-color:white; color:#895C9F;">the</span><br><br />
        <span style="color:white;">DT</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6AA4;">
        <span style="background-color:white; color:#5B6AA4;">main</span><br><br />
        <span style="color:white;">JJ</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6633;">
        <span style="background-color:white; color:#5B6633;">bedroom</span><br><br />
        <span style="color:white;">NN</span></div>
</td>
<td style="text-align: center; padding: 5px;"></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Light</span><br><br />
        <span style="color:white;">NNP</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">'em</span><br><br />
        <span style="color:white;">PRP</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#32CD32;">
        <span style="background-color:white; color:#32CD32;">all</span><br><br />
        <span style="color:white;">RB</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#32CD32;">
        <span style="background-color:white; color:#32CD32;">up</span><br><br />
        <span style="color:white;">RB</span></div>
</td>
<td style="text-align: center; padding: 5px;"></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#561B06;">
        <span style="background-color:white; color:#561B06;">Make</span><br><br />
        <span style="color:white;">VB</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">it</span><br><br />
        <span style="color:white;">PRP</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6AA4;">
        <span style="background-color:white; color:#5B6AA4;">dark</span><br><br />
        <span style="color:white;">JJ</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6633;">
        <span style="background-color:white; color:#5B6633;">downstairs</span><br><br />
        <span style="color:white;">NN</span></div>
</td>
<td style="text-align: center; padding: 5px;"></td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>[Note: the scripts in the repo just print to stdout which is perfect when using the command-line or IDEs. The visualization in the GoovyConsole kicks in only for the actual result. So, if you are following along at home and wanting to use the GroovyConsole, you'd change the <code>each</code> to <code>collect</code> and remove the <code>println</code>, and you should be good for visualization.]</p>
<p>The OpenNLP code is very similar:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>tokenizer = SimpleTokenizer.<span style="color:#9876aa;font-style:italic;">INSTANCE<br></span>sentences.each <span style="font-weight:bold;">{<br></span><span style="font-weight:bold;">    </span>String[] tokens = tokenizer.tokenize(it)<br>    <span style="color:#cc7832;">def </span>posTagger = <span style="color:#cc7832;">new </span>POSTaggerME(<span style="color:#6a8759;">'en'</span>)<br>    String[] tags = posTagger.tag(tokens)<br>    println tokens.<span style="color:#9876aa;">indices</span>.collect<span style="font-weight:bold;">{</span>tags[it] == tokens[it] ? tags[it] : <span style="color:#6a8759;">"</span>$<span style="font-weight:bold;">{</span>tags[it]<span style="font-weight:bold;">}</span><span style="color:#6a8759;">(</span>$<span style="font-weight:bold;">{</span>tokens[it]<span style="font-weight:bold;">}</span><span style="color:#6a8759;">)" </span><span style="font-weight:bold;">}</span>.join(<span style="color:#6a8759;">' '</span>)<br><span style="font-weight:bold;">}<br></span></pre>
<p>OpenNLP allows you to supply your own POS model but downloads a default one if none is specified.</p>
<p>When the script is run, it has this visualization:</p></p>
<table style="background-color: white; margin:5px; border: 1px solid gray;">
<tbody>
<tr>
<td style="padding: 5px;">
<table>
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Paul</span><br><br />
        <span style="color:white;">PROPN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">has</span><br><br />
        <span style="color:white;">VERB</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#DF401C;">
        <span style="background-color:white; color:#DF401C;">two</span><br><br />
        <span style="color:white;">NUM</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">sisters</span><br><br />
        <span style="color:white;">NOUN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#C54AA8;">
        <span style="background-color:white; color:#C54AA8;">,</span><br><br />
        <span style="color:white;">PUNCT</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Maree</span><br><br />
        <span style="color:white;">PROPN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#895C9F;">
        <span style="background-color:white; color:#895C9F;">and</span><br><br />
        <span style="color:white;">CCONJ</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Christine</span><br><br />
        <span style="color:white;">PROPN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#C54AA8;">
        <span style="background-color:white; color:#C54AA8;">.</span><br><br />
        <span style="color:white;">PUNCT</span></div>
</td>
<td style="text-align: center; padding: 5px;"></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6AA4;">
        <span style="background-color:white; color:#5B6AA4;">No</span><br><br />
        <span style="color:white;">DET</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6633;">
        <span style="background-color:white; color:#5B6633;">wise</span><br><br />
        <span style="color:white;">ADJ</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">fish</span><br><br />
        <span style="color:white;">NOUN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#FC5F00;">
        <span style="background-color:white; color:#FC5F00;">would</span><br><br />
        <span style="color:white;">AUX</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">go</span><br><br />
        <span style="color:white;">VERB</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#561B06;">
        <span style="background-color:white; color:#561B06;">anywhere</span><br><br />
        <span style="color:white;">ADV</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#32CD32;">
        <span style="background-color:white; color:#32CD32;">without</span><br><br />
        <span style="color:white;">ADP</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6AA4;">
        <span style="background-color:white; color:#5B6AA4;">a</span><br><br />
        <span style="color:white;">DET</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">porpoise</span><br><br />
        <span style="color:white;">NOUN</span></div>
</td>
<td style="text-align: center; padding: 5px;"></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0000CD;">
        <span style="background-color:white; color:#0000CD;">His</span><br><br />
        <span style="color:white;">PRON</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">bark</span><br><br />
        <span style="color:white;">NOUN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#FC5F00;">
        <span style="background-color:white; color:#FC5F00;">was</span><br><br />
        <span style="color:white;">AUX</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#561B06;">
        <span style="background-color:white; color:#561B06;">much</span><br><br />
        <span style="color:white;">ADV</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6633;">
        <span style="background-color:white; color:#5B6633;">worse</span><br><br />
        <span style="color:white;">ADJ</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#32CD32;">
        <span style="background-color:white; color:#32CD32;">than</span><br><br />
        <span style="color:white;">ADP</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0000CD;">
        <span style="background-color:white; color:#0000CD;">his</span><br><br />
        <span style="color:white;">PRON</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">bite</span><br><br />
        <span style="color:white;">NOUN</span></div>
</td>
<td style="text-align: center; padding: 5px;"></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">Turn</span><br><br />
        <span style="color:white;">VERB</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#32CD32;">
        <span style="background-color:white; color:#32CD32;">on</span><br><br />
        <span style="color:white;">ADP</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6AA4;">
        <span style="background-color:white; color:#5B6AA4;">the</span><br><br />
        <span style="color:white;">DET</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">lights</span><br><br />
        <span style="color:white;">NOUN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#32CD32;">
        <span style="background-color:white; color:#32CD32;">to</span><br><br />
        <span style="color:white;">ADP</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6AA4;">
        <span style="background-color:white; color:#5B6AA4;">the</span><br><br />
        <span style="color:white;">DET</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6633;">
        <span style="background-color:white; color:#5B6633;">main</span><br><br />
        <span style="color:white;">ADJ</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">bedroom</span><br><br />
        <span style="color:white;">NOUN</span></div>
</td>
<td style="text-align: center; padding: 5px;"></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">Light</span><br><br />
        <span style="color:white;">NOUN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#C54AA8;">
        <span style="background-color:white; color:#C54AA8;">'</span><br><br />
        <span style="color:white;">PUNCT</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">em</span><br><br />
        <span style="color:white;">NOUN</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#561B06;">
        <span style="background-color:white; color:#561B06;">all</span><br><br />
        <span style="color:white;">ADV</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#32CD32;">
        <span style="background-color:white; color:#32CD32;">up</span><br><br />
        <span style="color:white;">ADP</span></div>
</td>
<td style="text-align: center; padding: 5px;"></td>
</tr>
</tbody>
</table>
<table>
<tbody>
<tr>
<td style="padding: 5px;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">Make</span><br><br />
        <span style="color:white;">VERB</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0000CD;">
        <span style="background-color:white; color:#0000CD;">it</span><br><br />
        <span style="color:white;">PRON</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#5B6633;">
        <span style="background-color:white; color:#5B6633;">dark</span><br><br />
        <span style="color:white;">ADJ</span></div>
</td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">downstairs</span><br><br />
        <span style="color:white;">NOUN</span></div>
</td>
<td style="text-align: center; padding: 5px;"></td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>The observant reader may have noticed some slight differences in the tags used in this library. They are essentially the same but using slightly different names. This is something to be aware of when swapping between POS libraries or models. Make sure you look up the documentation for the library/model you are using to understand the available tag types.</p>
<h3>Entity Detection</h3>
<p>Named entity recognition (NER), seeks to identity and classify named entities in text. Categories of interest might be persons, organizations, locations dates, etc. It is another technology used in many fields of NLP.</p>
<p>We'll start with our sentences to analyse:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;">String[] sentences = [<br>    <span style="color:#6a8759;">"A commit by Daniel Sun on December 6, 2020 improved Groovy 4's language integrated query."</span>,<br>    <span style="color:#6a8759;">"A commit by Daniel on Sun., December 6, 2020 improved Groovy 4's language integrated query."</span>,<br>    <span style="color:#6a8759;">'The Groovy in Action book by Dierk Koenig et. al. is a bargain at $50, or indeed any price.'</span>,<br>    <span style="color:#6a8759;">'The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark.'</span>,<br>    <span style="color:#6a8759;">'I saw Ms. May Smith waving to June Jones.'</span>,<br>    <span style="color:#6a8759;">'The parcel was passed from May to June.'</span>,<br>    <span style="color:#6a8759;">'The Mona Lisa by Leonardo da Vinci has been on display in the Louvre, Paris since 1797.'<br></span>]<br></pre>
<p>We'll use some well-known models, we'll focus on the&nbsp;<i>person</i>, <i>money</i>, <i>date</i>, <i>time</i>, and <i>location</i>&nbsp;models:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>base = <span style="color:#6a8759;">'http://opennlp.sourceforge.net/models-1.5'<br></span><span style="color:#cc7832;">def </span>modelNames = [<span style="color:#6a8759;">'person'</span>, <span style="color:#6a8759;">'money'</span>, <span style="color:#6a8759;">'date'</span>, <span style="color:#6a8759;">'time'</span>, <span style="color:#6a8759;">'location'</span>]<br><span style="color:#cc7832;">def </span>finders = modelNames.collect <span style="font-weight:bold;">{ </span>model <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">    </span><span style="color:#cc7832;">new </span>NameFinderME(DownloadUtil.<span style="color:#9876aa;font-style:italic;">downloadModel</span>(<span style="color:#cc7832;">new </span>URL(<span style="color:#6a8759;">"</span>$base<span style="color:#6a8759;">/en-ner-</span>$<span style="font-weight:bold;">{</span>model<span style="font-weight:bold;">}</span><span style="color:#6a8759;">.bin"</span>), TokenNameFinderModel))<br><span style="font-weight:bold;">}<br></span></pre>
<p>We'll now tokenize our sentences:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>tokenizer = SimpleTokenizer.<span style="color:#9876aa;font-style:italic;">INSTANCE<br></span>sentences.each <span style="font-weight:bold;">{ </span>sentence <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">    </span>String[] tokens = tokenizer.tokenize(sentence)<br>    Span[] tokenSpans = tokenizer.tokenizePos(sentence)<br>    <span style="color:#cc7832;">def </span>entityText = [:]<br>    <span style="color:#cc7832;">def </span>entityPos = [:]<br>    finders.<span style="color:#9876aa;">indices</span>.each <span style="font-weight:bold;">{</span>fi <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">        </span><span style="color:#808080;">// could be made smarter by looking at probabilities and overlapping spans<br></span><span style="color:#808080;">        </span>Span[] spans = finders[fi].find(tokens)<br>        spans.each<span style="font-weight:bold;">{</span>span <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">            </span><span style="color:#cc7832;">def </span>se = span.<span style="color:#9876aa;">start</span>..<span.<span style="color:#9876aa;">end<br></span><span style="color:#9876aa;">            </span><span style="color:#cc7832;">def </span>pos = (tokenSpans[se.<span style="color:#9876aa;">from</span>].<span style="color:#9876aa;">start</span>)..<(tokenSpans[se.<span style="color:#9876aa;">to</span>].<span style="color:#9876aa;">end</span>)<br>            entityPos[span.<span style="color:#9876aa;">start</span>] = pos<br>            entityText[span.<span style="color:#9876aa;">start</span>] = <span style="color:#6a8759;">"</span>$span.<span style="color:#9876aa;">type</span><span style="color:#6a8759;">(</span>$<span style="font-weight:bold;">{</span>sentence[pos]<span style="font-weight:bold;">}</span><span style="color:#6a8759;">)"<br></span><span style="color:#6a8759;">        </span><span style="font-weight:bold;">}<br></span><span style="font-weight:bold;">    }<br></span><span style="font-weight:bold;">    </span>entityPos.keySet().sort().reverseEach <span style="font-weight:bold;">{<br></span><span style="font-weight:bold;">        </span><span style="color:#cc7832;">def </span>pos = entityPos[it]<br>        <span style="color:#cc7832;">def </span>(from, to) = [pos.from, pos.to + <span style="color:#6897bb;">1</span>]<br>        sentence = sentence[<span style="color:#6897bb;">0</span>..<from] + entityText[it] + sentence[to..-<span style="color:#6897bb;">1</span>]<br>    <span style="font-weight:bold;">}<br></span><span style="font-weight:bold;">    </span>println sentence<br><span style="font-weight:bold;">}<br></span></pre>
<p>And when visualized, shows this:</p></p>
<table style="border:1px solid grey; margin:5px; background-color:white">
<tbody>
<tr>
<td>
<table style="margin:5px;">
<tbody>
<tr>
<td style="padding:5px;">A commit by </td>
<td style="text-align:center;">
<div style="padding:5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Daniel Sun</span><br><br />
        <span style="color:white;">person</span></div>
</td>
<td style="text-align: center; padding:5px;">on </td>
<td style="text-align:center;">
<div style="padding:5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">December 6, 2020</span><br><br />
        <span style="color:white;">date</span></div>
</td>
<td style="text-align: center; padding:5px;">improved Groovy 4's language integrated query.</td>
</tr>
</tbody>
</table>
<table style="margin:5px;">
<tbody>
<tr>
<td style="text-align: center; padding:5px;">A commit by </td>
<td style="text-align: center;">
<div style="padding:5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Daniel</span><br><br />
        <span style="color:white;">person</span></div>
</td>
<td style="text-align:center; padding:5px;">on Sun., </td>
<td style="text-align:center;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">December 6, 2020</span><br><br />
        <span style="color:white;">date</span></div>
</td>
<td style="text-align: center; padding:5px;">improved Groovy 4's language integrated query.</td>
</tr>
</tbody>
</table>
<table style="margin:5px;">
<tbody>
<tr>
<td style="text-align: center; padding:5px;">The Groovy in Action book by </td>
<td style="text-align: center;">
<div style="padding:5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Dierk Koenig</span><br><br />
        <span style="color:white;">person</span></div>
</td>
<td style="text-align: center; padding:5px;">et. al. is a bargain at </td>
<td style="text-align:center;">
<div style="padding:5px; background-color:#DF401C;">
        <span style="background-color:white; color:#DF401C;">$50</span><br><br />
        <span style="color:white;">money</span></div>
</td>
<td style="text-align: center; padding:5px;">, or indeed any price.</td>
</tr>
</tbody>
</table>
<table style="margin:5px;">
<tbody>
<tr>
<td style="text-align: center; padding:5px;">The conference wrapped up </td>
<td style="text-align: center;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">yesterday</span><br><br />
        <span style="color:white;">date</span></div>
</td>
<td style="text-align: center; padding:5px;">at </td>
<td style="text-align:center;">
<div style="padding:5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">5:30 p.m.</span><br><br />
        <span style="color:white;">time</span></div>
</td>
<td style="text-align: center; padding:5px;">in </td>
<td style="text-align: center;">
<div style="padding:5px; background-color:#C54AA8;">
        <span style="background-color:white; color:#C54AA8;">Copenhagen</span><br><br />
        <span style="color:white;">location</span></div>
</td>
<td style="padding:5px;">, </td>
<td style="text-align:center;">
<div style="padding: 5px; background-color:#C54AA8;">
        <span style="background-color:white; color:#C54AA8;">Denmark</span><br><br />
        <span style="color:white;">location</span></div>
</td>
<td style="padding:5px;">.</td>
</tr>
</tbody>
</table>
<table style="margin:5px;">
<tbody>
<tr>
<td style="padding:5px;">I saw Ms. </td>
<td style="text-align:center;">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">May Smith</span><br><br />
        <span style="color:white;">person</span></div>
</td>
<td style="text-align: center; padding:5px;">waving to </td>
<td style="text-align:center;">
<div style="padding:5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">June Jones</span><br><br />
        <span style="color:white;">person</span></div>
</td>
<td style="text-align: center; padding:5px;">.</td>
</tr>
</tbody>
</table>
<table style="margin:5px;">
<tbody>
<tr>
<td style="padding:5px;">The parcel was passed from </td>
<td style="text-align:center;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">May to June</span><br><br />
        <span style="color:white;">date</span></div>
</td>
<td style="padding:5px;">.</td>
</tr>
</tbody>
</table>
<table style="margin:5px;">
<tbody>
<tr>
<td style="padding:5px;">The Mona Lisa by </td>
<td style="text-align:center;">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Leonardo da Vinci</span><br><br />
        <span style="color:white;">person</span></div>
</td>
<td style="padding:5px;">has been on display in the Louvre, </td>
<td style="text-align:center;">
<div style="padding:5px; background-color:#C54AA8;">
        <span style="background-color:white; color:#C54AA8;">Paris</span><br><br />
        <span style="color:white;">location</span></div>
</td>
<td style="text-align:center; padding:5px;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">since 1797</span><br><br />
        <span style="color:white;">date</span></div>
</td>
<td>.</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>We can see here that most examples have been categorized as we might expect. We'd have to improve our model for it to do a better job on the "May to June" example.</p>
<h3>Scaling Entity Detection</h3>
<p>We can also run our named entity detection algorithms on platforms like <a href="http://nlp.johnsnowlabs.com/" target="_blank">Spark NLP</a> which adds NLP functionality to <a href="https://spark.apache.org/" target="_blank">Apache Spark</a>. We'll use <a href="https://nlp.johnsnowlabs.com/2020/01/22/glove_100d.html" target="_blank">glove_100d</a> embeddings and the <a href="https://nlp.johnsnowlabs.com/2020/02/03/onto_100_en.html" target="_blank">onto_100</a> NER model.</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">var </span>assembler = <span style="color:#cc7832;">new </span>DocumentAssembler(<span style="color:#6a8759;">inputCol</span>: <span style="color:#6a8759;">'text'</span>, <span style="color:#6a8759;">outputCol</span>: <span style="color:#6a8759;">'document'</span>, <span style="color:#6a8759;">cleanupMode</span>: <span style="color:#6a8759;">'disabled'</span>)<br><br><span style="color:#cc7832;">var </span>tokenizer = <span style="color:#cc7832;">new </span>Tokenizer(<span style="color:#6a8759;">inputCols</span>: [<span style="color:#6a8759;">'document'</span>] <span style="color:#cc7832;">as </span>String[], <span style="color:#6a8759;">outputCol</span>: <span style="color:#6a8759;">'token'</span>)<br><br><span style="color:#cc7832;">var </span>embeddings = WordEmbeddingsModel.<span style="color:#9876aa;font-style:italic;">pretrained</span>(<span style="color:#6a8759;">'glove_100d'</span>).tap <span style="font-weight:bold;">{<br></span><span style="font-weight:bold;">    </span><span style="color:#9876aa;">inputCols </span>= [<span style="color:#6a8759;">'document'</span>, <span style="color:#6a8759;">'token'</span>] <span style="color:#cc7832;">as </span>String[]<br>    <span style="color:#9876aa;">outputCol </span>= <span style="color:#6a8759;">'embeddings'<br></span><span style="font-weight:bold;">}<br></span><span style="font-weight:bold;"><br></span><span style="color:#cc7832;">var </span>model = NerDLModel.<span style="color:#9876aa;font-style:italic;">pretrained</span>(<span style="color:#6a8759;">'onto_100'</span>, <span style="color:#6a8759;">'en'</span>).tap <span style="font-weight:bold;">{<br></span><span style="font-weight:bold;">    </span><span style="color:#9876aa;">inputCols </span>= [<span style="color:#6a8759;">'document'</span>, <span style="color:#6a8759;">'token'</span>, <span style="color:#6a8759;">'embeddings'</span>] <span style="color:#cc7832;">as </span>String[]<br>    <span style="color:#9876aa;">outputCol </span>=<span style="color:#6a8759;">'ner'<br></span><span style="font-weight:bold;">}<br></span><span style="font-weight:bold;"><br></span><span style="color:#cc7832;">var </span>converter = <span style="color:#cc7832;">new </span>NerConverter(<span style="color:#6a8759;">inputCols</span>: [<span style="color:#6a8759;">'document'</span>, <span style="color:#6a8759;">'token'</span>, <span style="color:#6a8759;">'ner'</span>] <span style="color:#cc7832;">as </span>String[], <span style="color:#6a8759;">outputCol</span>: <span style="color:#6a8759;">'ner_chunk'</span>)<br><br><span style="color:#cc7832;">var </span>pipeline = <span style="color:#cc7832;">new </span>Pipeline(<span style="color:#6a8759;">stages</span>: [assembler, tokenizer, embeddings, model, converter] <span style="color:#cc7832;">as </span>PipelineStage[])<br><br><span style="color:#cc7832;">var </span>spark = SparkNLP.<span style="color:#9876aa;font-style:italic;">start</span>(<span style="color:#cc7832;">false</span>, <span style="color:#cc7832;">false</span>, <span style="color:#6a8759;">'16G'</span>, <span style="color:#6a8759;">''</span>, <span style="color:#6a8759;">''</span>, <span style="color:#6a8759;">''</span>)<br><br><span style="color:#cc7832;">var </span>text = [<br>    <span style="color:#6a8759;">"The Mona Lisa is a 16th century oil painting created by Leonardo. It's held at the Louvre in Paris."<br></span>]<br><span style="color:#cc7832;">var </span>data = spark.createDataset(text, Encoders.<span style="color:#9876aa;font-style:italic;">STRING</span>()).toDF(<span style="color:#6a8759;">'text'</span>)<br><br><span style="color:#cc7832;">var </span>pipelineModel = pipeline.fit(data)<br><br><span style="color:#cc7832;">var </span>transformed = pipelineModel.transform(data)<br>transformed.show()<br><br>use(SparkCategory) <span style="font-weight:bold;">{<br></span><span style="font-weight:bold;">    </span>transformed.collectAsList().each <span style="font-weight:bold;">{ </span>row <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">        </span><span style="color:#cc7832;">def </span>res =  row.text<br>        <span style="color:#cc7832;">def </span>chunks = row.ner_chunk.reverseIterator()<br>        <span style="color:#cc7832;">while </span>(chunks.hasNext()) {<br>            <span style="color:#cc7832;">def </span>chunk = chunks.next()<br>            <span style="color:#cc7832;">int </span>begin = chunk.begin<br>            <span style="color:#cc7832;">int </span>end = chunk.end<br>            <span style="color:#cc7832;">def </span>entity = chunk.metadata.get(<span style="color:#6a8759;">'entity'</span>).get()<br>            res = res[<span style="color:#6897bb;">0</span>..<begin] + <span style="color:#6a8759;">"</span>$entity<span style="color:#6a8759;">(</span>$chunk.result<span style="color:#6a8759;">)" </span>+ res[end<..-<span style="color:#6897bb;">1</span>]<br>        }<br>        println res<br>    <span style="font-weight:bold;">}<br></span><span style="font-weight:bold;">}<br></span></pre>
<p>We won't go into all of the details here. In summary, the code sets up a pipeline that transforms our input sentences, via a series of steps, into chunks, where each chunk corresponds to a detected entity. Each chunk has a start and ending position, and an associated tag type.</p>
<p>This may not seem like it is much different to our earlier examples, but if we had large volumes of data and we were running in a large cluster, the work could be spread across worker nodes within the cluster.</p>
<p>Here we have used a utility <code>SparkCategory</code> class which makes accessing the information in Spark <code>Row</code> instances a little nicer in terms of Groovy shorthand syntax. We can use <code>row.text</code> instead of <code>row.get(row.fieldIndex('text'))</code>. Here is the code for this utility class:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">class </span>SparkCategory {<br>    <span style="color:#cc7832;">static </span>get(Row r, String field) { r.get(r.fieldIndex(field)) }<br>}<br></pre>
<p>If doing more than this simple example, the use of <code>SparkCategory</code> could be made implicit through various standard Groovy techniques.</p>
<p>When we run our script, we see the following output:</p>
<pre>22/08/07 12:31:39 INFO SparkContext: Running Spark version 3.3.0
...
glove_100d download started this may take some time.
Approximate size to download 145.3 MB
...
onto_100 download started this may take some time.
Approximate size to download 13.5 MB
...
+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
|                text|            document|               token|          embeddings|                 ner|           ner_chunk|
+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
|The Mona Lisa is ...|[{document, 0, 98...|[{token, 0, 2, Th...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 0, 12, T...|
+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
PERSON(The Mona Lisa) is a DATE(16th century) oil painting created by PERSON(Leonardo). It's held at the FAC(Louvre) in GPE(Paris).
</pre>
<p>The result has the following visualization:</p></p>
<table style="border:1px solid grey; margin:5px; background-color:white;">
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<table style="margin:5px;">
<tbody>
<tr>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">The Mona Lisa</span><br><br />
        <span style="color:white;">PERSON</span></div>
</td>
<td style="text-align: center; padding: 5px;">is a </td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#2B5F19;">
        <span style="background-color:white; color:#2B5F19;">16th century</span><br><br />
        <span style="color:white;">DATE</span></div>
</td>
<td style="text-align: center; padding: 5px;">oil painting created by </td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#0088FF;">
        <span style="background-color:white; color:#0088FF;">Leonardo</span><br><br />
        <span style="color:white;">PERSON</span></div>
</td>
<td style="text-align: center; padding: 5px;">. It's held at the </td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#DF401C;">
        <span style="background-color:white; color:#DF401C;">Louvre</span><br><br />
        <span style="color:white;">FAC</span></div>
</td>
<td style="text-align: center; padding: 5px;">in </td>
<td style="text-align: center; padding: 5px;">
<div style="padding: 5px; background-color:#A4772B;">
        <span style="background-color:white; color:#A4772B;">Paris</span><br><br />
        <span style="color:white;">GPE</span></div>
</td>
<td style="text-align: center; padding: 5px;">.</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>Here FAC is facility (buildings, airports, highways, bridges, etc.) and GPE is Geo-Political Entity (countries, cities, states, etc.).</p>
<h3>Sentence Detection</h3>
<p>Detecting sentences in text might seem a simple concept at first but there are numerous special cases.</p>
<p>Consider the following text:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>text = <span style="color:#6a8759;">'''<br></span><span style="color:#6a8759;">The most referenced scientific paper of all time is "Protein measurement with the<br></span><span style="color:#6a8759;">Folin phenol reagent" by Lowry, O. H., Rosebrough, N. J., Farr, A. L. &amp; Randall,<br></span><span style="color:#6a8759;">R. J. and was published in the J. BioChem. in 1951. It describes a method for<br></span><span style="color:#6a8759;">measuring the amount of protein (even as small as 0.2 &gamma;, were &gamma; is the specific<br></span><span style="color:#6a8759;">weight) in solutions and has been cited over 300,000 times and can be found here:<br></span><span style="color:#6a8759;">https://www.jbc.org/content/193/1/265.full.pdf. Dr. Lowry completed<br></span><span style="color:#6a8759;">two doctoral degrees under an M.D.-Ph.D. program from the University of Chicago<br></span><span style="color:#6a8759;">before moving to Harvard under A. Baird Hastings. He was also the H.O.D of<br></span><span style="color:#6a8759;">Pharmacology at Washington University in St. Louis for 29 years.<br></span><span style="color:#6a8759;">'''<br></span></pre>
<p>There are full stops at the end of each sentence (though in general, it could also be other punctuation like exclamation marks and question marks). There are also full stops and decimal points in abbreviations, URLs, decimal numbers and so forth. Sentence detection algorithms might have some special hard-coded cases, like "Dr.", "Ms.", or in an emoticon, and may also use some heuristics. In general, they might also be trained with examples like above.</p>
<p>Here is some code for OpenNLP for detecting sentences in the above:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>helper = <span style="color:#cc7832;">new </span>ResourceHelper(<span style="color:#6a8759;">'http://opennlp.sourceforge.net/models-1.5'</span>)<br><span style="color:#cc7832;">def </span>model = <span style="color:#cc7832;">new </span>SentenceModel(helper.load(<span style="color:#6a8759;">'en-sent'</span>))<br><span style="color:#cc7832;">def </span>detector = <span style="color:#cc7832;">new </span>SentenceDetectorME(model)<br><span style="color:#cc7832;">def </span>sentences = detector.sentDetect(text)<br><span style="color:#cc7832;">assert </span>text.count(<span style="color:#6a8759;">'.'</span>) == <span style="color:#6897bb;">28<br></span><span style="color:#cc7832;">assert </span>sentences.size() == <span style="color:#6897bb;">4<br></span>println <span style="color:#6a8759;">"Found </span>$<span style="font-weight:bold;">{</span>sentences.size()<span style="font-weight:bold;">}</span><span style="color:#6a8759;"> sentences:</span><span style="color:#cc7832;">\n</span><span style="color:#6a8759;">" </span>+ sentences.join(<span style="color:#6a8759;">'</span><span style="color:#cc7832;">\n\n</span><span style="color:#6a8759;">'</span>)<br></pre>
<p>It has the following output:</p>
<pre><span style="color:#D02020;">Downloading en-sent</span>
Found 4 sentences:
The most referenced scientific paper of all time is "Protein measurement with the
Folin phenol reagent" by Lowry, O. H., Rosebrough, N. J., Farr, A. L. &amp; Randall,
R. J. and was published in the J. BioChem. in 1951.

It describes a method for
measuring the amount of protein (even as small as 0.2 &gamma;, were &gamma; is the specific
weight) in solutions and has been cited over 300,000 times and can be found here:
https://www.jbc.org/content/193/1/265.full.pdf.

Dr. Lowry completed
two doctoral degrees under an M.D.-Ph.D. program from the University of Chicago
before moving to Harvard under A. Baird Hastings.

He was also the H.O.D of
Pharmacology at Washington University in St. Louis for 29 years.</pre>
<p>We can see here, it handled all of the tricky cases in the example.</p>
<h3>Relationship Extraction with Triples</h3>
<p>The next step after detecting named entities and the various parts of speech of certain words is to explore relationships between them. This is often done in the form of <i>subject-predicate-object</i> triplets. In our earlier NER example, for the sentence "<span style="background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 13px;">The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark.</span>", we found various date, time and location named entities.</p>
<p>We can extract triples using the <a href="https://github.com/uma-pi1/minie" target="_blank">MinIE library</a> (which in turns uses the Standford CoreNLP library) with the following code:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>parser = CoreNLPUtils.<span style="color:#9876aa;font-style:italic;">StanfordDepNNParser</span>()<br>sentences.each <span style="font-weight:bold;">{ </span>sentence <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">    </span><span style="color:#cc7832;">def </span>minie = <span style="color:#cc7832;">new </span>MinIE(sentence, parser, MinIE.Mode.<span style="color:#9876aa;font-style:italic;">SAFE</span>)<br><br>    println <span style="color:#6a8759;">"</span><span style="color:#cc7832;">\n</span><span style="color:#6a8759;">Input sentence: </span>$sentence<span style="color:#6a8759;">"<br></span><span style="color:#6a8759;">    </span>println <span style="color:#6a8759;">'============================='<br></span><span style="color:#6a8759;">    </span>println <span style="color:#6a8759;">'Extractions:'<br></span><span style="color:#6a8759;">    </span><span style="color:#cc7832;">for </span>(ap <span style="color:#cc7832;">in </span>minie.<span style="color:#9876aa;">propositions</span>) {<br>        println <span style="color:#6a8759;">"</span><span style="color:#cc7832;">\t</span><span style="color:#6a8759;">Triple: </span>$ap.<span style="color:#9876aa;">tripleAsString</span><span style="color:#6a8759;">"<br></span><span style="color:#6a8759;">        </span><span style="color:#cc7832;">def </span>attr = ap.<span style="color:#9876aa;">attribution</span>.<span style="color:#9876aa;">attributionPhrase </span>? ap.<span style="color:#9876aa;">attribution</span>.toStringCompact() : <span style="color:#6a8759;">'NONE'<br></span><span style="color:#6a8759;">        </span>println <span style="color:#6a8759;">"</span><span style="color:#cc7832;">\t</span><span style="color:#6a8759;">Factuality: </span>$ap.<span style="color:#9876aa;">factualityAsString</span><span style="color:#cc7832;">\t</span><span style="color:#6a8759;">Attribution: </span>$attr<span style="color:#6a8759;">"<br></span><span style="color:#6a8759;">        </span>println <span style="color:#6a8759;">'</span><span style="color:#cc7832;">\t</span><span style="color:#6a8759;">----------'<br></span><span style="color:#6a8759;">    </span>}<br><span style="font-weight:bold;">}<br></span></pre>
<p>The output for the previously mentioned sentence is shown below:</p>
<pre>Input sentence: The conference wrapped up yesterday at 5:30 p.m. in Copenhagen, Denmark.
=============================
Extractions:
        Triple: "conference"    "wrapped up yesterday at"       "5:30 p.m."
        Factuality: (+,CT)      Attribution: NONE
        ----------
        Triple: "conference"    "wrapped up yesterday in"       "Copenhagen"
        Factuality: (+,CT)      Attribution: NONE
        ----------
        Triple: "conference"    "wrapped up"    "yesterday"
        Factuality: (+,CT)      Attribution: NONE
</pre>
<p>We can now piece together the relationships between the earlier entities we detected.</p>
<p>There was also a problematic case amongst the earlier NER examples, "<span style="background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 13px;">The parcel was passed from May to June.</span>". Using the previous model, detected "<span style="background-color: rgb(245, 245, 245); color: rgb(51, 51, 51); font-family: Menlo, Monaco, Consolas, "Courier New", monospace; font-size: 13px;">May to June</span>" as a <i>date</i>. Let's explore that using CoreNLP's triple extraction directly. We won't show the source code here but CoreNLP supports <a href="https://github.com/paulk-asert/groovy-data-science/blob/master/subprojects/LanguageProcessing/src/main/groovy/DetectTriplesPOS_CoreNLP.groovy" target="_blank">simple</a> and <a href="https://github.com/paulk-asert/groovy-data-science/blob/master/subprojects/LanguageProcessing/src/main/groovy/DetectTriplesAnnotation_CoreNLP.groovy" target="_blank">more powerful</a> approaches to solving this problem. The output for the sentence in question using the more powerful technique is:</p>
<pre>Sentence #7: The parcel was passed from May to June.
root(ROOT-0, passed-4)
det(parcel-2, The-1)
nsubj:pass(passed-4, parcel-2)
aux:pass(passed-4, was-3)
case(May-6, from-5)
obl:from(passed-4, May-6)
case(June-8, to-7)
obl:to(passed-4, June-8)
punct(passed-4, .-9)

Triples:
1.0	parcel	was	passed
1.0	parcel	was passed to	June
1.0	parcel	was	passed from May to June
1.0	parcel	was passed from	May
</pre>
<p>We can see that this has done a better job of piecing together what entities we have and their relationships.</p>
<h3>Sentiment Analysis</h3>
<p>Sentiment analysis is a NLP technique used to determine whether data is positive, negative, or neutral. Standford CoreNLP has default models it uses for this purpose:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>doc = <span style="color:#cc7832;">new </span>Document(<span style="color:#6a8759;">'''<br></span><span style="color:#6a8759;">StanfordNLP is fantastic!<br></span><span style="color:#6a8759;">Groovy is great fun!<br></span><span style="color:#6a8759;">Math can be hard!<br></span><span style="color:#6a8759;">'''</span>)<br><span style="color:#cc7832;">for </span>(sent <span style="color:#cc7832;">in </span>doc.sentences()) {<br>    println <span style="color:#6a8759;">"</span>$<span style="font-weight:bold;">{</span>sent.toString().padRight(<span style="color:#6897bb;">40</span>)<span style="font-weight:bold;">} </span>$<span style="font-weight:bold;">{</span>sent.sentiment()<span style="font-weight:bold;">}</span><span style="color:#6a8759;">"<br></span>}<br></pre>
<p>Which has the following output:</p>
<pre><span style="color:#D02020;">[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.6 sec].</span>
<span style="color:#C02020;">[main] INFO edu.stanford.nlp.sentiment.SentimentModel - Loading sentiment model edu/stanford/nlp/models/sentiment/sentiment.ser.gz ... done [0.1 sec].</span>
StanfordNLP is fantastic!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; POSITIVE
Groovy is great fun!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;VERY_POSITIVE
Math can be hard!&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; NEUTRAL</pre>
<p>We can also train our own. Let's start with two datasets:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>datasets = [<br>    <span style="color:#6a8759;">positive</span>: getClass().<span style="color:#9876aa;">classLoader</span>.getResource(<span style="color:#6a8759;">"rt-polarity.pos"</span>).toURI(),<br>    <span style="color:#6a8759;">negative</span>: getClass().<span style="color:#9876aa;">classLoader</span>.getResource(<span style="color:#6a8759;">"rt-polarity.neg"</span>).toURI()<br>]<br></pre>
<p>We'll first use Datumbox which, as we saw earlier, requires training parameters for our algorithm:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>trainingParams = <span style="color:#cc7832;">new </span>TextClassifier.TrainingParameters(<br>    <span style="color:#6a8759;">numericalScalerTrainingParameters</span>: <span style="color:#cc7832;">null</span>,<br>    <span style="color:#6a8759;">featureSelectorTrainingParametersList</span>: [<span style="color:#cc7832;">new </span>ChisquareSelect.TrainingParameters()],<br>    <span style="color:#6a8759;">textExtractorParameters</span>: <span style="color:#cc7832;">new </span>NgramsExtractor.Parameters(),<br>    <span style="color:#6a8759;">modelerTrainingParameters</span>: <span style="color:#cc7832;">new </span>MultinomialNaiveBayes.TrainingParameters()<br>)<br></pre>
<p>We now create our algorithm, train it with or training dataset, and for illustrative purposes validate against the training dataset:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>config = Configuration.<span style="color:#9876aa;font-style:italic;">configuration<br></span>TextClassifier classifier = MLBuilder.<span style="color:#9876aa;font-style:italic;">create</span>(trainingParams, config)<br>classifier.fit(datasets)<br><span style="color:#cc7832;">def </span>metrics = classifier.validate(datasets)<br>println <span style="color:#6a8759;">"Classifier Accuracy (using training data): </span>$metrics.<span style="color:#9876aa;">accuracy</span><span style="color:#6a8759;">"<br></span></pre>
<p>The output is shown here:</p>
<pre><span style="color:#D02020;">[main] INFO com.datumbox.framework.core.common.dataobjects.Dataframe$Builder - Dataset Parsing positive class
[main] INFO com.datumbox.framework.core.common.dataobjects.Dataframe$Builder - Dataset Parsing negative class
...</span>
Classifier Accuracy (using training data): 0.8275959103273615
</pre>
<p>Now we can test our model against several sentences:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;">[<span style="color:#6a8759;">'Datumbox is divine!'</span>, <span style="color:#6a8759;">'Groovy is great fun!'</span>, <span style="color:#6a8759;">'Math can be hard!'</span>].each <span style="font-weight:bold;">{<br></span><span style="font-weight:bold;">    </span><span style="color:#cc7832;">def </span>r = classifier.predict(it)<br>    <span style="color:#cc7832;">def </span>predicted = r.<span style="color:#9876aa;">YPredicted<br></span><span style="color:#9876aa;">    </span><span style="color:#cc7832;">def </span>probability = sprintf <span style="color:#6a8759;">'%4.2f'</span>, r.<span style="color:#9876aa;">YPredictedProbabilities</span>.get(predicted)<br>    println <span style="color:#6a8759;">"Classifing: '</span>$it<span style="color:#6a8759;">',  Predicted: </span>$predicted<span style="color:#6a8759;">,  Probability: </span>$probability<span style="color:#6a8759;">"<br></span><span style="font-weight:bold;">}<br></span></pre>
<p>Which has this output:</p>
<pre><span style="color:#D02020;">...
[main] INFO com.datumbox.framework.applications.nlp.TextClassifier - predict()
...</span>
Classifing: 'Datumbox is divine!',  Predicted: positive,  Probability: 0.83
Classifing: 'Groovy is great fun!',  Predicted: positive,  Probability: 0.80
Classifing: 'Math can be hard!',  Predicted: negative,  Probability: 0.95
</pre>
<p>We can do the same thing but with OpenNLP. First, we collect our input data. OpenNLP is expecting it in a single dataset with tagged examples:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>trainingCollection = datasets.collect <span style="font-weight:bold;">{ </span>k, v <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">    </span><span style="color:#cc7832;">new </span>File(v).readLines().collect<span style="font-weight:bold;">{</span><span style="color:#6a8759;">"</span>$k $it<span style="color:#6a8759;">"</span>.toString() <span style="font-weight:bold;">}<br></span><span style="font-weight:bold;">}</span>.sum()</pre>
<p>Now, we'll train two models. One uses <i>na&iuml;ve bayes</i>, the other <i>maxent</i>. We train up both variants.</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>variants = [<br>        <span style="color:#6a8759;">Maxent    </span>: <span style="color:#cc7832;">new </span>TrainingParameters(),<br>        <span style="color:#6a8759;">NaiveBayes</span>: <span style="color:#cc7832;">new </span>TrainingParameters((<span style="color:#9876aa;font-style:italic;">CUTOFF_PARAM</span>): <span style="color:#6a8759;">'0'</span>, (<span style="color:#9876aa;font-style:italic;">ALGORITHM_PARAM</span>): <span style="color:#9876aa;font-style:italic;">NAIVE_BAYES_VALUE</span>)<br>]<br><span style="color:#cc7832;">def </span>models = [:]<br>variants.each<span style="font-weight:bold;">{ </span>key, trainingParams <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">    </span><span style="color:#cc7832;">def </span>trainingStream = <span style="color:#cc7832;">new </span>CollectionObjectStream(trainingCollection)<br>    <span style="color:#cc7832;">def </span>sampleStream = <span style="color:#cc7832;">new </span>DocumentSampleStream(trainingStream)<br>    println <span style="color:#6a8759;">"</span><span style="color:#cc7832;">\n</span><span style="color:#6a8759;">Training using </span>$key<span style="color:#6a8759;">"<br></span><span style="color:#6a8759;">    </span>models[key] = DocumentCategorizerME.<span style="color:#9876aa;font-style:italic;">train</span>(<span style="color:#6a8759;">'en'</span>, sampleStream, trainingParams, <span style="color:#cc7832;">new </span>DoccatFactory())<br><span style="font-weight:bold;">}<br></span></pre>
<p>Now we run sentiment predictions on our sample sentences using both variants:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>w = sentences*.size().max()<br><br>variants.each <span style="font-weight:bold;">{ </span>key, params <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">    </span><span style="color:#cc7832;">def </span>categorizer = <span style="color:#cc7832;">new </span>DocumentCategorizerME(models[key])<br>    println <span style="color:#6a8759;">"</span><span style="color:#cc7832;">\n</span><span style="color:#6a8759;">Analyzing using </span>$key<span style="color:#6a8759;">"<br></span><span style="color:#6a8759;">    </span>sentences.each <span style="font-weight:bold;">{<br></span><span style="font-weight:bold;">        </span><span style="color:#cc7832;">def </span>result = categorizer.categorize(it.split(<span style="color:#6a8759;">'</span><span style="color:#6a8759;background-color:#364135;">[ !]</span><span style="color:#6a8759;">'</span>))<br>        <span style="color:#cc7832;">def </span>category = categorizer.getBestCategory(result)<br>        <span style="color:#cc7832;">def </span>prob = sprintf <span style="color:#6a8759;">'%4.2f'</span>, result[categorizer.getIndex(category)]<br>        println <span style="color:#6a8759;">"</span>$<span style="font-weight:bold;">{</span>it.padRight(w)<span style="font-weight:bold;">} </span>$category<span style="color:#6a8759;"> (</span>$prob<span style="color:#6a8759;">)}"<br></span><span style="color:#6a8759;">    </span><span style="font-weight:bold;">}<br></span><span style="font-weight:bold;">}<br></span></pre>
<p>When we run this we get:</p>
<pre>Training using Maxent ...done.
...

Training using NaiveBayes ...done.
...

Analyzing using Maxent
OpenNLP is fantastic! positive (0.64)}
Groovy is great fun!  positive (0.74)}
Math can be hard!     negative (0.61)}

Analyzing using NaiveBayes
OpenNLP is fantastic! positive (0.72)}
Groovy is great fun!  positive (0.81)}
Math can be hard!     negative (0.72)}
</pre>
<p>The models here appear to have lower probability levels compared to the model we trained for Datumbox. We could try tweaking the training parameters further if this was a problem. We'd probably also need a bigger testing set to convince ourselves of the relative merits of each model. Some models can be over-trained on small datasets and perform very well with data similar to their training datasets but perform much worse for other data.</p>
<h3>Universal Sentence Encoding</h3>
<p>This example is inspired from the <a href="https://github.com/deepjavalibrary/djl/blob/master/examples/src/main/java/ai/djl/examples/inference/UniversalSentenceEncoder.java" target="_blank">UniversalSentenceEncoder</a> example in the <a href="https://github.com/deepjavalibrary/djl/tree/master/examples" target="_blank">DJL examples module</a>. It looks at using the universal sentence encoder model from <a href="https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects" target="_blank">TensorFlow Hub</a> via the <a href="https://djl.ai/" target="_blank">DeepJavaLibrary</a> (DJL) api.</p>
<p>First we define a translator. The <code>Translator</code> interface allow us to specify pre and post processing functionality.</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">class </span>MyTranslator <span style="color:#cc7832;">implements </span>NoBatchifyTranslator<String[], <span style="color:#cc7832;">double</span>[][]> {<br>    <span style="color:#bbb529;">@Override<br></span><span style="color:#bbb529;">    </span>NDList processInput(TranslatorContext ctx, String[] raw) {<br>        <span style="color:#cc7832;">var </span>factory = ctx.<span style="color:#9876aa;">NDManager<br></span><span style="color:#9876aa;">        </span><span style="color:#cc7832;">var </span>inputs = <span style="color:#cc7832;">new </span>NDList(raw.collect(factory::create))<br>        <span style="color:#cc7832;">new </span>NDList(NDArrays.<span style="color:#9876aa;font-style:italic;">stack</span>(inputs))<br>    }<br><br>    <span style="color:#bbb529;">@Override<br></span><span style="color:#bbb529;">    </span><span style="color:#cc7832;">double</span>[][] processOutput(TranslatorContext ctx, NDList list) {<br>        <span style="color:#cc7832;">long </span>numOutputs = list.singletonOrThrow().<span style="color:#9876aa;">shape</span>.get(<span style="color:#6897bb;">0</span>)<br>        NDList result = <span style="color:#e8bf6a;font-weight:bold;">[]<br></span><span style="color:#e8bf6a;font-weight:bold;">        </span><span style="color:#cc7832;">for </span>(i <span style="color:#cc7832;">in </span><span style="color:#6897bb;">0</span>..<numOutputs) {<br>            result << list.singletonOrThrow().get(i)<br>        }<br>        result*.toFloatArray() <span style="color:#cc7832;">as double</span>[][]<br>    }<br>}</pre>
<p>Here, we manually pack our input sentences into the required n-dimensional data types, and extract our output calculations into a 2D double array.</p>
<p>Next, we create our <code>predict</code> method by first defining the criteria for our prediction algorithm. We are going to use our translator, use the TensorFlow engine, use a predefined sentence encoder model from the TensorFlow Hub, and indicate that we are creating a text embedding application:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">def </span>predict(String[] inputs) {<br>    String modelUrl = <span style="color:#6a8759;">"https://storage.googleapis.com/tfhub-modules/google/universal-sentence-encoder/4.tar.gz"<br></span><span style="color:#6a8759;"><br></span><span style="color:#6a8759;">    </span>Criteria<String[], <span style="color:#cc7832;">double</span>[][]> criteria =<br>        Criteria.<span style="color:#9876aa;font-style:italic;">builder</span>()<br>            .optApplication(Application.NLP.<span style="color:#9876aa;font-style:italic;">TEXT_EMBEDDING</span>)<br>            .setTypes(String[], <span style="color:#cc7832;">double</span>[][])<br>            .optModelUrls(modelUrl)<br>            .optTranslator(<span style="color:#cc7832;">new </span>MyTranslator())<br>            .optEngine(<span style="color:#6a8759;">"TensorFlow"</span>)<br>            .optProgress(<span style="color:#cc7832;">new </span>ProgressBar())<br>            .build()<br>    <span style="color:#cc7832;">try </span>(<span style="color:#cc7832;">var </span>model = criteria.loadModel()<br>         <span style="color:#cc7832;">var </span>predictor = model.newPredictor()) {<br>        predictor.predict(inputs)<br>    }<br>}<br></pre>
<p>Next, let's define our input strings:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;">String[] inputs = [<br>    <span style="color:#6a8759;">"Cycling is low impact and great for cardio"</span>,<br>    <span style="color:#6a8759;">"Swimming is low impact and good for fitness"</span>,<br>    <span style="color:#6a8759;">"Palates is good for fitness and flexibility"</span>,<br>    <span style="color:#6a8759;">"Weights are good for strength and fitness"</span>,<br>    <span style="color:#6a8759;">"Orchids can be tricky to grow"</span>,<br>    <span style="color:#6a8759;">"Sunflowers are fun to grow"</span>,<br>    <span style="color:#6a8759;">"Radishes are easy to grow"</span>,<br>    <span style="color:#6a8759;">"The taste of radishes grows on you after a while"</span>,<br>]<br><span style="color:#cc7832;">var </span>k = inputs.size()</pre>
<p>Now, we'll use our predictor method to calculate the embeddings for each sentence. We'll print out the embeddings and also calculate the dot product of the embeddings. The dot product (the same as the inner product for this case) reveals how related the sentences are.</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">var </span>embeddings = predict(inputs)<br><br><span style="color:#cc7832;">var </span>z = <span style="color:#cc7832;">new double</span>[k][k]<br><span style="color:#cc7832;">for </span>(i <span style="color:#cc7832;">in </span><span style="color:#6897bb;">0</span>..<k) {<br>    println <span style="color:#6a8759;">"Embedding for: </span>$<span style="font-weight:bold;">{</span>inputs[i]<span style="font-weight:bold;">}</span><span style="color:#cc7832;">\n</span>$<span style="font-weight:bold;">{</span>embeddings[i]<span style="font-weight:bold;">}</span><span style="color:#6a8759;">"<br></span><span style="color:#6a8759;">    </span><span style="color:#cc7832;">for </span>(j <span style="color:#cc7832;">in </span><span style="color:#6897bb;">0</span>..<k) {<br>        z[i][j] = <span style="color:#9876aa;font-style:italic;">dot</span>(embeddings[i], embeddings[j])<br>    }<br>}<br></pre>
<p>Finally, we'll use the Heatmap class from Smile to present a nice display highlighting what the data reveals:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">new </span>Heatmap(inputs, inputs, z, Palette.<span style="color:#9876aa;font-style:italic;">heat</span>(<span style="color:#6897bb;">20</span>).reverse()).canvas().with <span style="font-weight:bold;">{<br></span><span style="font-weight:bold;">    </span><span style="color:#9876aa;">title </span>= <span style="color:#6a8759;">'Semantic textual similarity'<br></span><span style="color:#6a8759;">    </span>setAxisLabels(<span style="color:#6a8759;">''</span>, <span style="color:#6a8759;">''</span>)<br>    window()<br><span style="font-weight:bold;">}<br></span></pre>
<p>The output shows us the embeddings:</p>
<pre>Loading:     100% |========================================|
<span style="color:#D02020;">2022-08-07 17:10:43.212697: ... This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
...
2022-08-07 17:10:52.589396: ... SavedModel load for tags { serve }; Status: success: OK...
...</span>
Embedding for: Cycling is low impact and great for cardio
[-0.02865048497915268, 0.02069241739809513, 0.010843578726053238, -0.04450441896915436, ...]
...
Embedding for: The taste of radishes grows on you after a while
[0.015841705724596977, -0.03129228577017784, 0.01183396577835083, 0.022753292694687843, ...]
</pre>
<p>The embeddings are an indication of similarity. Two sentences with similar meaning typically have similar embeddings.</p>
<p>The displayed graphic is shown below:</p>
<p><img src="https://blogs.apache.org/groovy/mediaresource/812f4232-0334-4720-9408-9582489a93b4" style="width:100%;" alt="2022-08-06 22_18_05-Smile Plot 1.png"><br></p>
<p>This graphic shows that our first four sentences are somewhat related, as are the last four sentences, but that there is minimal relationship between those two groups.</p>
<h3>More information</h3>
<p>Further examples can be found in the related repos:</p>
<p><a href="https://github.com/paulk-asert/groovy-data-science/blob/master/subprojects/LanguageProcessing" target="_blank">https://github.com/paulk-asert/groovy-data-science/blob/master/subprojects/LanguageProcessing</a></p>
<p><a href="https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingSparkNLP" target="_blank">https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingSparkNLP</a><a href="https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingSparkNLP" target="_blank"></a></p>
<p><a href="https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingDjl" target="_blank">https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingDjl</a><a href="https://github.com/paulk-asert/groovy-data-science/tree/master/subprojects/LanguageProcessingDjl" target="_blank"></a></p>
<p><span style="color: inherit; font-family: inherit; font-size: 24px;">Conclusion</span><br></p>
<p>We have look at a range of NLP examples using various NLP libraries. Hopefully you can see some cases where you could use additional NLP technologies in some of your own applications.</p>
<p><br></p>

  </div><a class="u-url" href="/groovy/entry/natural-language-processing-with-groovy" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
