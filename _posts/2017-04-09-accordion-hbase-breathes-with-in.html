---
layout: post
status: PUBLISHED
published: true
title: 'Accordion: HBase Breathes with In-Memory Compaction'
id: 64d4f6ad-5ed2-4b5b-b831-934b69b674b3
date: '2017-04-09 23:18:02 -0400'
categories: hbase
tags:
- yahoo!
- in-memory
- hbase
- compactions
permalink: hbase/entry/accordion-hbase-breathes-with-in
---
<div><span id="docs-internal-guid-18fc9a22-5500-351a-ef2a-c8763289527b"> </p>
<p dir="ltr"><em>by Anastasia Braginsky (HBase Committer), Eshcar Hillel (HBase Committer) and Edward Bortnikov (Contributor) of Yahoo! Research</em></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Modern products powered by HBase exhibit ever-increasing &nbsp;expectations from its read and write performance. Ideally, HBase applications would like to enjoy the speed of in-memory databases without giving up on the reliable persistent storage guarantees. We introduce a new algorithm in HBase 2.0, named Accordion, which takes a significant step towards this goal. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">HBase partitions the data into </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">regions</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> controlled by a cluster of </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">RegionServer</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">&rsquo;s</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The internal (vertical) scalability of </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">RegionServer</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> is crucial for end-user performance as well as for the overall system utilization. Accordion improves the RegionServer scalability via a better use of RAM. It accommodates more data in memory and writes to disk less frequently. This manifests in multiple desirable phenomena. First, HBase&rsquo;s </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">disk</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"> </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">occupancy</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> and </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">write amplification</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> are </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">reduced</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. Second, more reads and writes get served from RAM, and less are stalled by disk I/O - in other words, HBase&rsquo;s </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">performance</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> is </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">increased</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. Traditionally, these different metrics were considered at odds, and tuned at each other&rsquo;s expense. With Accordion, they all get improved simultaneously. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Accordion is inspired by the </span><a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree"><span style="background-color: transparent; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">Log-Structured-Merge (LSM) tree</span></a><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> design pattern that governs the HBase storage organization. An HBase region is stored as a sequence of searchable key-value maps. The topmost is a mutable in-memory store, called </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">MemStore</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, which absorbs the recent write (</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">put</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">) operations. The rest are immutable HDFS files, called </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">HFile</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">s. Once a MemStore overflows, it is flushed to disk, creating a new HFile. HBase adopts the </span><a href="https://blogs.apache.org/hbase/entry/apache_hbase_internals_locking_and"><span style="background-color: transparent; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">multi-versioned concurrency control</span></a><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, that is, MemStore stores all data modifications as separate versions. Multiple versions of one key may therefore reside in MemStore and the HFile tier. A read (</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">get</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">) operation, which retrieves the value by key, scans the HFile data in </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">BlockCache</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, seeking for the latest version. To reduce the number of disk accesses, HFiles are merged in the background. This process, called </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">compaction</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, removes the redundant cells and creates larger files. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">LSM trees deliver superior write performance by transforming random application-level I/O to sequential disk I/O. However, their traditional design makes no attempt to compact the in-memory data. This stems from historical reasons: LSM trees have been designed in the age when RAM was very short resource, therefore the MemStore capacity was small. With recent changes in the hardware landscape, the overall MemStore memstore managed by RegionServer can be multiple gigabytes, leaving a lot of headroom for optimization. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Accordion reapplies the LSM principle to MemStore, in order to eliminate redundancies and other overhead while the data is still in RAM. </span><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;"> </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Doing so decreases the frequency of flushes to HDFS, thereby reducing the write amplification and the overall disk footprint. With less flushes, the write operations are stalled less frequently as the MemStore overflows, therefore the write performance is improved. Less data on disk also implies less pressure on the block cache, higher hit rates, and eventually better read response times. Finally, having less disk writes also means having less compaction happening in the background, i.e., less cycles are stolen from productive (read and write) work. All in all, the effect of in-memory compaction can be envisioned as a catalyst that enables the system move faster as a whole. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Accordion currently provides two levels of in-memory compaction - </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">basic </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">and </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">eager. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The former applies generic optimizations that are good for all data update patterns. The latter is most useful for applications with high data churn, like producer-consumer queues, shopping carts, shared counters, etc. All these use cases feature frequent updates of the same keys, which generate multiple redundant versions that the algorithm takes advantage of to provide more value. On the flip side, eager optimization may incur compute overhead (more memory copies and garbage collection), which may affect response times under intensive write loads. The overhead is high if the MemStore uses on-heap </span><a href="https://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/"><span style="background-color: transparent; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">MemStore-Local Allocation Buffer (MSLAB)</span></a><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> allocation; this configuration is not advised in conjunction with eager compaction. See more details about Accordion&rsquo;s compaction algorithms in the </span><a href="https://docs.google.com/document/d/1K_8plLz0K3pmV20dsgSWwRPn1qUNMRbLmi8aJkhB7z0/edit#heading=h.g59xsnm36ghn"><span style="background-color: transparent; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">next sections</span></a><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Future implementations may tune the optimal compaction policy automatically, based on the observed workload. </span></p>
<h1 dir="ltr" style="line-height: 1.38; margin-top: 20pt; margin-bottom: 6pt; text-align: justify;"><span style="background-color: transparent; font-weight: 400; vertical-align: baseline; white-space: pre-wrap;">How To Use </span></h1>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The in-memory compaction level can be configured both globally and per column family. The supported levels are </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">none</span><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;"> </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">(legacy implementation),</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"> </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">basic</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, and </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">eager</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">By default, all tables apply </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">basic</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> in-memory compaction. This global configuration can be overridden in </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">hbase-site.xml, </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">as follows: </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">
<property></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"><span class="Apple-tab-span" style="white-space: pre;"> </span></span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"><name>hbase.hregion.compacting.memstore.type</name></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"><span class="Apple-tab-span" style="white-space: pre;"> </span></span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"><value><none|basic|eager></value></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"> </property></span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The level can also be configured in the HBase shell per column family, as follows: &nbsp;</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">create &lsquo;<br />
<tablename>&rsquo;, </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">{NAME => &lsquo;<cfname>&rsquo;, IN_MEMORY_COMPACTION => &lsquo;</span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"><NONE|BASIC|EAGER>&rsquo;</span><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">}</span></p>
<h1 dir="ltr" style="line-height: 1.38; margin-top: 20pt; margin-bottom: 6pt; text-align: justify;"><span style="background-color: transparent; font-weight: 400; vertical-align: baseline; white-space: pre-wrap;">Performance Gains, or Why You Should Care</span></h1>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">We stress-tested HBase extensively via the popular Yahoo Cloud Service Benchmark (</span><a href="https://github.com/brianfrankcooper/YCSB"><span style="background-color: transparent; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">YCSB</span></a><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">). Our experiments used 100-200 GB datasets, and exercised a variety of representative workloads. The results demonstrate significant performance gains delivered by Accordion. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Heavy-tailed (Zipf) distribution. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The first experiment exercises a workload in which the key popularities follow the Zipf distribution that arises in most of the real-life scenarios. In this context, when 100% of the operations are writes, Accordion achieves up to 30% reduction of write amplification, 20% increase of write throughput, and 22% reduction of GC. When 50% of the operations are reads, the tail read latency is reduced by 12%.</span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Uniform distribution. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The second experiment exercises a workload in which all keys are equally popular. In this context, under 100% writes, Accordion delivers up to 25% reduction of write amplification, 50% increase of write throughput, and 36% reduction of GC. The tail read latencies are not impacted (which is expected, due to complete lack of locality). </span></p>
<h1 dir="ltr" style="line-height: 1.38; margin-top: 20pt; margin-bottom: 6pt; text-align: justify;"><span style="background-color: transparent; font-weight: 400; vertical-align: baseline; white-space: pre-wrap;">How Accordion Works</span></h1>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">High Level Design. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Accordion introduces </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">CompactingMemStore</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> - a MemStore implementation that applies compaction internally. Contrast to the default MemStore, which maintains all data in one monolithic data structure, Accordion manages it as a sequence of </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">segments</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. The youngest segment, called </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">active</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, is mutable; it absorbs the put operations. Upon overflow (by default, 32MB - 25% of the MemStore size bound), the active segment is moved to an in-memory </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">pipeline</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, and becomes immutable. We call this </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">in-memory flush.</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> Get operations scan through these segments and the HFiles (the latter are accessed via the block cache, as usual in HBase). </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">CompactingMemStore may merge multiple immutable segments in the background from time to time, creating larger and leaner segments. The pipeline is therefore &ldquo;breathing&rdquo; (expanding and contracting), similar to accordion bellows. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">When RegionServer decides to flush one or more MemStore&rsquo;s to disk to free up memory, it considers the CompactingMemStore&rsquo;s after the rest that have overflown. The rationale is to prolong the lifetime of MemStore&rsquo;s that manage their memory efficiently, in order to reduce the overall I/O. When such a flush does happen, all pipeline segments are moved to a composite </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">snapshot, </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">&nbsp;merged, and streamed to a new HFile. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Figure 1 illustrates the structure of CompactingMemStore versus the traditional design. </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"><img src="https://docs.google.com/drawings/d/sIGSDahHrdGlXCzoTB8l63g/image?w=593&amp;h=305&amp;rev=1&amp;ac=1" style="transform: rotate(0rad);" width="593" height="305" /></span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: center;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Figure 1. CompactingMemStore vs DefaultMemStore</span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Segment Structure.</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> Similarly to the default MemStore, CompactingMemStore maintains an index on top of cell storage, to allow fast search by key. Traditionally, this index was implemented as a Java skiplist &nbsp;(</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">ConcurrentSkipListMap</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">) - a dynamic but wasteful data structure that manages a lot of small objects. CompactingMemStore uses a space-efficient flat layout for immutable segment indexes. This universal optimization helps all compaction policies reduce the RAM overhead, even when the data has little-to-none redundancies. Once a segment is added to the pipeline, the store serializes its index into a sorted array named </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">CellArrayMap </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">that is amenable to fast binary search. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">CellArrayMap supports both direct allocation of cells from the Java heap and custom allocation from MSLAB&rsquo;s - either on-heap or off-heap. The implementation differences are abstracted away via the helper KeyValue objects that are referenced from the index (Figure 2). CellArrayMap itself is always allocated on-heap</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt; text-align: center;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"><img src="https://docs.google.com/drawings/d/sDuPuSo0bZpBddUDwNxliAg/image?w=499&amp;h=224&amp;rev=1&amp;ac=1" style="transform: rotate(0rad);" width="499" height="224" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: center;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Figure 2. Immutable segment with a flat CellArrayMap index and MSLAB cell storage.</span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Compaction Algorithms. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The in-memory compaction algorithms maintains a single flat index on top of the pipelined segments. This saves space, especially when the data items are small, and therefore pushes the disk flush further off away in time. A single index allows searching in one place, therefore bounding the tail read latency. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">When an active segment is flushed to memory, it is queued to the compaction pipeline, and a background </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">merge</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> task is immediately scheduled. The latter simultaneously scans all the segments in the pipeline (similarly to on-disk compaction) and merges their indexes into one. The differences between the </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">basic </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">and </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">eager </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">compaction policies manifest in how they handle the cell data. </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">Basic </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">compaction does not eliminate the redundant data versions in order to &nbsp;avoid physical copy; it just rearranges the references the KeyValue objects. </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">Eager</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> compaction, on the contrary, filters out the duplicates. This comes at the cost of extra compute and data migration - for example, with MSLAB storage the surviving cells are copied to the newly created MSLAB(s). The compaction overhead pays off when the data is highly redundant. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Future implementations of compaction may automate the choice between the </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">basic</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> and </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">eager</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> compaction policies. For example, the algorithm might try eager compaction once in awhile, and schedule the next compaction based on the value delivered (i.e., fraction of data eliminated). Such an approach could relieve the system administrator from deciding a-priori, and adapt to changing access patterns. </span></p>
<h1 dir="ltr" style="line-height: 1.38; margin-top: 20pt; margin-bottom: 6pt;"><span style="background-color: transparent; font-weight: 400; vertical-align: baseline; white-space: pre-wrap;">Summary</span></h1>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">In this blog post, we covered Accordion&rsquo;s basic principles, configuration, performance gains, and some details of the in-memory compaction algorithms. <a href="https://blogs.apache.org/hbase/entry/accordion-developer-view-of-in">The next post</a> will focus on system internals for HBase developers. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"><em>We thank Michael Stack, Anoop Sam John and Ramkrishna Vasudevan for their continuous support that made this project happen. </em></span></p>
<div><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"></p>
<p></span></div>
<p></span></div>
