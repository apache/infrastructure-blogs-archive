<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>HDFS HSM and HBase: Experiment (continued) (Part 5 of 7) | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="HDFS HSM and HBase: Experiment (continued) (Part 5 of 7)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is part 5 of a 7 part report by HBase Contributor, Jingcheng Du and HDFS contributor, Wei Zhou (Jingcheng and Wei are both Software Engineers at Intel) Introduction Cluster Setup Tuning Experiment Experiment (continued) Issues Conclusions 1TB Dataset in a Single Storage The performance for 1TB dataset in HDD and SSD is shown in Figure 6 and Figure 7. Due to the limitation of memory capability, 1TB dataset in RAMDISK is not tested. Figure 6. YCSB throughput of a single storage type with 1TB dataset Figure 7. YCSB latency of a single storage type with 1TB dataset The throughput and latency on SSD are both better than HDD (134% throughput and 35% latency). This is consistent with 50GB data test. The benefits gained for throughput by using SSD are different between 50GB and 1TB (from 128% to 134%), SSD gains more benefits in the 1TB test. This is because much more I/O intensive events such as compactions occur in 1TB dataset test than 50GB, and this shows the superiority of SSD in huge data scenarios. Figure 8 shows the changes of the network throughput during the tests. Figure 8. Network throughput measured for case 1T_HDD and 1T_SSD In 1T_HDD case the network throughput is lower than 10Gbps, and in 1T_SSD case the network throughput can be much larger than 10Gbps. This means if we use a 10Gbps switch in 1T_SSD case, the network should be the bottleneck. Figure 9. Disk throughput measured for case 1T_HDD and 1T_SSD In Figure 9, we can see the bottleneck for these two cases is disk bandwidth. In 1T_HDD, at the beginning of the test the throughput is almost 1000 MB/s, but after a while the throughput drops down due to memstore limitation of regions caused by slow flush. In 1T_SSD case, the throughput seems to be limited by a ceiling of around 1300 MB/s, nearly the same with the bandwidth limitation of SATA controllers. To further improve the throughput, more SATA controllers are needed (e.g. using HBA card) instead of more SSDs are needed.&lt;/p&gt; During 1T_SSD test, we observe that the operation latencies on eight SSDs per node are very different as shown in the following chart. In Figure 10, we only include latency of two disks, sdb represents disks with a high latency and sdf represents disks with a low latency. Figure 10. I/O await time measured for different disks Four of them have a better latency than the other ones. This is caused by the hardware design issue. You can find the details in Disk I/O Bandwidth and Latency Varies for Ports. The disk with higher latency might take the same workload as the disks with lower latency in the existing VolumeChoosingPolicy, this would slow down the performance. We suggest to implement a latency-aware VolumeChoosingPolicy in HDFS. Performance Estimation for RAMDISK with 1TB Dataset We cannot measure the performance of RAMDISK with 1T dataset due to RAMDISK limited capacity. Instead we have to evaluate its performance by analyzing the results of cases HDD and SSD. &lt;/p&gt; The performance between 1TB and 50GB dataset are pretty close in HDD and SSD. The throughput difference between 50GB and 1TB dataset for HDD is |242801250034-1|&times;100%=2.89% &lt;/p&gt; While for SSD the value is |325148320616-1|&times;100%=1.41% &lt;/p&gt; If we make an average of the above values as the throughput decrease in RAMDISK between 50GB and 1TB dataset, it is around 2.15% ((2.89%+1.41%)/2=2.15%), thus the throughput for RAMDISK with 1T dataset should be 406577&times;(1+2.15%)=415318 (ops/sec) Figure 11. &nbsp;YCSB throughput estimation for RAMDISK with 1TB dataset Please note: the throughput doesn&rsquo;t drop much in 1 TB dataset cases compared to 50 GB dataset cases because they do not use the same number of pre-split regions. The table is pre-split to 18 regions in 50 GB dataset cases, and it is pre-split to 210 regions in the 1 TB dataset. Performance for Tiered Storage In this section, we will study the HBase write performance on tiered storage (i.e. different storage mixed together in one test). This would show what performance it can achieve by mixing fast and slow storage together, and help us to conclude the best balance of storage between performance and cost. &lt;/p&gt; Figure 12 and Figure 13 show the performance for tiered storage. You can find the description of each case in Table 1. Most of the cases that introduce fast storage have better throughput and latency. With no surprise, 1T_RAM_SSD has the best performance among them. The real surprise is that the throughput of 1T_RAM_HDD is worse than 1T_HDD (-11%) and 1T_RAM_SSD_All_HDD is worse than 1T_SSD_All_HDD (-2%) after introducing RAMDISK, and 1T_SSD is worse than 1T_SSD_HDD (-2%). Figure 12. &nbsp;YCSB throughput data for tiered storage Figure 13. &nbsp;YCSB latency data for tiered storage We also investigate how much data is written to different storage types by collecting information from one DataNode. Figure 14. Distribution of data blocks on each storage of HDFS in one DataNode As shown in Figure 14, generally, more data are written to disks for test cases with higher throughput. Fast storage can accelerate the flush and compaction, which lead to more flushes and compactions. &nbsp;Thus, more data are written to disks. In some RAMDISK-related cases, only WAL can be written to RAMDISK, and there are 1216 GB WALs written to one DataNode. &lt;/p&gt; For tests without SSD (1T_HDD and 1T_RAM_HDD), we by purpose limiting the number of flush and compaction actions by using fewer flushers and compactors. This is due to limited IOPs capability of HDD, which lead to fewer flush &amp; compactions. Too many concurrent reads and writes can hurt HDD performance which eventually slows down the performance. &lt;/p&gt; Many BLOCKED DataNode threads can be blocked up to tens of seconds in 1T_RAM_HDD. We observe this in other cases as well, but it happens most often in 1T_RAM_HDD. This is because each DataNode holds one big lock when creating/finalizing HDFS blocks, these methods might take tens of seconds sometimes (see Long-time BLOCKED threads in DataNode), the more these methods are used (in HBase they are used in flusher, compactor, and WAL), the more often the BLOCKED occurs. Writing WAL in HBase needs to create/finalize blocks which can be blocked, and consequently users&rsquo; inputs are blocked. Multiple WAL with a large number of groups or WAL per region might also encounter this problem, especially in HDD. &lt;/p&gt; With the written data distribution in mind, let&rsquo;s look back at the performance result in Figure 12 and Figure 13. According to them, we have following observations: Mixing SSD and HDD can greatly improve the performance (136% throughput and 35% latency) compared to pure HDD. But fully replacing HDD with SSD doesn&rsquo;t show an improvement (98% throughput and 99% latency) over mixing SSD/HDD. This is because the hardware design cannot evenly split the I/O bandwidth to all eight disks, and 94% data are written in SSD while only 6% data are written to HDD in SSD/HDD mixing case. This strongly hints a mix use of SSD/HDD can achieve the best balance between performance and cost. More information is in Disk Bandwidth Limitation and Disk I/O Bandwidth and Latency Varies for Ports. Including RAMDISK in SSD/HDD tiered storage has different results with 1T_RAM_SSD_All_HDD and 1T_RAM_SSD_HDD. The case 1T_RAM_SSD_HDD shows a result when there are only a few data written to HDD, which improves the performance over SSD/HDD mixing cases. The results of 1T_RAM_SSD_All_HDD when there are a large number of data written to HDD is worse than SSD/HDD mixing cases. This means if we distribute the data appropriately to SSD and HDD in HBase, we can gain a good performance when mixing RAMDISK/SSD/HDD. The RAMDISK/SSD tiered storage is the winner of both throughput and latency (109% throughput and 67% latency of pure SSD case). So, if cost is not an issue and maximum performance is needed, RAMDISK/SSD should be chosen.&lt;/p&gt; The throughput decreases by 11% by comparing 1T_RAM_HDD to 1T_HDD. This is initially because 1T_RAM_HDD uses RAMDISK which consumes part of the RAM, which results in the OS buffer having less memory to cache the data. &lt;/p&gt; Further, with 1T_RAM_HDD, the YCSB client can push data at very high speed, cells are accumulated very fast in memstore while the flush and compaction in HDD are slow, the RegionTooBusyException occurs more often (the figure below shows a much larger memstore in 1T_RAM_HDD than 1T_HDD), and we observe much longer GC pause in 1T_RAM_HDD than 1T_HDD, it can be up to 20 seconds in a minute. Figure 15. Memstore size in 1T_RAM_HDD and 1T_HDD Finally, as we try to increase the number of flushers and compactors, the performance even goes worse because of the reasons mentioned when explaining why we use less flusher and compactors in HDD-related tests (see Long-time BLOCKED threads in DataNode). The performance reduction in 1T_RAM_SSD_All_HDD than 1T_SSD_All_HDD (-2%) is due to the same reasons mentioned above. &lt;/p&gt; We suggest: &lt;/p&gt; Implement a finer grained lock mechanism in DataNode. &lt;/p&gt; Use reasonable configurations for flusher and compactor, especially in HDD-related cases. Don&rsquo;t use the storage that has large performance gaps, such as directly mixing RAMDISK and HDD together. In many cases, we can observe the long GC pause around 10 seconds per minute. We need to implement an off-heap memstore in HBase to solve long GC pause issues. Implement a finer grained lock mechanism in DataNode. Go to part 6,&nbsp;Issues" />
<meta property="og:description" content="This is part 5 of a 7 part report by HBase Contributor, Jingcheng Du and HDFS contributor, Wei Zhou (Jingcheng and Wei are both Software Engineers at Intel) Introduction Cluster Setup Tuning Experiment Experiment (continued) Issues Conclusions 1TB Dataset in a Single Storage The performance for 1TB dataset in HDD and SSD is shown in Figure 6 and Figure 7. Due to the limitation of memory capability, 1TB dataset in RAMDISK is not tested. Figure 6. YCSB throughput of a single storage type with 1TB dataset Figure 7. YCSB latency of a single storage type with 1TB dataset The throughput and latency on SSD are both better than HDD (134% throughput and 35% latency). This is consistent with 50GB data test. The benefits gained for throughput by using SSD are different between 50GB and 1TB (from 128% to 134%), SSD gains more benefits in the 1TB test. This is because much more I/O intensive events such as compactions occur in 1TB dataset test than 50GB, and this shows the superiority of SSD in huge data scenarios. Figure 8 shows the changes of the network throughput during the tests. Figure 8. Network throughput measured for case 1T_HDD and 1T_SSD In 1T_HDD case the network throughput is lower than 10Gbps, and in 1T_SSD case the network throughput can be much larger than 10Gbps. This means if we use a 10Gbps switch in 1T_SSD case, the network should be the bottleneck. Figure 9. Disk throughput measured for case 1T_HDD and 1T_SSD In Figure 9, we can see the bottleneck for these two cases is disk bandwidth. In 1T_HDD, at the beginning of the test the throughput is almost 1000 MB/s, but after a while the throughput drops down due to memstore limitation of regions caused by slow flush. In 1T_SSD case, the throughput seems to be limited by a ceiling of around 1300 MB/s, nearly the same with the bandwidth limitation of SATA controllers. To further improve the throughput, more SATA controllers are needed (e.g. using HBA card) instead of more SSDs are needed.&lt;/p&gt; During 1T_SSD test, we observe that the operation latencies on eight SSDs per node are very different as shown in the following chart. In Figure 10, we only include latency of two disks, sdb represents disks with a high latency and sdf represents disks with a low latency. Figure 10. I/O await time measured for different disks Four of them have a better latency than the other ones. This is caused by the hardware design issue. You can find the details in Disk I/O Bandwidth and Latency Varies for Ports. The disk with higher latency might take the same workload as the disks with lower latency in the existing VolumeChoosingPolicy, this would slow down the performance. We suggest to implement a latency-aware VolumeChoosingPolicy in HDFS. Performance Estimation for RAMDISK with 1TB Dataset We cannot measure the performance of RAMDISK with 1T dataset due to RAMDISK limited capacity. Instead we have to evaluate its performance by analyzing the results of cases HDD and SSD. &lt;/p&gt; The performance between 1TB and 50GB dataset are pretty close in HDD and SSD. The throughput difference between 50GB and 1TB dataset for HDD is |242801250034-1|&times;100%=2.89% &lt;/p&gt; While for SSD the value is |325148320616-1|&times;100%=1.41% &lt;/p&gt; If we make an average of the above values as the throughput decrease in RAMDISK between 50GB and 1TB dataset, it is around 2.15% ((2.89%+1.41%)/2=2.15%), thus the throughput for RAMDISK with 1T dataset should be 406577&times;(1+2.15%)=415318 (ops/sec) Figure 11. &nbsp;YCSB throughput estimation for RAMDISK with 1TB dataset Please note: the throughput doesn&rsquo;t drop much in 1 TB dataset cases compared to 50 GB dataset cases because they do not use the same number of pre-split regions. The table is pre-split to 18 regions in 50 GB dataset cases, and it is pre-split to 210 regions in the 1 TB dataset. Performance for Tiered Storage In this section, we will study the HBase write performance on tiered storage (i.e. different storage mixed together in one test). This would show what performance it can achieve by mixing fast and slow storage together, and help us to conclude the best balance of storage between performance and cost. &lt;/p&gt; Figure 12 and Figure 13 show the performance for tiered storage. You can find the description of each case in Table 1. Most of the cases that introduce fast storage have better throughput and latency. With no surprise, 1T_RAM_SSD has the best performance among them. The real surprise is that the throughput of 1T_RAM_HDD is worse than 1T_HDD (-11%) and 1T_RAM_SSD_All_HDD is worse than 1T_SSD_All_HDD (-2%) after introducing RAMDISK, and 1T_SSD is worse than 1T_SSD_HDD (-2%). Figure 12. &nbsp;YCSB throughput data for tiered storage Figure 13. &nbsp;YCSB latency data for tiered storage We also investigate how much data is written to different storage types by collecting information from one DataNode. Figure 14. Distribution of data blocks on each storage of HDFS in one DataNode As shown in Figure 14, generally, more data are written to disks for test cases with higher throughput. Fast storage can accelerate the flush and compaction, which lead to more flushes and compactions. &nbsp;Thus, more data are written to disks. In some RAMDISK-related cases, only WAL can be written to RAMDISK, and there are 1216 GB WALs written to one DataNode. &lt;/p&gt; For tests without SSD (1T_HDD and 1T_RAM_HDD), we by purpose limiting the number of flush and compaction actions by using fewer flushers and compactors. This is due to limited IOPs capability of HDD, which lead to fewer flush &amp; compactions. Too many concurrent reads and writes can hurt HDD performance which eventually slows down the performance. &lt;/p&gt; Many BLOCKED DataNode threads can be blocked up to tens of seconds in 1T_RAM_HDD. We observe this in other cases as well, but it happens most often in 1T_RAM_HDD. This is because each DataNode holds one big lock when creating/finalizing HDFS blocks, these methods might take tens of seconds sometimes (see Long-time BLOCKED threads in DataNode), the more these methods are used (in HBase they are used in flusher, compactor, and WAL), the more often the BLOCKED occurs. Writing WAL in HBase needs to create/finalize blocks which can be blocked, and consequently users&rsquo; inputs are blocked. Multiple WAL with a large number of groups or WAL per region might also encounter this problem, especially in HDD. &lt;/p&gt; With the written data distribution in mind, let&rsquo;s look back at the performance result in Figure 12 and Figure 13. According to them, we have following observations: Mixing SSD and HDD can greatly improve the performance (136% throughput and 35% latency) compared to pure HDD. But fully replacing HDD with SSD doesn&rsquo;t show an improvement (98% throughput and 99% latency) over mixing SSD/HDD. This is because the hardware design cannot evenly split the I/O bandwidth to all eight disks, and 94% data are written in SSD while only 6% data are written to HDD in SSD/HDD mixing case. This strongly hints a mix use of SSD/HDD can achieve the best balance between performance and cost. More information is in Disk Bandwidth Limitation and Disk I/O Bandwidth and Latency Varies for Ports. Including RAMDISK in SSD/HDD tiered storage has different results with 1T_RAM_SSD_All_HDD and 1T_RAM_SSD_HDD. The case 1T_RAM_SSD_HDD shows a result when there are only a few data written to HDD, which improves the performance over SSD/HDD mixing cases. The results of 1T_RAM_SSD_All_HDD when there are a large number of data written to HDD is worse than SSD/HDD mixing cases. This means if we distribute the data appropriately to SSD and HDD in HBase, we can gain a good performance when mixing RAMDISK/SSD/HDD. The RAMDISK/SSD tiered storage is the winner of both throughput and latency (109% throughput and 67% latency of pure SSD case). So, if cost is not an issue and maximum performance is needed, RAMDISK/SSD should be chosen.&lt;/p&gt; The throughput decreases by 11% by comparing 1T_RAM_HDD to 1T_HDD. This is initially because 1T_RAM_HDD uses RAMDISK which consumes part of the RAM, which results in the OS buffer having less memory to cache the data. &lt;/p&gt; Further, with 1T_RAM_HDD, the YCSB client can push data at very high speed, cells are accumulated very fast in memstore while the flush and compaction in HDD are slow, the RegionTooBusyException occurs more often (the figure below shows a much larger memstore in 1T_RAM_HDD than 1T_HDD), and we observe much longer GC pause in 1T_RAM_HDD than 1T_HDD, it can be up to 20 seconds in a minute. Figure 15. Memstore size in 1T_RAM_HDD and 1T_HDD Finally, as we try to increase the number of flushers and compactors, the performance even goes worse because of the reasons mentioned when explaining why we use less flusher and compactors in HDD-related tests (see Long-time BLOCKED threads in DataNode). The performance reduction in 1T_RAM_SSD_All_HDD than 1T_SSD_All_HDD (-2%) is due to the same reasons mentioned above. &lt;/p&gt; We suggest: &lt;/p&gt; Implement a finer grained lock mechanism in DataNode. &lt;/p&gt; Use reasonable configurations for flusher and compactor, especially in HDD-related cases. Don&rsquo;t use the storage that has large performance gaps, such as directly mixing RAMDISK and HDD together. In many cases, we can observe the long GC pause around 10 seconds per minute. We need to implement an off-heap memstore in HBase to solve long GC pause issues. Implement a finer grained lock mechanism in DataNode. Go to part 6,&nbsp;Issues" />
<link rel="canonical" href="http://localhost:4000/hbase/entry/hdfs_hsm_and_hbase_cluster2" />
<meta property="og:url" content="http://localhost:4000/hbase/entry/hdfs_hsm_and_hbase_cluster2" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-03-02T21:42:29-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="HDFS HSM and HBase: Experiment (continued) (Part 5 of 7)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2019-03-02T21:42:29-05:00","datePublished":"2019-03-02T21:42:29-05:00","description":"This is part 5 of a 7 part report by HBase Contributor, Jingcheng Du and HDFS contributor, Wei Zhou (Jingcheng and Wei are both Software Engineers at Intel) Introduction Cluster Setup Tuning Experiment Experiment (continued) Issues Conclusions 1TB Dataset in a Single Storage The performance for 1TB dataset in HDD and SSD is shown in Figure 6 and Figure 7. Due to the limitation of memory capability, 1TB dataset in RAMDISK is not tested. Figure 6. YCSB throughput of a single storage type with 1TB dataset Figure 7. YCSB latency of a single storage type with 1TB dataset The throughput and latency on SSD are both better than HDD (134% throughput and 35% latency). This is consistent with 50GB data test. The benefits gained for throughput by using SSD are different between 50GB and 1TB (from 128% to 134%), SSD gains more benefits in the 1TB test. This is because much more I/O intensive events such as compactions occur in 1TB dataset test than 50GB, and this shows the superiority of SSD in huge data scenarios. Figure 8 shows the changes of the network throughput during the tests. Figure 8. Network throughput measured for case 1T_HDD and 1T_SSD In 1T_HDD case the network throughput is lower than 10Gbps, and in 1T_SSD case the network throughput can be much larger than 10Gbps. This means if we use a 10Gbps switch in 1T_SSD case, the network should be the bottleneck. Figure 9. Disk throughput measured for case 1T_HDD and 1T_SSD In Figure 9, we can see the bottleneck for these two cases is disk bandwidth. In 1T_HDD, at the beginning of the test the throughput is almost 1000 MB/s, but after a while the throughput drops down due to memstore limitation of regions caused by slow flush. In 1T_SSD case, the throughput seems to be limited by a ceiling of around 1300 MB/s, nearly the same with the bandwidth limitation of SATA controllers. To further improve the throughput, more SATA controllers are needed (e.g. using HBA card) instead of more SSDs are needed.&lt;/p&gt; During 1T_SSD test, we observe that the operation latencies on eight SSDs per node are very different as shown in the following chart. In Figure 10, we only include latency of two disks, sdb represents disks with a high latency and sdf represents disks with a low latency. Figure 10. I/O await time measured for different disks Four of them have a better latency than the other ones. This is caused by the hardware design issue. You can find the details in Disk I/O Bandwidth and Latency Varies for Ports. The disk with higher latency might take the same workload as the disks with lower latency in the existing VolumeChoosingPolicy, this would slow down the performance. We suggest to implement a latency-aware VolumeChoosingPolicy in HDFS. Performance Estimation for RAMDISK with 1TB Dataset We cannot measure the performance of RAMDISK with 1T dataset due to RAMDISK limited capacity. Instead we have to evaluate its performance by analyzing the results of cases HDD and SSD. &lt;/p&gt; The performance between 1TB and 50GB dataset are pretty close in HDD and SSD. The throughput difference between 50GB and 1TB dataset for HDD is |242801250034-1|&times;100%=2.89% &lt;/p&gt; While for SSD the value is |325148320616-1|&times;100%=1.41% &lt;/p&gt; If we make an average of the above values as the throughput decrease in RAMDISK between 50GB and 1TB dataset, it is around 2.15% ((2.89%+1.41%)/2=2.15%), thus the throughput for RAMDISK with 1T dataset should be 406577&times;(1+2.15%)=415318 (ops/sec) Figure 11. &nbsp;YCSB throughput estimation for RAMDISK with 1TB dataset Please note: the throughput doesn&rsquo;t drop much in 1 TB dataset cases compared to 50 GB dataset cases because they do not use the same number of pre-split regions. The table is pre-split to 18 regions in 50 GB dataset cases, and it is pre-split to 210 regions in the 1 TB dataset. Performance for Tiered Storage In this section, we will study the HBase write performance on tiered storage (i.e. different storage mixed together in one test). This would show what performance it can achieve by mixing fast and slow storage together, and help us to conclude the best balance of storage between performance and cost. &lt;/p&gt; Figure 12 and Figure 13 show the performance for tiered storage. You can find the description of each case in Table 1. Most of the cases that introduce fast storage have better throughput and latency. With no surprise, 1T_RAM_SSD has the best performance among them. The real surprise is that the throughput of 1T_RAM_HDD is worse than 1T_HDD (-11%) and 1T_RAM_SSD_All_HDD is worse than 1T_SSD_All_HDD (-2%) after introducing RAMDISK, and 1T_SSD is worse than 1T_SSD_HDD (-2%). Figure 12. &nbsp;YCSB throughput data for tiered storage Figure 13. &nbsp;YCSB latency data for tiered storage We also investigate how much data is written to different storage types by collecting information from one DataNode. Figure 14. Distribution of data blocks on each storage of HDFS in one DataNode As shown in Figure 14, generally, more data are written to disks for test cases with higher throughput. Fast storage can accelerate the flush and compaction, which lead to more flushes and compactions. &nbsp;Thus, more data are written to disks. In some RAMDISK-related cases, only WAL can be written to RAMDISK, and there are 1216 GB WALs written to one DataNode. &lt;/p&gt; For tests without SSD (1T_HDD and 1T_RAM_HDD), we by purpose limiting the number of flush and compaction actions by using fewer flushers and compactors. This is due to limited IOPs capability of HDD, which lead to fewer flush &amp; compactions. Too many concurrent reads and writes can hurt HDD performance which eventually slows down the performance. &lt;/p&gt; Many BLOCKED DataNode threads can be blocked up to tens of seconds in 1T_RAM_HDD. We observe this in other cases as well, but it happens most often in 1T_RAM_HDD. This is because each DataNode holds one big lock when creating/finalizing HDFS blocks, these methods might take tens of seconds sometimes (see Long-time BLOCKED threads in DataNode), the more these methods are used (in HBase they are used in flusher, compactor, and WAL), the more often the BLOCKED occurs. Writing WAL in HBase needs to create/finalize blocks which can be blocked, and consequently users&rsquo; inputs are blocked. Multiple WAL with a large number of groups or WAL per region might also encounter this problem, especially in HDD. &lt;/p&gt; With the written data distribution in mind, let&rsquo;s look back at the performance result in Figure 12 and Figure 13. According to them, we have following observations: Mixing SSD and HDD can greatly improve the performance (136% throughput and 35% latency) compared to pure HDD. But fully replacing HDD with SSD doesn&rsquo;t show an improvement (98% throughput and 99% latency) over mixing SSD/HDD. This is because the hardware design cannot evenly split the I/O bandwidth to all eight disks, and 94% data are written in SSD while only 6% data are written to HDD in SSD/HDD mixing case. This strongly hints a mix use of SSD/HDD can achieve the best balance between performance and cost. More information is in Disk Bandwidth Limitation and Disk I/O Bandwidth and Latency Varies for Ports. Including RAMDISK in SSD/HDD tiered storage has different results with 1T_RAM_SSD_All_HDD and 1T_RAM_SSD_HDD. The case 1T_RAM_SSD_HDD shows a result when there are only a few data written to HDD, which improves the performance over SSD/HDD mixing cases. The results of 1T_RAM_SSD_All_HDD when there are a large number of data written to HDD is worse than SSD/HDD mixing cases. This means if we distribute the data appropriately to SSD and HDD in HBase, we can gain a good performance when mixing RAMDISK/SSD/HDD. The RAMDISK/SSD tiered storage is the winner of both throughput and latency (109% throughput and 67% latency of pure SSD case). So, if cost is not an issue and maximum performance is needed, RAMDISK/SSD should be chosen.&lt;/p&gt; The throughput decreases by 11% by comparing 1T_RAM_HDD to 1T_HDD. This is initially because 1T_RAM_HDD uses RAMDISK which consumes part of the RAM, which results in the OS buffer having less memory to cache the data. &lt;/p&gt; Further, with 1T_RAM_HDD, the YCSB client can push data at very high speed, cells are accumulated very fast in memstore while the flush and compaction in HDD are slow, the RegionTooBusyException occurs more often (the figure below shows a much larger memstore in 1T_RAM_HDD than 1T_HDD), and we observe much longer GC pause in 1T_RAM_HDD than 1T_HDD, it can be up to 20 seconds in a minute. Figure 15. Memstore size in 1T_RAM_HDD and 1T_HDD Finally, as we try to increase the number of flushers and compactors, the performance even goes worse because of the reasons mentioned when explaining why we use less flusher and compactors in HDD-related tests (see Long-time BLOCKED threads in DataNode). The performance reduction in 1T_RAM_SSD_All_HDD than 1T_SSD_All_HDD (-2%) is due to the same reasons mentioned above. &lt;/p&gt; We suggest: &lt;/p&gt; Implement a finer grained lock mechanism in DataNode. &lt;/p&gt; Use reasonable configurations for flusher and compactor, especially in HDD-related cases. Don&rsquo;t use the storage that has large performance gaps, such as directly mixing RAMDISK and HDD together. In many cases, we can observe the long GC pause around 10 seconds per minute. We need to implement an off-heap memstore in HBase to solve long GC pause issues. Implement a finer grained lock mechanism in DataNode. Go to part 6,&nbsp;Issues","headline":"HDFS HSM and HBase: Experiment (continued) (Part 5 of 7)","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/hbase/entry/hdfs_hsm_and_hbase_cluster2"},"url":"http://localhost:4000/hbase/entry/hdfs_hsm_and_hbase_cluster2"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">HDFS HSM and HBase: Experiment (continued) (Part 5 of 7)</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-03-02T21:42:29-05:00" itemprop="datePublished">Mar 2, 2019
      </time>â€¢ <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Michael Stack", "login"=>"stack", "email"=>"stack@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This is part 5 of a 7 part report by HBase Contributor, Jingcheng Du and HDFS contributor, Wei Zhou (Jingcheng and Wei are both Software Engineers at Intel) </p>
<ol>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_part"><font size="1">Introduction</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster"><font size="1">Cluster Setup</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_tuning"><font size="1">Tuning</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster1"><font size="1">Experiment</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster2"><font size="1">Experiment (continued)</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster3"><font size="1">Issues</font></a></li>
<li><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster4"><font size="1">Conclusions</font></a></li>
</ol>
<h3 dir="ltr" style="line-height: 1.38; margin-top: 14pt; margin-bottom: 4pt;" id="docs-internal-guid-894f636c-4053-f7bb-3e5c-3cf5c6a69c67"><span style="font-size: 16px; font-family: Calibri; color: #0563c1; font-weight: 400; vertical-align: baseline; background-color: transparent;">1TB Dataset in a Single Storage</span></h3>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The performance for 1TB dataset in HDD and SSD is shown in Figure 6 and Figure 7. Due to the limitation of memory capability, 1TB dataset in RAMDISK is not tested.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh4.googleusercontent.com/MYR8t15XS-6cOZ0G942EGYVk8HQ2AQFZqfwI2JM6wC08_7IkYWz7CziHcQC8GBJ84NI-ppUvAhX75wBmHZ9PKeKKK9rlZ8GHEDp34BOQ0Fb6a8TULpfUFDlxQhe29qiTO0JADodl" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 6. YCSB throughput of a single storage type with 1TB dataset</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;"><img src="https://lh6.googleusercontent.com/DYg8Fa-uIHfNMtHzLk6UpPmcU2-O4vi80AieUGgJ4jZ-Dtk9mBRE2ZHYP0tVUjZofVC1A3NCFGK_QP07jQ-vFolG1QyfmDFCyp99Se3pKR6RlSFoW1NehGAcFYSOh7MEx0ZMFxvi" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 7. YCSB latency of a single storage type with 1TB dataset</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The throughput and latency on SSD are both better than HDD (134% throughput and 35% latency). This is consistent with 50GB data test.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The benefits gained for throughput by using SSD are different between 50GB and 1TB (from 128% to 134%), SSD gains more benefits in the 1TB test. This is because much more I/O intensive events such as compactions occur in 1TB dataset test than 50GB, and this shows the superiority of SSD in huge data scenarios. Figure 8 shows the changes of the network throughput during the tests.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh5.googleusercontent.com/EoJ1a-x51qiMbic5D-VYKXlJk51BWmVHftTz85Q7CQLC2C_Wwl95Py9XOnTW8YLmLsuCRIr4VUt2WTxqtPCdDFrQKVWeREpr-BhX73ZimfYsGHKHpeASdcKSPvwRJ2sbPCoDmzdI" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 8. Network throughput measured for case 1T_HDD and 1T_SSD</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">In 1T_HDD case the network throughput is lower than 10Gbps, and in 1T_SSD case the network throughput can be much larger than 10Gbps. This means if we use a 10Gbps switch in 1T_SSD case, the network should be the bottleneck.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh5.googleusercontent.com/XCo4nOOIHpFOk-f8UHkwp1MRIefbJEU44YBCc9tDMnP_estMgEFblT51DA-f64Y-A-xRJTLmH0bA-wODvImtI8gO7EmKMzA9AgBcDGxZhCcqog3nPwGkDdnamSOw2D4TXpCeTBtm" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 9. Disk throughput measured for case 1T_HDD and 1T_SSD</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">In Figure 9, we can see the bottleneck for these two cases is disk bandwidth.</span></p>
<ul style="margin-top: 0pt; margin-bottom: 0pt;">
<li dir="ltr" style="list-style-type: disc; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">In 1T_HDD, at the beginning of the test the throughput is almost 1000 MB/s, but after a while the throughput drops down due to memstore limitation of regions caused by slow flush.</span></p>
</li>
<li dir="ltr" style="list-style-type: disc; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">In 1T_SSD case, the throughput seems to be limited by a ceiling of around 1300 MB/s, nearly the same with the bandwidth limitation of SATA controllers. To further improve the throughput, more SATA controllers are needed (e.g. using HBA card) instead of more SSDs are needed.</span></p></p>
</li>
</ul>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">During 1T_SSD test, we observe that the operation latencies on eight SSDs per node are very different as shown in the following chart. In Figure 10, we only include latency of two disks, sdb represents disks with a high latency and sdf represents disks with a low latency.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh6.googleusercontent.com/3oF43nq68BJ_GNnAhQ8UKytRoyQn5WYa0A7luL35V6Vdrd8uqss3fAg8sss3KQJr4fMFU9T6-2wpwwhzUltPrujtf9UnSO-89z7gMF0hE5HCTOYQ9iPBcf-BSrNsrVR6FGQauVs6" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 10. I/O await time measured for different disks</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Four of them have a better latency than the other ones. This is caused by the hardware design issue. You can find the details in </span><a href="https://docs.google.com/document/d/1CEpvMfrh5HxbUl7DfkrRXZHI4wCzOIehFlvDlfYr4LA/edit#heading=h.q7x244fcmnlo"><span style="font-size: 14.6667px; font-family: Calibri; color: #1155cc; text-decoration: underline; vertical-align: baseline; background-color: transparent;">Disk I/O Bandwidth and Latency Varies for Ports</span></a><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">. The disk with higher latency might take the same workload as the disks with lower latency in the existing VolumeChoosingPolicy, this would slow down the performance. We suggest to implement a latency-aware VolumeChoosingPolicy in HDFS.</span></p>
<h3 dir="ltr" style="line-height: 1.38; margin-top: 14pt; margin-bottom: 4pt;"><span style="font-size: 16px; font-family: Calibri; color: #0563c1; font-weight: 400; vertical-align: baseline; background-color: transparent;">Performance Estimation for RAMDISK with 1TB Dataset</span></h3>
<p dir="ltr" style="line-height: 1.272; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">We cannot measure the performance of RAMDISK with 1T dataset due to RAMDISK limited capacity. Instead we have to evaluate its performance by analyzing the results of cases HDD and SSD.</span></p></p>
<p dir="ltr" style="line-height: 1.272; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The performance between 1TB and 50GB dataset are pretty close in HDD and SSD.</span></p>
<p dir="ltr" style="line-height: 1.272; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The throughput difference between 50GB and 1TB dataset for HDD is</span></p>
<p dir="ltr" style="line-height: 1.284; margin-top: 0pt; margin-bottom: 2pt; text-align: justify;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">|</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">242801</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">250034</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">-1|&times;100%=2.89%</span></p></p>
<p dir="ltr" style="line-height: 1.272; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">While for SSD the value is</span></p>
<p dir="ltr" style="line-height: 1.284; margin-top: 0pt; margin-bottom: 2pt; text-align: justify;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">|</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">325148</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">320616</span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">-1|&times;100%=1.41%</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">If we make an average of the above values as the throughput decrease in RAMDISK between 50GB and 1TB dataset, it is around 2.15% ((2.89%+1.41%)/2=2.15%), thus the throughput for RAMDISK with 1T dataset should be</span></p>
<p dir="ltr" style="line-height: 1.284; margin-top: 0pt; margin-bottom: 2pt; text-align: justify;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">406577&times;(1+2.15%)=415318 (ops/sec)</span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh5.googleusercontent.com/jYwZJ5VSd1cleLRiw9kDPCJdd0W8l-93VGb4R3avpL2qmJge2vGF5xhnCcjktgUen9T7F-97_jzCA4tPoFDHRX5hAeoUGoHapacm5zAUE6bFtcFPcFzc4VgtG3yV_SVMU7_fXNvl" style="transform: rotate(0rad);" /></span><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 11. &nbsp;YCSB throughput estimation for RAMDISK with 1TB dataset</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Please note: the throughput doesn&rsquo;t drop much in 1 TB dataset cases compared to 50 GB dataset cases because they do not use the same number of pre-split regions. The table is pre-split to 18 regions in 50 GB dataset cases, and it is pre-split to 210 regions in the 1 TB dataset.</span></p>
<h2 dir="ltr" style="line-height: 1.38; margin-top: 18pt; margin-bottom: 4pt;" id="docs-internal-guid-894f636c-4054-b155-6bbf-2a9c2a2c6f24"><span style="font-size: 17.3333px; font-family: Calibri; color: #1e4d78; font-weight: 400; vertical-align: baseline; background-color: transparent;">Performance for Tiered Storage</span></h2>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">In this section, we will study the HBase write performance on tiered storage (i.e. different storage mixed together in one test). This would show what performance it can achieve by mixing fast and slow storage together, and help us to conclude the best balance of storage between performance and cost.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Figure 12 and Figure 13 show the performance for tiered storage. You can find the description of each case in Table 1.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Most of the cases that introduce fast storage have better throughput and latency. With no surprise, 1T_RAM_SSD has the best performance among them. The real surprise is that the throughput of 1T_RAM_HDD is worse than 1T_HDD (-11%) and 1T_RAM_SSD_All_HDD is worse than 1T_SSD_All_HDD (-2%) after introducing RAMDISK, and 1T_SSD is worse than 1T_SSD_HDD (-2%).</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh6.googleusercontent.com/zK8dt71iE6k2dJSz_z4fiZIoqSbDVpE1AOlPu5kJKAVuCqXpehNYFYRPYD-qzYtK0DDImyPIdVck4BDqRxJVUL2EvK_ZRjPVQroz0sgi_C1MK33OaO-_MAMEyEQtfW2NEHA8jd0I" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 12. &nbsp;YCSB throughput data for tiered storage</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh3.googleusercontent.com/cniaqnU2o2mNQU2fUJvYXBpqKXtZ3ZNqsQ7ymeFl55MFQVzuCOIKqR7f75Ia_k1-Zvm1jWibXkqqL2csh6nzOo89g0FFPB8PT7_kzB58pkbCaErt6tCCf-ASDgbOQ3RbdH_nqtlZ" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 13. &nbsp;YCSB latency data for tiered storage</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">We also investigate how much data is written to different storage types by collecting information from one DataNode.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh6.googleusercontent.com/Lj4D1xeN-7_lIFp7louMD1-VguZUqdI6nveLHn_eeC3PzrN24AAL8ognrg4VZ8Vi6qTo4Scma529r8yrBsXNGize19_fWfXlKTtdc0RRUAXYHvMSjWQx2YlesXiVohlWKvkXqmKv" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 14. Distribution of data blocks on each storage of HDFS in one DataNode</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">As shown in Figure 14, generally, more data are written to disks for test cases with higher throughput. Fast storage can accelerate the flush and compaction, which lead to more flushes and compactions. &nbsp;Thus, more data are written to disks. In some RAMDISK-related cases, only WAL can be written to RAMDISK, and there are 1216 GB WALs written to one DataNode.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">For tests without SSD (1T_HDD and 1T_RAM_HDD), we by purpose limiting the number of flush and compaction actions by using fewer flushers and compactors. This is due to limited IOPs capability of HDD, which lead to fewer flush &amp; compactions. Too many concurrent reads and writes can hurt HDD performance which eventually slows down the performance.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Many BLOCKED DataNode threads can be blocked up to tens of seconds in 1T_RAM_HDD. We observe this in other cases as well, but it happens most often in 1T_RAM_HDD. This is because each DataNode holds one big lock when creating/finalizing HDFS blocks, these methods might take tens of seconds sometimes (see </span><a href="https://docs.google.com/document/d/1CEpvMfrh5HxbUl7DfkrRXZHI4wCzOIehFlvDlfYr4LA/edit#heading=h.956plr9mrft9"><span style="font-size: 14.6667px; font-family: Calibri; color: #1155cc; text-decoration: underline; vertical-align: baseline; background-color: transparent;">Long-time BLOCKED threads in DataNode</span></a><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">), the more these methods are used (in HBase they are used in flusher, compactor, and WAL), the more often the BLOCKED occurs. Writing WAL in HBase needs to create/finalize blocks which can be blocked, and consequently users&rsquo; inputs are blocked. Multiple WAL with a large number of groups or WAL per region might also encounter this problem, especially in HDD.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">With the written data distribution in mind, let&rsquo;s look back at the performance result in Figure 12 and Figure 13. According to them, we have following observations:</span></p>
<ol style="margin-top: 0pt; margin-bottom: 0pt;">
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Mixing SSD and HDD can greatly improve the performance (136% throughput and 35% latency) compared to pure HDD. But fully replacing HDD with SSD doesn&rsquo;t show an improvement (98% throughput and 99% latency) over mixing SSD/HDD. This is because the hardware design cannot evenly split the I/O bandwidth to all eight disks, and 94% data are written in SSD while only 6% data are written to HDD in SSD/HDD mixing case. This strongly hints a mix use of SSD/HDD can achieve the best balance between performance and cost. More information is in </span><a href="https://docs.google.com/document/d/1CEpvMfrh5HxbUl7DfkrRXZHI4wCzOIehFlvDlfYr4LA/edit#heading=h.uf53tqx1vmld"><span style="font-size: 14.6667px; color: #1155cc; text-decoration: underline; vertical-align: baseline; background-color: transparent;">Disk Bandwidth Limitation</span></a><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;"> and </span><a href="https://docs.google.com/document/d/1CEpvMfrh5HxbUl7DfkrRXZHI4wCzOIehFlvDlfYr4LA/edit#heading=h.q7x244fcmnlo"><span style="font-size: 14.6667px; color: #1155cc; text-decoration: underline; vertical-align: baseline; background-color: transparent;">Disk I/O Bandwidth and Latency Varies for Ports</span></a><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">.</span></p>
</li>
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Including RAMDISK in SSD/HDD tiered storage has different results with 1T_RAM_SSD_All_HDD and 1T_RAM_SSD_HDD. The case 1T_RAM_SSD_HDD shows a result when there are only a few data written to HDD, which improves the performance over SSD/HDD mixing cases. The results of 1T_RAM_SSD_All_HDD when there are a large number of data written to HDD is worse than SSD/HDD mixing cases. This means if we distribute the data appropriately to SSD and HDD in HBase, we can gain a good performance when mixing RAMDISK/SSD/HDD.</span></p>
</li>
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">The RAMDISK/SSD tiered storage is the winner of both throughput and latency (109% throughput and 67% latency of pure SSD case). So, if cost is not an issue and maximum performance is needed, RAMDISK/SSD should be chosen.</span></p></p>
</li>
</ol>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The throughput decreases by 11% by comparing 1T_RAM_HDD to 1T_HDD. This is initially because 1T_RAM_HDD uses RAMDISK which consumes part of the RAM, which results in the OS buffer having less memory to cache the data.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Further, with 1T_RAM_HDD, the YCSB client can push data at very high speed, cells are accumulated very fast in memstore while the flush and compaction in HDD are slow, the RegionTooBusyException occurs more often (the figure below shows a much larger memstore in 1T_RAM_HDD than 1T_HDD), and we observe much longer GC pause in 1T_RAM_HDD than 1T_HDD, it can be up to 20 seconds in a minute.</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"><img src="https://lh6.googleusercontent.com/G_iccez14CXyzQfKQ_74zJnOZKfqJm2O7s1J8XwzcMs4w8v9q9MxxH4jjaCCzbWJUS73ffnJ9ODPhvB9X-8QUGHTAdJ6L6wGqgsALDGvb9RAt7dtkQYXi-it-zAqp5VFaKCHBk6X" style="transform: rotate(0rad);" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 12px; font-family: Calibri; font-style: italic; vertical-align: baseline; background-color: transparent;">Figure 15. Memstore size in 1T_RAM_HDD and 1T_HDD</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;"> </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">Finally, as we try to increase the number of flushers and compactors, the performance even goes worse because of the reasons mentioned when explaining why we use less flusher and compactors in HDD-related tests (see </span><a href="https://docs.google.com/document/d/1CEpvMfrh5HxbUl7DfkrRXZHI4wCzOIehFlvDlfYr4LA/edit#heading=h.956plr9mrft9"><span style="font-size: 14.6667px; font-family: Calibri; color: #1155cc; text-decoration: underline; vertical-align: baseline; background-color: transparent;">Long-time BLOCKED threads in DataNode</span></a><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">).</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">The performance reduction in 1T_RAM_SSD_All_HDD than 1T_SSD_All_HDD (-2%) is due to the same reasons mentioned above.</span></p></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">We suggest:</span></p></p>
<p dir="ltr" style="font-family: Calibri; font-size: 14.6667px; line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; display: inline !important; background-color: transparent;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Implement a finer grained lock mechanism in DataNode.</span></p></p>
<ol style="margin-top: 0pt; margin-bottom: 0pt;">
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Use reasonable configurations for flusher and compactor, especially in HDD-related cases.</span></p>
</li>
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Don&rsquo;t use the storage that has large performance gaps, such as directly mixing RAMDISK and HDD together.</span></p>
</li>
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">In many cases, we can observe the long GC pause around 10 seconds per minute. We need to implement an off-heap memstore in HBase to solve long GC pause issues.</span></p>
</li>
<li dir="ltr" style="list-style-type: decimal; font-size: 14.6667px; font-family: Calibri; vertical-align: baseline; background-color: transparent;">
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt;"><span style="font-size: 14.6667px; vertical-align: baseline; background-color: transparent;">Implement a finer grained lock mechanism in DataNode.</span><span style="font-size: 14.6667px; line-height: normal; background-color: transparent;"> </span></p>
<p></p>
</li>
</ol>
<div><font face="Calibri"><span style="font-size: 14.6667px;">Go to part 6,&nbsp;</span></font><a href="https://blogs.apache.org/hbase/entry/hdfs_hsm_and_hbase_cluster3">Issues</a></div>
<p><span style="font-size: 14.6667px; font-family: Arial; vertical-align: baseline; background-color: transparent;"></span> </p>

  </div><a class="u-url" href="/hbase/entry/hdfs_hsm_and_hbase_cluster2" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
