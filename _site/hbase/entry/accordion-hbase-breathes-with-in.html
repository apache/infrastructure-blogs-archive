<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Accordion: HBase Breathes with In-Memory Compaction | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Accordion: HBase Breathes with In-Memory Compaction" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="&lt;/p&gt; by Anastasia Braginsky (HBase Committer), Eshcar Hillel (HBase Committer) and Edward Bortnikov (Contributor) of Yahoo! Research Modern products powered by HBase exhibit ever-increasing &nbsp;expectations from its read and write performance. Ideally, HBase applications would like to enjoy the speed of in-memory databases without giving up on the reliable persistent storage guarantees. We introduce a new algorithm in HBase 2.0, named Accordion, which takes a significant step towards this goal. HBase partitions the data into regions controlled by a cluster of RegionServer&rsquo;s. The internal (vertical) scalability of RegionServer is crucial for end-user performance as well as for the overall system utilization. Accordion improves the RegionServer scalability via a better use of RAM. It accommodates more data in memory and writes to disk less frequently. This manifests in multiple desirable phenomena. First, HBase&rsquo;s disk occupancy and write amplification are reduced. Second, more reads and writes get served from RAM, and less are stalled by disk I/O - in other words, HBase&rsquo;s performance is increased. Traditionally, these different metrics were considered at odds, and tuned at each other&rsquo;s expense. With Accordion, they all get improved simultaneously. Accordion is inspired by the Log-Structured-Merge (LSM) tree design pattern that governs the HBase storage organization. An HBase region is stored as a sequence of searchable key-value maps. The topmost is a mutable in-memory store, called MemStore, which absorbs the recent write (put) operations. The rest are immutable HDFS files, called HFiles. Once a MemStore overflows, it is flushed to disk, creating a new HFile. HBase adopts the multi-versioned concurrency control, that is, MemStore stores all data modifications as separate versions. Multiple versions of one key may therefore reside in MemStore and the HFile tier. A read (get) operation, which retrieves the value by key, scans the HFile data in BlockCache, seeking for the latest version. To reduce the number of disk accesses, HFiles are merged in the background. This process, called compaction, removes the redundant cells and creates larger files. LSM trees deliver superior write performance by transforming random application-level I/O to sequential disk I/O. However, their traditional design makes no attempt to compact the in-memory data. This stems from historical reasons: LSM trees have been designed in the age when RAM was very short resource, therefore the MemStore capacity was small. With recent changes in the hardware landscape, the overall MemStore memstore managed by RegionServer can be multiple gigabytes, leaving a lot of headroom for optimization. Accordion reapplies the LSM principle to MemStore, in order to eliminate redundancies and other overhead while the data is still in RAM. Doing so decreases the frequency of flushes to HDFS, thereby reducing the write amplification and the overall disk footprint. With less flushes, the write operations are stalled less frequently as the MemStore overflows, therefore the write performance is improved. Less data on disk also implies less pressure on the block cache, higher hit rates, and eventually better read response times. Finally, having less disk writes also means having less compaction happening in the background, i.e., less cycles are stolen from productive (read and write) work. All in all, the effect of in-memory compaction can be envisioned as a catalyst that enables the system move faster as a whole. Accordion currently provides two levels of in-memory compaction - basic and eager. The former applies generic optimizations that are good for all data update patterns. The latter is most useful for applications with high data churn, like producer-consumer queues, shopping carts, shared counters, etc. All these use cases feature frequent updates of the same keys, which generate multiple redundant versions that the algorithm takes advantage of to provide more value. On the flip side, eager optimization may incur compute overhead (more memory copies and garbage collection), which may affect response times under intensive write loads. The overhead is high if the MemStore uses on-heap MemStore-Local Allocation Buffer (MSLAB) allocation; this configuration is not advised in conjunction with eager compaction. See more details about Accordion&rsquo;s compaction algorithms in the next sections. Future implementations may tune the optimal compaction policy automatically, based on the observed workload. How To Use The in-memory compaction level can be configured both globally and per column family. The supported levels are none (legacy implementation), basic, and eager. By default, all tables apply basic in-memory compaction. This global configuration can be overridden in hbase-site.xml, as follows: &lt;/span&gt;&lt;/p&gt; hbase.hregion.compacting.memstore.type &lt;none|basic|eager&gt; &lt;/property&gt; The level can also be configured in the HBase shell per column family, as follows: &nbsp; create &lsquo; &rsquo;, &lt;/span&gt;&lt;/p&gt; {NAME =&gt; &lsquo;&rsquo;, IN_MEMORY_COMPACTION =&gt; &lsquo;&lt;/span&gt;&lt;NONE|BASIC|EAGER&gt;&rsquo;}&lt;/p&gt; Performance Gains, or Why You Should Care We stress-tested HBase extensively via the popular Yahoo Cloud Service Benchmark (YCSB). Our experiments used 100-200 GB datasets, and exercised a variety of representative workloads. The results demonstrate significant performance gains delivered by Accordion. Heavy-tailed (Zipf) distribution. The first experiment exercises a workload in which the key popularities follow the Zipf distribution that arises in most of the real-life scenarios. In this context, when 100% of the operations are writes, Accordion achieves up to 30% reduction of write amplification, 20% increase of write throughput, and 22% reduction of GC. When 50% of the operations are reads, the tail read latency is reduced by 12%. Uniform distribution. The second experiment exercises a workload in which all keys are equally popular. In this context, under 100% writes, Accordion delivers up to 25% reduction of write amplification, 50% increase of write throughput, and 36% reduction of GC. The tail read latencies are not impacted (which is expected, due to complete lack of locality). How Accordion Works High Level Design. Accordion introduces CompactingMemStore - a MemStore implementation that applies compaction internally. Contrast to the default MemStore, which maintains all data in one monolithic data structure, Accordion manages it as a sequence of segments. The youngest segment, called active, is mutable; it absorbs the put operations. Upon overflow (by default, 32MB - 25% of the MemStore size bound), the active segment is moved to an in-memory pipeline, and becomes immutable. We call this in-memory flush. Get operations scan through these segments and the HFiles (the latter are accessed via the block cache, as usual in HBase). CompactingMemStore may merge multiple immutable segments in the background from time to time, creating larger and leaner segments. The pipeline is therefore &ldquo;breathing&rdquo; (expanding and contracting), similar to accordion bellows. When RegionServer decides to flush one or more MemStore&rsquo;s to disk to free up memory, it considers the CompactingMemStore&rsquo;s after the rest that have overflown. The rationale is to prolong the lifetime of MemStore&rsquo;s that manage their memory efficiently, in order to reduce the overall I/O. When such a flush does happen, all pipeline segments are moved to a composite snapshot, &nbsp;merged, and streamed to a new HFile. Figure 1 illustrates the structure of CompactingMemStore versus the traditional design. Figure 1. CompactingMemStore vs DefaultMemStore Segment Structure. Similarly to the default MemStore, CompactingMemStore maintains an index on top of cell storage, to allow fast search by key. Traditionally, this index was implemented as a Java skiplist &nbsp;(ConcurrentSkipListMap) - a dynamic but wasteful data structure that manages a lot of small objects. CompactingMemStore uses a space-efficient flat layout for immutable segment indexes. This universal optimization helps all compaction policies reduce the RAM overhead, even when the data has little-to-none redundancies. Once a segment is added to the pipeline, the store serializes its index into a sorted array named CellArrayMap that is amenable to fast binary search. CellArrayMap supports both direct allocation of cells from the Java heap and custom allocation from MSLAB&rsquo;s - either on-heap or off-heap. The implementation differences are abstracted away via the helper KeyValue objects that are referenced from the index (Figure 2). CellArrayMap itself is always allocated on-heap. Figure 2. Immutable segment with a flat CellArrayMap index and MSLAB cell storage. Compaction Algorithms. The in-memory compaction algorithms maintains a single flat index on top of the pipelined segments. This saves space, especially when the data items are small, and therefore pushes the disk flush further off away in time. A single index allows searching in one place, therefore bounding the tail read latency. When an active segment is flushed to memory, it is queued to the compaction pipeline, and a background merge task is immediately scheduled. The latter simultaneously scans all the segments in the pipeline (similarly to on-disk compaction) and merges their indexes into one. The differences between the basic and eager compaction policies manifest in how they handle the cell data. Basic compaction does not eliminate the redundant data versions in order to &nbsp;avoid physical copy; it just rearranges the references the KeyValue objects. Eager compaction, on the contrary, filters out the duplicates. This comes at the cost of extra compute and data migration - for example, with MSLAB storage the surviving cells are copied to the newly created MSLAB(s). The compaction overhead pays off when the data is highly redundant. Future implementations of compaction may automate the choice between the basic and eager compaction policies. For example, the algorithm might try eager compaction once in awhile, and schedule the next compaction based on the value delivered (i.e., fraction of data eliminated). Such an approach could relieve the system administrator from deciding a-priori, and adapt to changing access patterns. Summary In this blog post, we covered Accordion&rsquo;s basic principles, configuration, performance gains, and some details of the in-memory compaction algorithms. The next post will focus on system internals for HBase developers. We thank Michael Stack, Anoop Sam John and Ramkrishna Vasudevan for their continuous support that made this project happen. &lt;/p&gt; &lt;/span&gt;&lt;/div&gt; &lt;/span&gt;&lt;/div&gt;" />
<meta property="og:description" content="&lt;/p&gt; by Anastasia Braginsky (HBase Committer), Eshcar Hillel (HBase Committer) and Edward Bortnikov (Contributor) of Yahoo! Research Modern products powered by HBase exhibit ever-increasing &nbsp;expectations from its read and write performance. Ideally, HBase applications would like to enjoy the speed of in-memory databases without giving up on the reliable persistent storage guarantees. We introduce a new algorithm in HBase 2.0, named Accordion, which takes a significant step towards this goal. HBase partitions the data into regions controlled by a cluster of RegionServer&rsquo;s. The internal (vertical) scalability of RegionServer is crucial for end-user performance as well as for the overall system utilization. Accordion improves the RegionServer scalability via a better use of RAM. It accommodates more data in memory and writes to disk less frequently. This manifests in multiple desirable phenomena. First, HBase&rsquo;s disk occupancy and write amplification are reduced. Second, more reads and writes get served from RAM, and less are stalled by disk I/O - in other words, HBase&rsquo;s performance is increased. Traditionally, these different metrics were considered at odds, and tuned at each other&rsquo;s expense. With Accordion, they all get improved simultaneously. Accordion is inspired by the Log-Structured-Merge (LSM) tree design pattern that governs the HBase storage organization. An HBase region is stored as a sequence of searchable key-value maps. The topmost is a mutable in-memory store, called MemStore, which absorbs the recent write (put) operations. The rest are immutable HDFS files, called HFiles. Once a MemStore overflows, it is flushed to disk, creating a new HFile. HBase adopts the multi-versioned concurrency control, that is, MemStore stores all data modifications as separate versions. Multiple versions of one key may therefore reside in MemStore and the HFile tier. A read (get) operation, which retrieves the value by key, scans the HFile data in BlockCache, seeking for the latest version. To reduce the number of disk accesses, HFiles are merged in the background. This process, called compaction, removes the redundant cells and creates larger files. LSM trees deliver superior write performance by transforming random application-level I/O to sequential disk I/O. However, their traditional design makes no attempt to compact the in-memory data. This stems from historical reasons: LSM trees have been designed in the age when RAM was very short resource, therefore the MemStore capacity was small. With recent changes in the hardware landscape, the overall MemStore memstore managed by RegionServer can be multiple gigabytes, leaving a lot of headroom for optimization. Accordion reapplies the LSM principle to MemStore, in order to eliminate redundancies and other overhead while the data is still in RAM. Doing so decreases the frequency of flushes to HDFS, thereby reducing the write amplification and the overall disk footprint. With less flushes, the write operations are stalled less frequently as the MemStore overflows, therefore the write performance is improved. Less data on disk also implies less pressure on the block cache, higher hit rates, and eventually better read response times. Finally, having less disk writes also means having less compaction happening in the background, i.e., less cycles are stolen from productive (read and write) work. All in all, the effect of in-memory compaction can be envisioned as a catalyst that enables the system move faster as a whole. Accordion currently provides two levels of in-memory compaction - basic and eager. The former applies generic optimizations that are good for all data update patterns. The latter is most useful for applications with high data churn, like producer-consumer queues, shopping carts, shared counters, etc. All these use cases feature frequent updates of the same keys, which generate multiple redundant versions that the algorithm takes advantage of to provide more value. On the flip side, eager optimization may incur compute overhead (more memory copies and garbage collection), which may affect response times under intensive write loads. The overhead is high if the MemStore uses on-heap MemStore-Local Allocation Buffer (MSLAB) allocation; this configuration is not advised in conjunction with eager compaction. See more details about Accordion&rsquo;s compaction algorithms in the next sections. Future implementations may tune the optimal compaction policy automatically, based on the observed workload. How To Use The in-memory compaction level can be configured both globally and per column family. The supported levels are none (legacy implementation), basic, and eager. By default, all tables apply basic in-memory compaction. This global configuration can be overridden in hbase-site.xml, as follows: &lt;/span&gt;&lt;/p&gt; hbase.hregion.compacting.memstore.type &lt;none|basic|eager&gt; &lt;/property&gt; The level can also be configured in the HBase shell per column family, as follows: &nbsp; create &lsquo; &rsquo;, &lt;/span&gt;&lt;/p&gt; {NAME =&gt; &lsquo;&rsquo;, IN_MEMORY_COMPACTION =&gt; &lsquo;&lt;/span&gt;&lt;NONE|BASIC|EAGER&gt;&rsquo;}&lt;/p&gt; Performance Gains, or Why You Should Care We stress-tested HBase extensively via the popular Yahoo Cloud Service Benchmark (YCSB). Our experiments used 100-200 GB datasets, and exercised a variety of representative workloads. The results demonstrate significant performance gains delivered by Accordion. Heavy-tailed (Zipf) distribution. The first experiment exercises a workload in which the key popularities follow the Zipf distribution that arises in most of the real-life scenarios. In this context, when 100% of the operations are writes, Accordion achieves up to 30% reduction of write amplification, 20% increase of write throughput, and 22% reduction of GC. When 50% of the operations are reads, the tail read latency is reduced by 12%. Uniform distribution. The second experiment exercises a workload in which all keys are equally popular. In this context, under 100% writes, Accordion delivers up to 25% reduction of write amplification, 50% increase of write throughput, and 36% reduction of GC. The tail read latencies are not impacted (which is expected, due to complete lack of locality). How Accordion Works High Level Design. Accordion introduces CompactingMemStore - a MemStore implementation that applies compaction internally. Contrast to the default MemStore, which maintains all data in one monolithic data structure, Accordion manages it as a sequence of segments. The youngest segment, called active, is mutable; it absorbs the put operations. Upon overflow (by default, 32MB - 25% of the MemStore size bound), the active segment is moved to an in-memory pipeline, and becomes immutable. We call this in-memory flush. Get operations scan through these segments and the HFiles (the latter are accessed via the block cache, as usual in HBase). CompactingMemStore may merge multiple immutable segments in the background from time to time, creating larger and leaner segments. The pipeline is therefore &ldquo;breathing&rdquo; (expanding and contracting), similar to accordion bellows. When RegionServer decides to flush one or more MemStore&rsquo;s to disk to free up memory, it considers the CompactingMemStore&rsquo;s after the rest that have overflown. The rationale is to prolong the lifetime of MemStore&rsquo;s that manage their memory efficiently, in order to reduce the overall I/O. When such a flush does happen, all pipeline segments are moved to a composite snapshot, &nbsp;merged, and streamed to a new HFile. Figure 1 illustrates the structure of CompactingMemStore versus the traditional design. Figure 1. CompactingMemStore vs DefaultMemStore Segment Structure. Similarly to the default MemStore, CompactingMemStore maintains an index on top of cell storage, to allow fast search by key. Traditionally, this index was implemented as a Java skiplist &nbsp;(ConcurrentSkipListMap) - a dynamic but wasteful data structure that manages a lot of small objects. CompactingMemStore uses a space-efficient flat layout for immutable segment indexes. This universal optimization helps all compaction policies reduce the RAM overhead, even when the data has little-to-none redundancies. Once a segment is added to the pipeline, the store serializes its index into a sorted array named CellArrayMap that is amenable to fast binary search. CellArrayMap supports both direct allocation of cells from the Java heap and custom allocation from MSLAB&rsquo;s - either on-heap or off-heap. The implementation differences are abstracted away via the helper KeyValue objects that are referenced from the index (Figure 2). CellArrayMap itself is always allocated on-heap. Figure 2. Immutable segment with a flat CellArrayMap index and MSLAB cell storage. Compaction Algorithms. The in-memory compaction algorithms maintains a single flat index on top of the pipelined segments. This saves space, especially when the data items are small, and therefore pushes the disk flush further off away in time. A single index allows searching in one place, therefore bounding the tail read latency. When an active segment is flushed to memory, it is queued to the compaction pipeline, and a background merge task is immediately scheduled. The latter simultaneously scans all the segments in the pipeline (similarly to on-disk compaction) and merges their indexes into one. The differences between the basic and eager compaction policies manifest in how they handle the cell data. Basic compaction does not eliminate the redundant data versions in order to &nbsp;avoid physical copy; it just rearranges the references the KeyValue objects. Eager compaction, on the contrary, filters out the duplicates. This comes at the cost of extra compute and data migration - for example, with MSLAB storage the surviving cells are copied to the newly created MSLAB(s). The compaction overhead pays off when the data is highly redundant. Future implementations of compaction may automate the choice between the basic and eager compaction policies. For example, the algorithm might try eager compaction once in awhile, and schedule the next compaction based on the value delivered (i.e., fraction of data eliminated). Such an approach could relieve the system administrator from deciding a-priori, and adapt to changing access patterns. Summary In this blog post, we covered Accordion&rsquo;s basic principles, configuration, performance gains, and some details of the in-memory compaction algorithms. The next post will focus on system internals for HBase developers. We thank Michael Stack, Anoop Sam John and Ramkrishna Vasudevan for their continuous support that made this project happen. &lt;/p&gt; &lt;/span&gt;&lt;/div&gt; &lt;/span&gt;&lt;/div&gt;" />
<link rel="canonical" href="http://localhost:4000/hbase/entry/accordion-hbase-breathes-with-in" />
<meta property="og:url" content="http://localhost:4000/hbase/entry/accordion-hbase-breathes-with-in" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-03-02T21:41:22-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Accordion: HBase Breathes with In-Memory Compaction" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2019-03-02T21:41:22-05:00","datePublished":"2019-03-02T21:41:22-05:00","description":"&lt;/p&gt; by Anastasia Braginsky (HBase Committer), Eshcar Hillel (HBase Committer) and Edward Bortnikov (Contributor) of Yahoo! Research Modern products powered by HBase exhibit ever-increasing &nbsp;expectations from its read and write performance. Ideally, HBase applications would like to enjoy the speed of in-memory databases without giving up on the reliable persistent storage guarantees. We introduce a new algorithm in HBase 2.0, named Accordion, which takes a significant step towards this goal. HBase partitions the data into regions controlled by a cluster of RegionServer&rsquo;s. The internal (vertical) scalability of RegionServer is crucial for end-user performance as well as for the overall system utilization. Accordion improves the RegionServer scalability via a better use of RAM. It accommodates more data in memory and writes to disk less frequently. This manifests in multiple desirable phenomena. First, HBase&rsquo;s disk occupancy and write amplification are reduced. Second, more reads and writes get served from RAM, and less are stalled by disk I/O - in other words, HBase&rsquo;s performance is increased. Traditionally, these different metrics were considered at odds, and tuned at each other&rsquo;s expense. With Accordion, they all get improved simultaneously. Accordion is inspired by the Log-Structured-Merge (LSM) tree design pattern that governs the HBase storage organization. An HBase region is stored as a sequence of searchable key-value maps. The topmost is a mutable in-memory store, called MemStore, which absorbs the recent write (put) operations. The rest are immutable HDFS files, called HFiles. Once a MemStore overflows, it is flushed to disk, creating a new HFile. HBase adopts the multi-versioned concurrency control, that is, MemStore stores all data modifications as separate versions. Multiple versions of one key may therefore reside in MemStore and the HFile tier. A read (get) operation, which retrieves the value by key, scans the HFile data in BlockCache, seeking for the latest version. To reduce the number of disk accesses, HFiles are merged in the background. This process, called compaction, removes the redundant cells and creates larger files. LSM trees deliver superior write performance by transforming random application-level I/O to sequential disk I/O. However, their traditional design makes no attempt to compact the in-memory data. This stems from historical reasons: LSM trees have been designed in the age when RAM was very short resource, therefore the MemStore capacity was small. With recent changes in the hardware landscape, the overall MemStore memstore managed by RegionServer can be multiple gigabytes, leaving a lot of headroom for optimization. Accordion reapplies the LSM principle to MemStore, in order to eliminate redundancies and other overhead while the data is still in RAM. Doing so decreases the frequency of flushes to HDFS, thereby reducing the write amplification and the overall disk footprint. With less flushes, the write operations are stalled less frequently as the MemStore overflows, therefore the write performance is improved. Less data on disk also implies less pressure on the block cache, higher hit rates, and eventually better read response times. Finally, having less disk writes also means having less compaction happening in the background, i.e., less cycles are stolen from productive (read and write) work. All in all, the effect of in-memory compaction can be envisioned as a catalyst that enables the system move faster as a whole. Accordion currently provides two levels of in-memory compaction - basic and eager. The former applies generic optimizations that are good for all data update patterns. The latter is most useful for applications with high data churn, like producer-consumer queues, shopping carts, shared counters, etc. All these use cases feature frequent updates of the same keys, which generate multiple redundant versions that the algorithm takes advantage of to provide more value. On the flip side, eager optimization may incur compute overhead (more memory copies and garbage collection), which may affect response times under intensive write loads. The overhead is high if the MemStore uses on-heap MemStore-Local Allocation Buffer (MSLAB) allocation; this configuration is not advised in conjunction with eager compaction. See more details about Accordion&rsquo;s compaction algorithms in the next sections. Future implementations may tune the optimal compaction policy automatically, based on the observed workload. How To Use The in-memory compaction level can be configured both globally and per column family. The supported levels are none (legacy implementation), basic, and eager. By default, all tables apply basic in-memory compaction. This global configuration can be overridden in hbase-site.xml, as follows: &lt;/span&gt;&lt;/p&gt; hbase.hregion.compacting.memstore.type &lt;none|basic|eager&gt; &lt;/property&gt; The level can also be configured in the HBase shell per column family, as follows: &nbsp; create &lsquo; &rsquo;, &lt;/span&gt;&lt;/p&gt; {NAME =&gt; &lsquo;&rsquo;, IN_MEMORY_COMPACTION =&gt; &lsquo;&lt;/span&gt;&lt;NONE|BASIC|EAGER&gt;&rsquo;}&lt;/p&gt; Performance Gains, or Why You Should Care We stress-tested HBase extensively via the popular Yahoo Cloud Service Benchmark (YCSB). Our experiments used 100-200 GB datasets, and exercised a variety of representative workloads. The results demonstrate significant performance gains delivered by Accordion. Heavy-tailed (Zipf) distribution. The first experiment exercises a workload in which the key popularities follow the Zipf distribution that arises in most of the real-life scenarios. In this context, when 100% of the operations are writes, Accordion achieves up to 30% reduction of write amplification, 20% increase of write throughput, and 22% reduction of GC. When 50% of the operations are reads, the tail read latency is reduced by 12%. Uniform distribution. The second experiment exercises a workload in which all keys are equally popular. In this context, under 100% writes, Accordion delivers up to 25% reduction of write amplification, 50% increase of write throughput, and 36% reduction of GC. The tail read latencies are not impacted (which is expected, due to complete lack of locality). How Accordion Works High Level Design. Accordion introduces CompactingMemStore - a MemStore implementation that applies compaction internally. Contrast to the default MemStore, which maintains all data in one monolithic data structure, Accordion manages it as a sequence of segments. The youngest segment, called active, is mutable; it absorbs the put operations. Upon overflow (by default, 32MB - 25% of the MemStore size bound), the active segment is moved to an in-memory pipeline, and becomes immutable. We call this in-memory flush. Get operations scan through these segments and the HFiles (the latter are accessed via the block cache, as usual in HBase). CompactingMemStore may merge multiple immutable segments in the background from time to time, creating larger and leaner segments. The pipeline is therefore &ldquo;breathing&rdquo; (expanding and contracting), similar to accordion bellows. When RegionServer decides to flush one or more MemStore&rsquo;s to disk to free up memory, it considers the CompactingMemStore&rsquo;s after the rest that have overflown. The rationale is to prolong the lifetime of MemStore&rsquo;s that manage their memory efficiently, in order to reduce the overall I/O. When such a flush does happen, all pipeline segments are moved to a composite snapshot, &nbsp;merged, and streamed to a new HFile. Figure 1 illustrates the structure of CompactingMemStore versus the traditional design. Figure 1. CompactingMemStore vs DefaultMemStore Segment Structure. Similarly to the default MemStore, CompactingMemStore maintains an index on top of cell storage, to allow fast search by key. Traditionally, this index was implemented as a Java skiplist &nbsp;(ConcurrentSkipListMap) - a dynamic but wasteful data structure that manages a lot of small objects. CompactingMemStore uses a space-efficient flat layout for immutable segment indexes. This universal optimization helps all compaction policies reduce the RAM overhead, even when the data has little-to-none redundancies. Once a segment is added to the pipeline, the store serializes its index into a sorted array named CellArrayMap that is amenable to fast binary search. CellArrayMap supports both direct allocation of cells from the Java heap and custom allocation from MSLAB&rsquo;s - either on-heap or off-heap. The implementation differences are abstracted away via the helper KeyValue objects that are referenced from the index (Figure 2). CellArrayMap itself is always allocated on-heap. Figure 2. Immutable segment with a flat CellArrayMap index and MSLAB cell storage. Compaction Algorithms. The in-memory compaction algorithms maintains a single flat index on top of the pipelined segments. This saves space, especially when the data items are small, and therefore pushes the disk flush further off away in time. A single index allows searching in one place, therefore bounding the tail read latency. When an active segment is flushed to memory, it is queued to the compaction pipeline, and a background merge task is immediately scheduled. The latter simultaneously scans all the segments in the pipeline (similarly to on-disk compaction) and merges their indexes into one. The differences between the basic and eager compaction policies manifest in how they handle the cell data. Basic compaction does not eliminate the redundant data versions in order to &nbsp;avoid physical copy; it just rearranges the references the KeyValue objects. Eager compaction, on the contrary, filters out the duplicates. This comes at the cost of extra compute and data migration - for example, with MSLAB storage the surviving cells are copied to the newly created MSLAB(s). The compaction overhead pays off when the data is highly redundant. Future implementations of compaction may automate the choice between the basic and eager compaction policies. For example, the algorithm might try eager compaction once in awhile, and schedule the next compaction based on the value delivered (i.e., fraction of data eliminated). Such an approach could relieve the system administrator from deciding a-priori, and adapt to changing access patterns. Summary In this blog post, we covered Accordion&rsquo;s basic principles, configuration, performance gains, and some details of the in-memory compaction algorithms. The next post will focus on system internals for HBase developers. We thank Michael Stack, Anoop Sam John and Ramkrishna Vasudevan for their continuous support that made this project happen. &lt;/p&gt; &lt;/span&gt;&lt;/div&gt; &lt;/span&gt;&lt;/div&gt;","headline":"Accordion: HBase Breathes with In-Memory Compaction","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/hbase/entry/accordion-hbase-breathes-with-in"},"url":"http://localhost:4000/hbase/entry/accordion-hbase-breathes-with-in"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Accordion: HBase Breathes with In-Memory Compaction</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-03-02T21:41:22-05:00" itemprop="datePublished">Mar 2, 2019
      </time>â€¢ <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Michael Stack", "login"=>"stack", "email"=>"stack@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <div><span id="docs-internal-guid-18fc9a22-5500-351a-ef2a-c8763289527b"> </p>
<p dir="ltr"><em>by Anastasia Braginsky (HBase Committer), Eshcar Hillel (HBase Committer) and Edward Bortnikov (Contributor) of Yahoo! Research</em></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Modern products powered by HBase exhibit ever-increasing &nbsp;expectations from its read and write performance. Ideally, HBase applications would like to enjoy the speed of in-memory databases without giving up on the reliable persistent storage guarantees. We introduce a new algorithm in HBase 2.0, named Accordion, which takes a significant step towards this goal. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">HBase partitions the data into </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">regions</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> controlled by a cluster of </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">RegionServer</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">&rsquo;s</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The internal (vertical) scalability of </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">RegionServer</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> is crucial for end-user performance as well as for the overall system utilization. Accordion improves the RegionServer scalability via a better use of RAM. It accommodates more data in memory and writes to disk less frequently. This manifests in multiple desirable phenomena. First, HBase&rsquo;s </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">disk</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"> </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">occupancy</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> and </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">write amplification</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> are </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">reduced</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. Second, more reads and writes get served from RAM, and less are stalled by disk I/O - in other words, HBase&rsquo;s </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">performance</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> is </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">increased</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. Traditionally, these different metrics were considered at odds, and tuned at each other&rsquo;s expense. With Accordion, they all get improved simultaneously. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Accordion is inspired by the </span><a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree"><span style="background-color: transparent; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">Log-Structured-Merge (LSM) tree</span></a><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> design pattern that governs the HBase storage organization. An HBase region is stored as a sequence of searchable key-value maps. The topmost is a mutable in-memory store, called </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">MemStore</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, which absorbs the recent write (</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">put</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">) operations. The rest are immutable HDFS files, called </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">HFile</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">s. Once a MemStore overflows, it is flushed to disk, creating a new HFile. HBase adopts the </span><a href="https://blogs.apache.org/hbase/entry/apache_hbase_internals_locking_and"><span style="background-color: transparent; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">multi-versioned concurrency control</span></a><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, that is, MemStore stores all data modifications as separate versions. Multiple versions of one key may therefore reside in MemStore and the HFile tier. A read (</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">get</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">) operation, which retrieves the value by key, scans the HFile data in </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">BlockCache</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, seeking for the latest version. To reduce the number of disk accesses, HFiles are merged in the background. This process, called </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">compaction</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, removes the redundant cells and creates larger files. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">LSM trees deliver superior write performance by transforming random application-level I/O to sequential disk I/O. However, their traditional design makes no attempt to compact the in-memory data. This stems from historical reasons: LSM trees have been designed in the age when RAM was very short resource, therefore the MemStore capacity was small. With recent changes in the hardware landscape, the overall MemStore memstore managed by RegionServer can be multiple gigabytes, leaving a lot of headroom for optimization. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Accordion reapplies the LSM principle to MemStore, in order to eliminate redundancies and other overhead while the data is still in RAM. </span><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;"> </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Doing so decreases the frequency of flushes to HDFS, thereby reducing the write amplification and the overall disk footprint. With less flushes, the write operations are stalled less frequently as the MemStore overflows, therefore the write performance is improved. Less data on disk also implies less pressure on the block cache, higher hit rates, and eventually better read response times. Finally, having less disk writes also means having less compaction happening in the background, i.e., less cycles are stolen from productive (read and write) work. All in all, the effect of in-memory compaction can be envisioned as a catalyst that enables the system move faster as a whole. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Accordion currently provides two levels of in-memory compaction - </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">basic </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">and </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">eager. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The former applies generic optimizations that are good for all data update patterns. The latter is most useful for applications with high data churn, like producer-consumer queues, shopping carts, shared counters, etc. All these use cases feature frequent updates of the same keys, which generate multiple redundant versions that the algorithm takes advantage of to provide more value. On the flip side, eager optimization may incur compute overhead (more memory copies and garbage collection), which may affect response times under intensive write loads. The overhead is high if the MemStore uses on-heap </span><a href="https://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/"><span style="background-color: transparent; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">MemStore-Local Allocation Buffer (MSLAB)</span></a><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> allocation; this configuration is not advised in conjunction with eager compaction. See more details about Accordion&rsquo;s compaction algorithms in the </span><a href="https://docs.google.com/document/d/1K_8plLz0K3pmV20dsgSWwRPn1qUNMRbLmi8aJkhB7z0/edit#heading=h.g59xsnm36ghn"><span style="background-color: transparent; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">next sections</span></a><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Future implementations may tune the optimal compaction policy automatically, based on the observed workload. </span></p>
<h1 dir="ltr" style="line-height: 1.38; margin-top: 20pt; margin-bottom: 6pt; text-align: justify;"><span style="background-color: transparent; font-weight: 400; vertical-align: baseline; white-space: pre-wrap;">How To Use </span></h1>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The in-memory compaction level can be configured both globally and per column family. The supported levels are </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">none</span><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;"> </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">(legacy implementation),</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"> </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">basic</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, and </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">eager</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">By default, all tables apply </span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">basic</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> in-memory compaction. This global configuration can be overridden in </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">hbase-site.xml, </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">as follows: </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">
<property></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"><span class="Apple-tab-span" style="white-space: pre;"> </span></span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"><name>hbase.hregion.compacting.memstore.type</name></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"><span class="Apple-tab-span" style="white-space: pre;"> </span></span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"><value><none|basic|eager></value></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"> </property></span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The level can also be configured in the HBase shell per column family, as follows: &nbsp;</span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">create &lsquo;<br />
<tablename>&rsquo;, </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">{NAME => &lsquo;<cfname>&rsquo;, IN_MEMORY_COMPACTION => &lsquo;</span><span style="background-color: transparent; font-weight: 700; font-style: italic; vertical-align: baseline; white-space: pre-wrap;"><NONE|BASIC|EAGER>&rsquo;</span><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">}</span></p>
<h1 dir="ltr" style="line-height: 1.38; margin-top: 20pt; margin-bottom: 6pt; text-align: justify;"><span style="background-color: transparent; font-weight: 400; vertical-align: baseline; white-space: pre-wrap;">Performance Gains, or Why You Should Care</span></h1>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">We stress-tested HBase extensively via the popular Yahoo Cloud Service Benchmark (</span><a href="https://github.com/brianfrankcooper/YCSB"><span style="background-color: transparent; text-decoration: underline; vertical-align: baseline; white-space: pre-wrap;">YCSB</span></a><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">). Our experiments used 100-200 GB datasets, and exercised a variety of representative workloads. The results demonstrate significant performance gains delivered by Accordion. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Heavy-tailed (Zipf) distribution. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The first experiment exercises a workload in which the key popularities follow the Zipf distribution that arises in most of the real-life scenarios. In this context, when 100% of the operations are writes, Accordion achieves up to 30% reduction of write amplification, 20% increase of write throughput, and 22% reduction of GC. When 50% of the operations are reads, the tail read latency is reduced by 12%.</span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Uniform distribution. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The second experiment exercises a workload in which all keys are equally popular. In this context, under 100% writes, Accordion delivers up to 25% reduction of write amplification, 50% increase of write throughput, and 36% reduction of GC. The tail read latencies are not impacted (which is expected, due to complete lack of locality). </span></p>
<h1 dir="ltr" style="line-height: 1.38; margin-top: 20pt; margin-bottom: 6pt; text-align: justify;"><span style="background-color: transparent; font-weight: 400; vertical-align: baseline; white-space: pre-wrap;">How Accordion Works</span></h1>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">High Level Design. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Accordion introduces </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">CompactingMemStore</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> - a MemStore implementation that applies compaction internally. Contrast to the default MemStore, which maintains all data in one monolithic data structure, Accordion manages it as a sequence of </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">segments</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. The youngest segment, called </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">active</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, is mutable; it absorbs the put operations. Upon overflow (by default, 32MB - 25% of the MemStore size bound), the active segment is moved to an in-memory </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">pipeline</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">, and becomes immutable. We call this </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">in-memory flush.</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> Get operations scan through these segments and the HFiles (the latter are accessed via the block cache, as usual in HBase). </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">CompactingMemStore may merge multiple immutable segments in the background from time to time, creating larger and leaner segments. The pipeline is therefore &ldquo;breathing&rdquo; (expanding and contracting), similar to accordion bellows. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">When RegionServer decides to flush one or more MemStore&rsquo;s to disk to free up memory, it considers the CompactingMemStore&rsquo;s after the rest that have overflown. The rationale is to prolong the lifetime of MemStore&rsquo;s that manage their memory efficiently, in order to reduce the overall I/O. When such a flush does happen, all pipeline segments are moved to a composite </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">snapshot, </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">&nbsp;merged, and streamed to a new HFile. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Figure 1 illustrates the structure of CompactingMemStore versus the traditional design. </span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"><img src="https://docs.google.com/drawings/d/sIGSDahHrdGlXCzoTB8l63g/image?w=593&amp;h=305&amp;rev=1&amp;ac=1" style="transform: rotate(0rad);" width="593" height="305" /></span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: center;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Figure 1. CompactingMemStore vs DefaultMemStore</span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Segment Structure.</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> Similarly to the default MemStore, CompactingMemStore maintains an index on top of cell storage, to allow fast search by key. Traditionally, this index was implemented as a Java skiplist &nbsp;(</span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">ConcurrentSkipListMap</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">) - a dynamic but wasteful data structure that manages a lot of small objects. CompactingMemStore uses a space-efficient flat layout for immutable segment indexes. This universal optimization helps all compaction policies reduce the RAM overhead, even when the data has little-to-none redundancies. Once a segment is added to the pipeline, the store serializes its index into a sorted array named </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">CellArrayMap </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">that is amenable to fast binary search. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">CellArrayMap supports both direct allocation of cells from the Java heap and custom allocation from MSLAB&rsquo;s - either on-heap or off-heap. The implementation differences are abstracted away via the helper KeyValue objects that are referenced from the index (Figure 2). CellArrayMap itself is always allocated on-heap</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.2; margin-top: 0pt; margin-bottom: 0pt; text-align: center;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"><img src="https://docs.google.com/drawings/d/sDuPuSo0bZpBddUDwNxliAg/image?w=499&amp;h=224&amp;rev=1&amp;ac=1" style="transform: rotate(0rad);" width="499" height="224" /></span></p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: center;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Figure 2. Immutable segment with a flat CellArrayMap index and MSLAB cell storage.</span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; font-weight: 700; vertical-align: baseline; white-space: pre-wrap;">Compaction Algorithms. </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">The in-memory compaction algorithms maintains a single flat index on top of the pipelined segments. This saves space, especially when the data items are small, and therefore pushes the disk flush further off away in time. A single index allows searching in one place, therefore bounding the tail read latency. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">When an active segment is flushed to memory, it is queued to the compaction pipeline, and a background </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">merge</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> task is immediately scheduled. The latter simultaneously scans all the segments in the pipeline (similarly to on-disk compaction) and merges their indexes into one. The differences between the </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">basic </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">and </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">eager </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">compaction policies manifest in how they handle the cell data. </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">Basic </span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">compaction does not eliminate the redundant data versions in order to &nbsp;avoid physical copy; it just rearranges the references the KeyValue objects. </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">Eager</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> compaction, on the contrary, filters out the duplicates. This comes at the cost of extra compute and data migration - for example, with MSLAB storage the surviving cells are copied to the newly created MSLAB(s). The compaction overhead pays off when the data is highly redundant. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">Future implementations of compaction may automate the choice between the </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">basic</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> and </span><span style="background-color: transparent; font-style: italic; vertical-align: baseline; white-space: pre-wrap;">eager</span><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"> compaction policies. For example, the algorithm might try eager compaction once in awhile, and schedule the next compaction based on the value delivered (i.e., fraction of data eliminated). Such an approach could relieve the system administrator from deciding a-priori, and adapt to changing access patterns. </span></p>
<h1 dir="ltr" style="line-height: 1.38; margin-top: 20pt; margin-bottom: 6pt;"><span style="background-color: transparent; font-weight: 400; vertical-align: baseline; white-space: pre-wrap;">Summary</span></h1>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;">In this blog post, we covered Accordion&rsquo;s basic principles, configuration, performance gains, and some details of the in-memory compaction algorithms. <a href="https://blogs.apache.org/hbase/entry/accordion-developer-view-of-in">The next post</a> will focus on system internals for HBase developers. </span></p>
<p> </p>
<p dir="ltr" style="line-height: 1.38; margin-top: 0pt; margin-bottom: 0pt; text-align: justify;"><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"><em>We thank Michael Stack, Anoop Sam John and Ramkrishna Vasudevan for their continuous support that made this project happen. </em></span></p>
<div><span style="background-color: transparent; vertical-align: baseline; white-space: pre-wrap;"></p>
<p></span></div>
<p></span></div>

  </div><a class="u-url" href="/hbase/entry/accordion-hbase-breathes-with-in" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
