<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Apache MXNet 1.4 is available! | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Apache MXNet 1.4 is available!" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Apache MXNet 1.4 is available! Today the Apache MXNet community is excited to announce the 1.4.0 release of the Apache MXNet deep learning framework. We would like to thank the Apache MXNet community for all their contributions towards this power packed v1.4 release. This release brings Java bindings for inference, Julia bindings, experimental control flow operators, JVM memory management, graph optimization and quantizations, and many more features and under-the-hood performance enhancements and usability improvements. Check out the full release highlights on MXNet v1.4 release notes. Java Inference APIs This release includes new Java Inference APIs which offer easy to use, idiomatic high level APIs for performing predictions in Java with deep learning models trained using MXNet. This simplifies production deployment of Apache MXNet models for enterprise systems that run on Java. More details are available in this blog post which introduces how to use the new Java Inference APIs. Julia API The Julia package brings flexible and efficient GPU computing and state-of-the-art deep learning to Julia. Some highlights of the Julia package include efficient tensor computation across multiple devices including multiple CPUs, GPUs and distributed server nodes. Control Flow Operators (experimental) With the control flow operators variable dynamic neural network graphs can be turned into optimized static computation graphs. The optimized graph can greatly improve the speed for training and inference for dynamic models. To learn more details about these operators, check out the Control Flow Operators tutorial. Automated JVM Memory Management As the name suggests, the memory management feature provides an automated way for managing native memory when using the JVM language bindings of MXNet. Developers now get a seamless memory management system for managing both CPU and GPU memory footprint without any degradation in performance. More details on how to use can be found in this README file. MXNet Horovod Integration Apache MXNet now supports distributed training using Horovod framework. Horovod is an open source distributed framework created at Uber. It leverages efficient inter-GPU communication to distribute and aggregate model parameters across multiple workers thus allowing efficient use of network bandwidth and scaling of training of deep learning models. To learn more about MXNet-Horovod integration, check out this blog. Subgraph API Subgraph API empowers Apache MXNet to integrate different kinds of backend libraries such as TVM, MKLDNN, TensorRT, Intel nGraph and many more. Enhanced integration with different backend libraries provides MXNet with a significant performance boost, by optimizing the execution of graph by breaking it into smaller components. Check out integrating with external backend libraries to learn more. MKLDNN : Graph Optimization and Quantization MKLDNN takes advantage of the MXNet Subgraph API to implement graph optimizations in the form of operator fusion for inference. Fusions such as Convolution + ReLU and Batch Normalization Folding, provide a great boost in the speeds for inference. Quantization allows the use of reduced precision (INT8), which reduces memory usage and improves inference time without a significant loss in accuracy. On models such as ResNet50, Inception-BN, MobileNet the observed accuracy loss was less than 0.5 %. More details on this project are available on the MXNet Graph Optimization and Quantization page. Other Maintenance improvements : The community fixed 81 bugs, improving MXNet&#39;s stability and reliability. The community also addressed over 55 documentation issues, improving the user experience for using MXNet and earning trust of the users. Getting started with MXNet Getting started with MXNet is simple, visit the install page to get started. To learn more about MXNet Gluon interface and deep learning, you can follow our 60-minute crash course, and then later complete this comprehensive set of tutorials, which covers everything from an introduction to deep learning to how to implement cutting-edge neural network models. You can check out other related MXNet tutorials, MXNet blog posts and MXNet YouTube channel. Have fun with MXNet 1.4.0! Acknowledgements: We would like to thank everyone from the Apache MXNet community for their contributions to the 1.4.0 release." />
<meta property="og:description" content="Apache MXNet 1.4 is available! Today the Apache MXNet community is excited to announce the 1.4.0 release of the Apache MXNet deep learning framework. We would like to thank the Apache MXNet community for all their contributions towards this power packed v1.4 release. This release brings Java bindings for inference, Julia bindings, experimental control flow operators, JVM memory management, graph optimization and quantizations, and many more features and under-the-hood performance enhancements and usability improvements. Check out the full release highlights on MXNet v1.4 release notes. Java Inference APIs This release includes new Java Inference APIs which offer easy to use, idiomatic high level APIs for performing predictions in Java with deep learning models trained using MXNet. This simplifies production deployment of Apache MXNet models for enterprise systems that run on Java. More details are available in this blog post which introduces how to use the new Java Inference APIs. Julia API The Julia package brings flexible and efficient GPU computing and state-of-the-art deep learning to Julia. Some highlights of the Julia package include efficient tensor computation across multiple devices including multiple CPUs, GPUs and distributed server nodes. Control Flow Operators (experimental) With the control flow operators variable dynamic neural network graphs can be turned into optimized static computation graphs. The optimized graph can greatly improve the speed for training and inference for dynamic models. To learn more details about these operators, check out the Control Flow Operators tutorial. Automated JVM Memory Management As the name suggests, the memory management feature provides an automated way for managing native memory when using the JVM language bindings of MXNet. Developers now get a seamless memory management system for managing both CPU and GPU memory footprint without any degradation in performance. More details on how to use can be found in this README file. MXNet Horovod Integration Apache MXNet now supports distributed training using Horovod framework. Horovod is an open source distributed framework created at Uber. It leverages efficient inter-GPU communication to distribute and aggregate model parameters across multiple workers thus allowing efficient use of network bandwidth and scaling of training of deep learning models. To learn more about MXNet-Horovod integration, check out this blog. Subgraph API Subgraph API empowers Apache MXNet to integrate different kinds of backend libraries such as TVM, MKLDNN, TensorRT, Intel nGraph and many more. Enhanced integration with different backend libraries provides MXNet with a significant performance boost, by optimizing the execution of graph by breaking it into smaller components. Check out integrating with external backend libraries to learn more. MKLDNN : Graph Optimization and Quantization MKLDNN takes advantage of the MXNet Subgraph API to implement graph optimizations in the form of operator fusion for inference. Fusions such as Convolution + ReLU and Batch Normalization Folding, provide a great boost in the speeds for inference. Quantization allows the use of reduced precision (INT8), which reduces memory usage and improves inference time without a significant loss in accuracy. On models such as ResNet50, Inception-BN, MobileNet the observed accuracy loss was less than 0.5 %. More details on this project are available on the MXNet Graph Optimization and Quantization page. Other Maintenance improvements : The community fixed 81 bugs, improving MXNet&#39;s stability and reliability. The community also addressed over 55 documentation issues, improving the user experience for using MXNet and earning trust of the users. Getting started with MXNet Getting started with MXNet is simple, visit the install page to get started. To learn more about MXNet Gluon interface and deep learning, you can follow our 60-minute crash course, and then later complete this comprehensive set of tutorials, which covers everything from an introduction to deep learning to how to implement cutting-edge neural network models. You can check out other related MXNet tutorials, MXNet blog posts and MXNet YouTube channel. Have fun with MXNet 1.4.0! Acknowledgements: We would like to thank everyone from the Apache MXNet community for their contributions to the 1.4.0 release." />
<link rel="canonical" href="http://localhost:4000/mxnet/entry/apache-mxnet-1-4-is1" />
<meta property="og:url" content="http://localhost:4000/mxnet/entry/apache-mxnet-1-4-is1" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-03-04T22:41:40-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Apache MXNet 1.4 is available!" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2019-03-04T22:41:40-05:00","datePublished":"2019-03-04T22:41:40-05:00","description":"Apache MXNet 1.4 is available! Today the Apache MXNet community is excited to announce the 1.4.0 release of the Apache MXNet deep learning framework. We would like to thank the Apache MXNet community for all their contributions towards this power packed v1.4 release. This release brings Java bindings for inference, Julia bindings, experimental control flow operators, JVM memory management, graph optimization and quantizations, and many more features and under-the-hood performance enhancements and usability improvements. Check out the full release highlights on MXNet v1.4 release notes. Java Inference APIs This release includes new Java Inference APIs which offer easy to use, idiomatic high level APIs for performing predictions in Java with deep learning models trained using MXNet. This simplifies production deployment of Apache MXNet models for enterprise systems that run on Java. More details are available in this blog post which introduces how to use the new Java Inference APIs. Julia API The Julia package brings flexible and efficient GPU computing and state-of-the-art deep learning to Julia. Some highlights of the Julia package include efficient tensor computation across multiple devices including multiple CPUs, GPUs and distributed server nodes. Control Flow Operators (experimental) With the control flow operators variable dynamic neural network graphs can be turned into optimized static computation graphs. The optimized graph can greatly improve the speed for training and inference for dynamic models. To learn more details about these operators, check out the Control Flow Operators tutorial. Automated JVM Memory Management As the name suggests, the memory management feature provides an automated way for managing native memory when using the JVM language bindings of MXNet. Developers now get a seamless memory management system for managing both CPU and GPU memory footprint without any degradation in performance. More details on how to use can be found in this README file. MXNet Horovod Integration Apache MXNet now supports distributed training using Horovod framework. Horovod is an open source distributed framework created at Uber. It leverages efficient inter-GPU communication to distribute and aggregate model parameters across multiple workers thus allowing efficient use of network bandwidth and scaling of training of deep learning models. To learn more about MXNet-Horovod integration, check out this blog. Subgraph API Subgraph API empowers Apache MXNet to integrate different kinds of backend libraries such as TVM, MKLDNN, TensorRT, Intel nGraph and many more. Enhanced integration with different backend libraries provides MXNet with a significant performance boost, by optimizing the execution of graph by breaking it into smaller components. Check out integrating with external backend libraries to learn more. MKLDNN : Graph Optimization and Quantization MKLDNN takes advantage of the MXNet Subgraph API to implement graph optimizations in the form of operator fusion for inference. Fusions such as Convolution + ReLU and Batch Normalization Folding, provide a great boost in the speeds for inference. Quantization allows the use of reduced precision (INT8), which reduces memory usage and improves inference time without a significant loss in accuracy. On models such as ResNet50, Inception-BN, MobileNet the observed accuracy loss was less than 0.5 %. More details on this project are available on the MXNet Graph Optimization and Quantization page. Other Maintenance improvements : The community fixed 81 bugs, improving MXNet&#39;s stability and reliability. The community also addressed over 55 documentation issues, improving the user experience for using MXNet and earning trust of the users. Getting started with MXNet Getting started with MXNet is simple, visit the install page to get started. To learn more about MXNet Gluon interface and deep learning, you can follow our 60-minute crash course, and then later complete this comprehensive set of tutorials, which covers everything from an introduction to deep learning to how to implement cutting-edge neural network models. You can check out other related MXNet tutorials, MXNet blog posts and MXNet YouTube channel. Have fun with MXNet 1.4.0! Acknowledgements: We would like to thank everyone from the Apache MXNet community for their contributions to the 1.4.0 release.","headline":"Apache MXNet 1.4 is available!","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/mxnet/entry/apache-mxnet-1-4-is1"},"url":"http://localhost:4000/mxnet/entry/apache-mxnet-1-4-is1"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Apache MXNet 1.4 is available!</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-03-04T22:41:40-05:00" itemprop="datePublished">Mar 4, 2019
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Qing Lan", "login"=>"lanking", "email"=>"lanking520@live.com"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1><a id="Apache_MXNet_14_is_available_0"></a>Apache MXNet 1.4 is available!</h1>
<p>Today the Apache MXNet community is excited to announce the 1.4.0 release of the Apache MXNet deep learning framework. We would like to thank the Apache MXNet community for all their contributions towards this power packed v1.4 release.</p>
<p>This release brings Java bindings for inference, Julia bindings, experimental control flow operators, JVM memory management, graph optimization and quantizations, and many more features and under-the-hood performance enhancements and usability improvements. Check out the full release highlights on <a href="https://github.com/apache/incubator-mxnet/releases/tag/1.4.0">MXNet v1.4 release notes</a>.</p>
<p><img src="https://cdn-images-1.medium.com/max/300/1*rKZEfsoyvreuyQZKzubF5A.png" alt="mxnet-v-1-4-0.png" height=100 width=100></p>
<h3><a id="Java_Inference_APIs_10"></a>Java Inference APIs</h3>
<p>This release includes new Java Inference APIs which offer easy to use, idiomatic high level APIs for performing predictions in Java with deep learning models trained using MXNet. This simplifies production deployment of Apache MXNet models for enterprise systems that run on Java. More details are available in <a href="https://medium.com/apache-mxnet/introducing-java-apis-for-deep-learning-inference-with-apache-mxnet-8406a698fa5a">this blog post</a> which introduces how to use the new Java Inference APIs.</p>
<h3><a id="Julia_API_14"></a>Julia API</h3>
<p>The <a href="https://github.com/apache/incubator-mxnet/tree/master/julia">Julia package</a> brings flexible and efficient GPU computing and state-of-the-art deep learning to Julia. Some highlights of the Julia package include efficient tensor computation across multiple devices including multiple CPUs, GPUs and distributed server nodes.</p>
<h3><a id="Control_Flow_Operators_experimental_18"></a>Control Flow Operators (experimental)</h3>
<p>With the control flow operators variable dynamic neural network graphs can be turned into optimized static computation graphs. The optimized graph can greatly improve the speed for training and inference for dynamic models.<br />
To learn more details about these operators, check out the <a href="https://mxnet.incubator.apache.org/versions/master/tutorials/control_flow/ControlFlowTutorial.html">Control Flow Operators tutorial</a>.</p>
<h3><a id="Automated_JVM_Memory_Management_23"></a>Automated JVM Memory Management</h3>
<p>As the name suggests, the memory management feature provides an automated way for managing native memory when using the JVM language bindings of MXNet. Developers now get a seamless memory management system for managing both CPU and GPU memory footprint without any degradation in performance. More details on how to use can be found in this <a href="https://github.com/apache/incubator-mxnet/blob/master/scala-package/memory-management.md">README file</a>.</p>
<h3><a id="MXNet_Horovod_Integration_25"></a>MXNet Horovod Integration</h3>
<p>
Apache MXNet now supports distributed training using Horovod framework. <a href="https://github.com/horovod/horovod">Horovod</a> is an open source distributed framework created at Uber. It leverages efficient inter-GPU communication to distribute and aggregate model parameters across multiple workers thus allowing efficient use of network bandwidth and scaling of training of deep learning models. To learn more about MXNet-Horovod integration, check out this <a href="https://eng.uber.com/horovod-pyspark-apache-mxnet-support/">blog</a>. </p>
<h3><a id="Subgraph_API_27"></a>Subgraph API</h3>
<p>Subgraph API empowers Apache MXNet to integrate different kinds of backend libraries such as TVM, MKLDNN, TensorRT, Intel nGraph and many more. Enhanced integration with different backend libraries provides MXNet with a significant performance boost, by optimizing the execution of graph by breaking it into smaller components. Check out <a href="https://cwiki.apache.org/confluence/display/MXNET/Unified+integration+with+external+backend+libraries">integrating with external backend libraries</a> to learn more.</p>
<h3><a id="MKLDNN _Graph_Optimization_and_Quantization_31"></a>MKLDNN : Graph Optimization and Quantization</h3>
<p>MKLDNN takes advantage of the MXNet Subgraph API to implement graph optimizations in the form of operator fusion for inference. Fusions such as Convolution + ReLU and Batch Normalization Folding, provide a great boost in the speeds for inference. <br><br />
Quantization allows the use of reduced precision (INT8), which reduces memory usage and improves inference time without a significant loss in accuracy. On models such as ResNet50, Inception-BN, MobileNet the observed accuracy loss was less than 0.5 %. More details on this project are available on the <a href="https://cwiki.apache.org/confluence/display/MXNET/MXNet+Graph+Optimization+and+Quantization+based+on+subgraph+and+MKL-DNN">MXNet Graph Optimization and Quantization page</a>.</p>
<h3><a id="Other_Maintenance_improvements _36"></a>Other Maintenance improvements :</h3>
<ol>
<li>The community fixed <a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+(incubating)+1.4.0+Release+Notes#ApacheMXNet(incubating)1.4.0ReleaseNotes-Bugfixes">81 bugs</a>, improving MXNet&#39;s stability and reliability.</li>
<li>The community also addressed over <a href="https://cwiki.apache.org/confluence/display/MXNET/Apache+MXNet+(incubating)+1.4.0+Release+Notes#ApacheMXNet(incubating)1.4.0ReleaseNotes-Documentation">55 documentation issues</a>, improving the user experience for using MXNet and earning trust of the users.</li>
</ol>
<h3><a id="Getting_started_with MXNet_41"></a>Getting started with MXNet</h3>
<p>Getting started with MXNet is simple, visit the <a href="https://mxnet.incubator.apache.org/versions/master/install/index.html?platform=Linux&language=Python&processor=CPU">install page</a> to get started. To learn more about MXNet Gluon interface and deep learning, you can follow our <a href="https://medium.com/apache-mxnet/mxnet-gluon-in-60-minutes-3d49eccaf266">60-minute crash course</a>, and then later complete this <a href="https://gluon.mxnet.io">comprehensive set of tutorials</a>, which covers everything from an introduction to deep learning to how to implement cutting-edge neural network models. You can check out other related <a href="https://mxnet.incubator.apache.org/versions/master/tutorials/index.html">MXNet tutorials</a>, <a href="https://medium.com/apache-mxnet">MXNet blog posts</a> and <a href="https://www.youtube.com/channel/UCQua2ZAkbr_Shsgfk1LCy6A">MXNet YouTube channel</a>.</p>
<p>Have fun with MXNet 1.4.0!</p>
<h3><a id="Acknowledgements_47"></a>Acknowledgements:</h3>
<p>We would like to thank everyone from the Apache MXNet community for their contributions to the 1.4.0 release.</p>

  </div><a class="u-url" href="/mxnet/entry/apache-mxnet-1-4-is1" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
