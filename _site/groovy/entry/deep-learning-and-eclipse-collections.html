<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Deep Learning and Eclipse Collections | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Deep Learning and Eclipse Collections" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="DeepLearning4J and Eclipse Collections revisited In previous blogs, we have covered Eclipse Collections and Deep Learning. Recently, a couple of the highly recommended katas for Eclipse Collections have been revamped to include &quot;pet&quot; and &quot;fruit&quot; emojis for a little bit of extra fun. What could be better than Learning Eclipse Collections? Deep Learning and Eclipse Collections of course! First, we create a PetType enum with the emoji toString, and then Pet and Person records. We&#39;ll populate a people list as is done in the kata. The full details are in the repo. Let&#39;s use a GQuery expression to explore the pre-populated list: println GQ { from p in people select p.fullName, p.pets} The result is: Now let&#39;s duplicate the assertion from the getCountsByPetType test in exercise3 which checks pet counts: As we expect, it passes. Now, for a bit of fun, we will use a neural network trained to detect cat and dog images and apply it to our emojis. We&#39;ll follow the process described here. It uses DeepLearning4J to train and then use a model. The images used to train the model were real cat and dog images, not emojis, so we aren&#39;t expecting our model to be super accurate. The first attempt was to write the emojis into swing JLabel components and then save using a buffered image. This lead to poor looking images: And consequently, poor image inference. Recent JDK versions on some platforms might do better but we gave up on this approach. Instead, emoji image files from the Noto Color Emoji font were used and saved under the pet type in the resources folder. These look much nicer: Here is the code which makes use of those saved images to detect the animal types (note the use of type aliasing since we have two PetType classes; we rename one to PT): import ramo.klevis.ml.vg16.PetType as PTimport ramo.klevis.ml.vg16.VG16ForCatvar vg16ForCat = new VG16ForCat().tap{ loadModel() }var results = []people.each{ p -&gt; results &lt;&lt; p.pets.collect { pet -&gt; var file = new File(&quot;resources/${pet.type.name()}.png&quot;) PT petType = vg16ForCat.detectCat(file, 0.675d) var desc = switch(petType) { case PT.CAT -&gt; &#39;is a cat&#39; case PT.DOG -&gt; &#39;is a dog&#39; default -&gt; &#39;is unknown&#39; } &quot;$pet.name $desc&quot; }}println results.flatten().join(&#39;\n&#39;) Note that the model exceeds the maximum allowable size for normal github repos, so you should create it following the original repo instructions and then store the resulting model.zip in the resources folder. When we run the script, we get the following output: [main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [CpuBackend] backend ... [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [OPENBLAS] ... ============================================================================================================================================ VertexName (VertexType) nIn,nOut TotalParams ParamsShape Vertex Inputs ============================================================================================================================================ input_1 (InputVertex) -,- - - - block1_conv1 (Frozen ConvolutionLayer) 3,64 1792 b:{1,64}, W:{64,3,3,3} [input_1] block1_conv2 (Frozen ConvolutionLayer) 64,64 36928 b:{1,64}, W:{64,64,3,3} [block1_conv1] block1_pool (Frozen SubsamplingLayer) -,- 0 - [block1_conv2] block2_conv1 (Frozen ConvolutionLayer) 64,128 73856 b:{1,128}, W:{128,64,3,3} [block1_pool] block2_conv2 (Frozen ConvolutionLayer) 128,128 147584 b:{1,128}, W:{128,128,3,3} [block2_conv1] block2_pool (Frozen SubsamplingLayer) -,- 0 - [block2_conv2] block3_conv1 (Frozen ConvolutionLayer) 128,256 295168 b:{1,256}, W:{256,128,3,3} [block2_pool] block3_conv2 (Frozen ConvolutionLayer) 256,256 590080 b:{1,256}, W:{256,256,3,3} [block3_conv1] block3_conv3 (Frozen ConvolutionLayer) 256,256 590080 b:{1,256}, W:{256,256,3,3} [block3_conv2] block3_pool (Frozen SubsamplingLayer) -,- 0 - [block3_conv3] block4_conv1 (Frozen ConvolutionLayer) 256,512 1180160 b:{1,512}, W:{512,256,3,3} [block3_pool] block4_conv2 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block4_conv1] block4_conv3 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block4_conv2] block4_pool (Frozen SubsamplingLayer) -,- 0 - [block4_conv3] block5_conv1 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block4_pool] block5_conv2 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block5_conv1] block5_conv3 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block5_conv2] block5_pool (Frozen SubsamplingLayer) -,- 0 - [block5_conv3] flatten (PreprocessorVertex) -,- - - [block5_pool] fc1 (Frozen DenseLayer) 25088,4096 102764544 b:{1,4096}, W:{25088,4096} [flatten] fc2 (Frozen DenseLayer) 4096,4096 16781312 b:{1,4096}, W:{4096,4096} [fc1] predictions (OutputLayer) 4096,2 8194 b:{1,2}, W:{4096,2} [fc2] -------------------------------------------------------------------------------------------------------------------------------------------- Total Parameters: 134268738 Trainable Parameters: 8194 Frozen Parameters: 134260544 ============================================================================================================================================ ... Tabby is a cat Dolly is a cat Spot is a dog Spike is a dog Serpy is a cat Tweety is unknown Speedy is a dog Fuzzy is unknown Wuzzy is unknown As we can see, it correctly predicted the cats (Tabby and Dolly) and dogs (Spot and Spike) but incorrectly thought a snake (Serpy) was a cat and a turtle (Speedy) was a dog. Given the lack of detail in the emoji images compared to the training images, this lack of accuracy isn&#39;t unexpected. We could certainly use better images or train our model differently if we wanted better results but it is fun to see our model not doing too badly even with emojis!" />
<meta property="og:description" content="DeepLearning4J and Eclipse Collections revisited In previous blogs, we have covered Eclipse Collections and Deep Learning. Recently, a couple of the highly recommended katas for Eclipse Collections have been revamped to include &quot;pet&quot; and &quot;fruit&quot; emojis for a little bit of extra fun. What could be better than Learning Eclipse Collections? Deep Learning and Eclipse Collections of course! First, we create a PetType enum with the emoji toString, and then Pet and Person records. We&#39;ll populate a people list as is done in the kata. The full details are in the repo. Let&#39;s use a GQuery expression to explore the pre-populated list: println GQ { from p in people select p.fullName, p.pets} The result is: Now let&#39;s duplicate the assertion from the getCountsByPetType test in exercise3 which checks pet counts: As we expect, it passes. Now, for a bit of fun, we will use a neural network trained to detect cat and dog images and apply it to our emojis. We&#39;ll follow the process described here. It uses DeepLearning4J to train and then use a model. The images used to train the model were real cat and dog images, not emojis, so we aren&#39;t expecting our model to be super accurate. The first attempt was to write the emojis into swing JLabel components and then save using a buffered image. This lead to poor looking images: And consequently, poor image inference. Recent JDK versions on some platforms might do better but we gave up on this approach. Instead, emoji image files from the Noto Color Emoji font were used and saved under the pet type in the resources folder. These look much nicer: Here is the code which makes use of those saved images to detect the animal types (note the use of type aliasing since we have two PetType classes; we rename one to PT): import ramo.klevis.ml.vg16.PetType as PTimport ramo.klevis.ml.vg16.VG16ForCatvar vg16ForCat = new VG16ForCat().tap{ loadModel() }var results = []people.each{ p -&gt; results &lt;&lt; p.pets.collect { pet -&gt; var file = new File(&quot;resources/${pet.type.name()}.png&quot;) PT petType = vg16ForCat.detectCat(file, 0.675d) var desc = switch(petType) { case PT.CAT -&gt; &#39;is a cat&#39; case PT.DOG -&gt; &#39;is a dog&#39; default -&gt; &#39;is unknown&#39; } &quot;$pet.name $desc&quot; }}println results.flatten().join(&#39;\n&#39;) Note that the model exceeds the maximum allowable size for normal github repos, so you should create it following the original repo instructions and then store the resulting model.zip in the resources folder. When we run the script, we get the following output: [main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [CpuBackend] backend ... [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [OPENBLAS] ... ============================================================================================================================================ VertexName (VertexType) nIn,nOut TotalParams ParamsShape Vertex Inputs ============================================================================================================================================ input_1 (InputVertex) -,- - - - block1_conv1 (Frozen ConvolutionLayer) 3,64 1792 b:{1,64}, W:{64,3,3,3} [input_1] block1_conv2 (Frozen ConvolutionLayer) 64,64 36928 b:{1,64}, W:{64,64,3,3} [block1_conv1] block1_pool (Frozen SubsamplingLayer) -,- 0 - [block1_conv2] block2_conv1 (Frozen ConvolutionLayer) 64,128 73856 b:{1,128}, W:{128,64,3,3} [block1_pool] block2_conv2 (Frozen ConvolutionLayer) 128,128 147584 b:{1,128}, W:{128,128,3,3} [block2_conv1] block2_pool (Frozen SubsamplingLayer) -,- 0 - [block2_conv2] block3_conv1 (Frozen ConvolutionLayer) 128,256 295168 b:{1,256}, W:{256,128,3,3} [block2_pool] block3_conv2 (Frozen ConvolutionLayer) 256,256 590080 b:{1,256}, W:{256,256,3,3} [block3_conv1] block3_conv3 (Frozen ConvolutionLayer) 256,256 590080 b:{1,256}, W:{256,256,3,3} [block3_conv2] block3_pool (Frozen SubsamplingLayer) -,- 0 - [block3_conv3] block4_conv1 (Frozen ConvolutionLayer) 256,512 1180160 b:{1,512}, W:{512,256,3,3} [block3_pool] block4_conv2 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block4_conv1] block4_conv3 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block4_conv2] block4_pool (Frozen SubsamplingLayer) -,- 0 - [block4_conv3] block5_conv1 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block4_pool] block5_conv2 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block5_conv1] block5_conv3 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block5_conv2] block5_pool (Frozen SubsamplingLayer) -,- 0 - [block5_conv3] flatten (PreprocessorVertex) -,- - - [block5_pool] fc1 (Frozen DenseLayer) 25088,4096 102764544 b:{1,4096}, W:{25088,4096} [flatten] fc2 (Frozen DenseLayer) 4096,4096 16781312 b:{1,4096}, W:{4096,4096} [fc1] predictions (OutputLayer) 4096,2 8194 b:{1,2}, W:{4096,2} [fc2] -------------------------------------------------------------------------------------------------------------------------------------------- Total Parameters: 134268738 Trainable Parameters: 8194 Frozen Parameters: 134260544 ============================================================================================================================================ ... Tabby is a cat Dolly is a cat Spot is a dog Spike is a dog Serpy is a cat Tweety is unknown Speedy is a dog Fuzzy is unknown Wuzzy is unknown As we can see, it correctly predicted the cats (Tabby and Dolly) and dogs (Spot and Spike) but incorrectly thought a snake (Serpy) was a cat and a turtle (Speedy) was a dog. Given the lack of detail in the emoji images compared to the training images, this lack of accuracy isn&#39;t unexpected. We could certainly use better images or train our model differently if we wanted better results but it is fun to see our model not doing too badly even with emojis!" />
<link rel="canonical" href="http://localhost:4000/groovy/entry/deep-learning-and-eclipse-collections" />
<meta property="og:url" content="http://localhost:4000/groovy/entry/deep-learning-and-eclipse-collections" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-10-11T10:41:58-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deep Learning and Eclipse Collections" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-10-11T10:41:58-04:00","datePublished":"2022-10-11T10:41:58-04:00","description":"DeepLearning4J and Eclipse Collections revisited In previous blogs, we have covered Eclipse Collections and Deep Learning. Recently, a couple of the highly recommended katas for Eclipse Collections have been revamped to include &quot;pet&quot; and &quot;fruit&quot; emojis for a little bit of extra fun. What could be better than Learning Eclipse Collections? Deep Learning and Eclipse Collections of course! First, we create a PetType enum with the emoji toString, and then Pet and Person records. We&#39;ll populate a people list as is done in the kata. The full details are in the repo. Let&#39;s use a GQuery expression to explore the pre-populated list: println GQ { from p in people select p.fullName, p.pets} The result is: Now let&#39;s duplicate the assertion from the getCountsByPetType test in exercise3 which checks pet counts: As we expect, it passes. Now, for a bit of fun, we will use a neural network trained to detect cat and dog images and apply it to our emojis. We&#39;ll follow the process described here. It uses DeepLearning4J to train and then use a model. The images used to train the model were real cat and dog images, not emojis, so we aren&#39;t expecting our model to be super accurate. The first attempt was to write the emojis into swing JLabel components and then save using a buffered image. This lead to poor looking images: And consequently, poor image inference. Recent JDK versions on some platforms might do better but we gave up on this approach. Instead, emoji image files from the Noto Color Emoji font were used and saved under the pet type in the resources folder. These look much nicer: Here is the code which makes use of those saved images to detect the animal types (note the use of type aliasing since we have two PetType classes; we rename one to PT): import ramo.klevis.ml.vg16.PetType as PTimport ramo.klevis.ml.vg16.VG16ForCatvar vg16ForCat = new VG16ForCat().tap{ loadModel() }var results = []people.each{ p -&gt; results &lt;&lt; p.pets.collect { pet -&gt; var file = new File(&quot;resources/${pet.type.name()}.png&quot;) PT petType = vg16ForCat.detectCat(file, 0.675d) var desc = switch(petType) { case PT.CAT -&gt; &#39;is a cat&#39; case PT.DOG -&gt; &#39;is a dog&#39; default -&gt; &#39;is unknown&#39; } &quot;$pet.name $desc&quot; }}println results.flatten().join(&#39;\\n&#39;) Note that the model exceeds the maximum allowable size for normal github repos, so you should create it following the original repo instructions and then store the resulting model.zip in the resources folder. When we run the script, we get the following output: [main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [CpuBackend] backend ... [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [OPENBLAS] ... ============================================================================================================================================ VertexName (VertexType) nIn,nOut TotalParams ParamsShape Vertex Inputs ============================================================================================================================================ input_1 (InputVertex) -,- - - - block1_conv1 (Frozen ConvolutionLayer) 3,64 1792 b:{1,64}, W:{64,3,3,3} [input_1] block1_conv2 (Frozen ConvolutionLayer) 64,64 36928 b:{1,64}, W:{64,64,3,3} [block1_conv1] block1_pool (Frozen SubsamplingLayer) -,- 0 - [block1_conv2] block2_conv1 (Frozen ConvolutionLayer) 64,128 73856 b:{1,128}, W:{128,64,3,3} [block1_pool] block2_conv2 (Frozen ConvolutionLayer) 128,128 147584 b:{1,128}, W:{128,128,3,3} [block2_conv1] block2_pool (Frozen SubsamplingLayer) -,- 0 - [block2_conv2] block3_conv1 (Frozen ConvolutionLayer) 128,256 295168 b:{1,256}, W:{256,128,3,3} [block2_pool] block3_conv2 (Frozen ConvolutionLayer) 256,256 590080 b:{1,256}, W:{256,256,3,3} [block3_conv1] block3_conv3 (Frozen ConvolutionLayer) 256,256 590080 b:{1,256}, W:{256,256,3,3} [block3_conv2] block3_pool (Frozen SubsamplingLayer) -,- 0 - [block3_conv3] block4_conv1 (Frozen ConvolutionLayer) 256,512 1180160 b:{1,512}, W:{512,256,3,3} [block3_pool] block4_conv2 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block4_conv1] block4_conv3 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block4_conv2] block4_pool (Frozen SubsamplingLayer) -,- 0 - [block4_conv3] block5_conv1 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block4_pool] block5_conv2 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block5_conv1] block5_conv3 (Frozen ConvolutionLayer) 512,512 2359808 b:{1,512}, W:{512,512,3,3} [block5_conv2] block5_pool (Frozen SubsamplingLayer) -,- 0 - [block5_conv3] flatten (PreprocessorVertex) -,- - - [block5_pool] fc1 (Frozen DenseLayer) 25088,4096 102764544 b:{1,4096}, W:{25088,4096} [flatten] fc2 (Frozen DenseLayer) 4096,4096 16781312 b:{1,4096}, W:{4096,4096} [fc1] predictions (OutputLayer) 4096,2 8194 b:{1,2}, W:{4096,2} [fc2] -------------------------------------------------------------------------------------------------------------------------------------------- Total Parameters: 134268738 Trainable Parameters: 8194 Frozen Parameters: 134260544 ============================================================================================================================================ ... Tabby is a cat Dolly is a cat Spot is a dog Spike is a dog Serpy is a cat Tweety is unknown Speedy is a dog Fuzzy is unknown Wuzzy is unknown As we can see, it correctly predicted the cats (Tabby and Dolly) and dogs (Spot and Spike) but incorrectly thought a snake (Serpy) was a cat and a turtle (Speedy) was a dog. Given the lack of detail in the emoji images compared to the training images, this lack of accuracy isn&#39;t unexpected. We could certainly use better images or train our model differently if we wanted better results but it is fun to see our model not doing too badly even with emojis!","headline":"Deep Learning and Eclipse Collections","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/groovy/entry/deep-learning-and-eclipse-collections"},"url":"http://localhost:4000/groovy/entry/deep-learning-and-eclipse-collections"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Deep Learning and Eclipse Collections</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-10-11T10:41:58-04:00" itemprop="datePublished">Oct 11, 2022
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Paul King", "login"=>"paulk", "email"=>"paulk@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2>DeepLearning4J and Eclipse Collections revisited</h2>
<p>In previous blogs, we have covered <a href="https://blogs.apache.org/groovy/entry/deck-of-cards-with-groovy" target="_blank">Eclipse Collections</a> and <a href="https://blogs.apache.org/groovy/entry/detecting-objects-with-groovy-the" target="_blank">Deep Learning</a>. Recently, a couple of the highly recommended katas for Eclipse Collections have been revamped to include "pet" and "fruit" emojis for a little bit of extra fun. What could be better than <i>Learning</i> Eclipse Collections? <i>Deep Learning</i> and Eclipse Collections of course!</p>
<p>First, we create a <code>PetType</code> enum with the emoji <code>toString</code>, and then <code>Pet</code> and <code>Person</code> records. We'll populate a <code>people</code> list as is done in the kata. The full details are in the <a href="https://github.com/paulk-asert/deep-learning-eclipse-collections" target="_blank">repo</a>.</p>
<p>Let's use a GQuery expression to explore the pre-populated list:</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;">println <span style="color:#9876aa;font-style:italic;">GQ </span>{<br>    <span style="color:#cc7832;">from </span>p <span style="color:#cc7832;">in </span>people<br>    <span style="color:#cc7832;">select </span>p.<span style="color:#9876aa;">fullName</span>, p.<span style="color:#9876aa;">pets<br></span>}<br></pre>
<p>The result is:</p>
<p><img src="https://blogs.apache.org/groovy/mediaresource/fb059e16-ae50-4681-8d2f-848b8f98041a" style="width:35%" alt="deep-learning-eclipse-collections pre-populated field"></p>
<p>Now let's duplicate the assertion from the <code>getCountsByPetType</code> test in exercise3 which checks pet counts:</p>
<p><img src="https://blogs.apache.org/groovy/mediaresource/1bffcd1c-67e6-41a9-9511-680fc246778e" style="width:90%" alt="2022-10-11 20_06_12-Groovy web console.png"></p>
<p>As we expect, it passes.</p>
<p>Now, for a bit of fun, we will use a neural network trained to detect cat and dog images and apply it to our emojis. We'll follow the process described <a href="http://ramok.tech/2018/01/03/java-image-cat-vs-dog-recognizer-with-deep-neural-networks/" target="_blank">here</a>. It uses DeepLearning4J to train and then use a model. The images used to train the model were real cat and dog images, not emojis, so we aren't expecting our model to be super accurate.</p>
<p>The first attempt was to write the emojis into swing JLabel components and then save using a buffered image. This lead to poor looking images:</p>
<p><img src="https://blogs.apache.org/groovy/mediaresource/80954fdc-8484-47c5-9cba-c64747e1ea5b" style="width:30%" alt="PetAsFonts.jpg"></p>
<p>And consequently, poor image inference. Recent JDK versions on some platforms might do better but we gave up on this approach.</p>
<p>Instead, emoji image files from the <a href="https://fonts.google.com/noto/specimen/Noto+Color+Emoji?preview.text=%F0%9F%98%BB%F0%9F%90%B6%F0%9F%90%B9%F0%9F%90%A2%F0%9F%90%A6%F0%9F%90%8D&amp;preview.text_type=custom" target="_blank">Noto Color Emoji</a> font were used and saved under the pet type in the resources folder. These look much nicer:</p>
<p><img src="https://blogs.apache.org/groovy/mediaresource/3513ee3e-98cf-4439-9496-20ae9d976262" style="width:50%" alt="2022-10-11 18_24_38-Noto Color Emoji - Google Fonts.png"></p>
<p>Here is the code which makes use of those saved images to detect the animal types (note the use of type aliasing since we have two <code>PetType</code> classes; we rename one to <code>PT</code>):</p>
<pre style="background-color:#2b2b2b;color:#a9b7c6;font-family:'JetBrains Mono',monospace;font-size:9.6pt;"><span style="color:#cc7832;">import </span>ramo.klevis.ml.vg16.PetType <span style="color:#cc7832;">as </span>PT<br><span style="color:#cc7832;">import </span>ramo.klevis.ml.vg16.VG16ForCat<br><br><span style="color:#cc7832;">var </span>vg16ForCat = <span style="color:#cc7832;">new </span>VG16ForCat().tap<span style="font-weight:bold;">{ </span>loadModel() <span style="font-weight:bold;">}<br></span><span style="color:#cc7832;">var </span>results = []<br>people.each<span style="font-weight:bold;">{ </span>p <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">    </span>results << p.<span style="color:#9876aa;">pets</span>.collect <span style="font-weight:bold;">{ </span>pet <span style="font-weight:bold;">-><br></span><span style="font-weight:bold;">        </span><span style="color:#cc7832;">var </span>file = <span style="color:#cc7832;">new </span>File(<span style="color:#6a8759;">"resources/</span>$<span style="font-weight:bold;">{</span>pet.<span style="color:#9876aa;">type</span>.name()<span style="font-weight:bold;">}</span><span style="color:#6a8759;">.png"</span>)<br>        PT petType = vg16ForCat.detectCat(file, <span style="color:#6897bb;">0.675d</span>)<br>        <span style="color:#cc7832;">var </span>desc = <span style="color:#cc7832;">switch</span>(petType) {<br>            <span style="color:#cc7832;">case </span>PT.<span style="color:#9876aa;font-style:italic;">CAT </span>-> <span style="color:#6a8759;">'is a cat'<br></span><span style="color:#6a8759;">            </span><span style="color:#cc7832;">case </span>PT.<span style="color:#9876aa;font-style:italic;">DOG </span>-> <span style="color:#6a8759;">'is a dog'<br></span><span style="color:#6a8759;">            </span><span style="color:#cc7832;">default </span>-> <span style="color:#6a8759;">'is unknown'<br></span><span style="color:#6a8759;">        </span>}<br>        <span style="color:#6a8759;">"</span>$pet.<span style="color:#9876aa;">name </span>$desc<span style="color:#6a8759;">"<br></span><span style="color:#6a8759;">    </span><span style="font-weight:bold;">}<br></span><span style="font-weight:bold;">}<br></span>println results.flatten().join(<span style="color:#6a8759;">'</span><span style="color:#cc7832;">\n</span><span style="color:#6a8759;">'</span>)<br></pre>
<p>Note that the model exceeds the maximum allowable size for normal github repos, so you should create it following the original repo <a href="https://github.com/klevis/CatAndDogRecognizer" target="_blank">instructions</a> and then store the resulting model.zip in the resources folder.</p>
<p>When we run the script, we get the following output:</p>
<pre><span style="color:#dd4444">
[main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [CpuBackend] backend
...
[main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [OPENBLAS]
...
============================================================================================================================================
VertexName (VertexType)                 nIn,nOut       TotalParams    ParamsShape                    Vertex Inputs
============================================================================================================================================
input_1 (InputVertex)                   -,-            -              -                              -
block1_conv1 (Frozen ConvolutionLayer)  3,64           1792           b:{1,64}, W:{64,3,3,3}         [input_1]
block1_conv2 (Frozen ConvolutionLayer)  64,64          36928          b:{1,64}, W:{64,64,3,3}        [block1_conv1]
block1_pool (Frozen SubsamplingLayer)   -,-            0              -                              [block1_conv2]
block2_conv1 (Frozen ConvolutionLayer)  64,128         73856          b:{1,128}, W:{128,64,3,3}      [block1_pool]
block2_conv2 (Frozen ConvolutionLayer)  128,128        147584         b:{1,128}, W:{128,128,3,3}     [block2_conv1]
block2_pool (Frozen SubsamplingLayer)   -,-            0              -                              [block2_conv2]
block3_conv1 (Frozen ConvolutionLayer)  128,256        295168         b:{1,256}, W:{256,128,3,3}     [block2_pool]
block3_conv2 (Frozen ConvolutionLayer)  256,256        590080         b:{1,256}, W:{256,256,3,3}     [block3_conv1]
block3_conv3 (Frozen ConvolutionLayer)  256,256        590080         b:{1,256}, W:{256,256,3,3}     [block3_conv2]
block3_pool (Frozen SubsamplingLayer)   -,-            0              -                              [block3_conv3]
block4_conv1 (Frozen ConvolutionLayer)  256,512        1180160        b:{1,512}, W:{512,256,3,3}     [block3_pool]
block4_conv2 (Frozen ConvolutionLayer)  512,512        2359808        b:{1,512}, W:{512,512,3,3}     [block4_conv1]
block4_conv3 (Frozen ConvolutionLayer)  512,512        2359808        b:{1,512}, W:{512,512,3,3}     [block4_conv2]
block4_pool (Frozen SubsamplingLayer)   -,-            0              -                              [block4_conv3]
block5_conv1 (Frozen ConvolutionLayer)  512,512        2359808        b:{1,512}, W:{512,512,3,3}     [block4_pool]
block5_conv2 (Frozen ConvolutionLayer)  512,512        2359808        b:{1,512}, W:{512,512,3,3}     [block5_conv1]
block5_conv3 (Frozen ConvolutionLayer)  512,512        2359808        b:{1,512}, W:{512,512,3,3}     [block5_conv2]
block5_pool (Frozen SubsamplingLayer)   -,-            0              -                              [block5_conv3]
flatten (PreprocessorVertex)            -,-            -              -                              [block5_pool]
fc1 (Frozen DenseLayer)                 25088,4096     102764544      b:{1,4096}, W:{25088,4096}     [flatten]
fc2 (Frozen DenseLayer)                 4096,4096      16781312       b:{1,4096}, W:{4096,4096}      [fc1]
predictions (OutputLayer)               4096,2         8194           b:{1,2}, W:{4096,2}            [fc2]
--------------------------------------------------------------------------------------------------------------------------------------------
            Total Parameters:  134268738
        Trainable Parameters:  8194
           Frozen Parameters:  134260544
============================================================================================================================================
...</span>
Tabby is a cat
Dolly is a cat
Spot is a dog
Spike is a dog
Serpy is a cat
Tweety is unknown
Speedy is a dog
Fuzzy is unknown
Wuzzy is unknown
</pre>
<p>As we can see, it correctly predicted the cats (Tabby and Dolly) and dogs (Spot and Spike) but incorrectly thought a snake (Serpy) was a cat and a turtle (Speedy) was a dog. Given the lack of detail in the emoji images compared to the training images, this lack of accuracy isn't unexpected. We could certainly use better images or train our model differently if we wanted better results but it is fun to see our model not doing too badly even with emojis!</p>

  </div><a class="u-url" href="/groovy/entry/deep-learning-and-eclipse-collections" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
