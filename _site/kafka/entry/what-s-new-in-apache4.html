<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>What’s New in Apache Kafka 2.7.0 | Blogs Archive</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="What’s New in Apache Kafka 2.7.0" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I&#39;m proud to announce the release of Apache Kafka 2.7.0 on behalf of the Apache Kafka&reg; community. The 2.7.0 release contains many new features and improvements. This blog post highlights some of the more prominent ones. Be sure to see the release notes for the full list of changes. You can also watch the release video for a summary of what&rsquo;s new. In this release, we&rsquo;ve continued steady progress toward the task of replacing ZooKeeper in Kafka with KIP-497, which adds a new inter-broker API for altering the in-sync replica (ISR). This release also provides the addition of the Core Raft Implementation as part of KIP-595. Now there is a separate &quot;raft&quot; module containing the core consensus protocol. Until integration with the controller (the broker in the Kafka cluster responsible for managing the state of partitions and replicas) is complete, there is a standalone server that you can use for testing the performance of the Raft implementation. Of course, there are additional efforts underway toward replacing Zookeeper, with seven KIPs in active development to provide support for more partitions per cluster, simpler operation, and tighter security. Tiered Storage work continues and unlocks infinite scaling and faster rebalance times via KIP-405. Kafka broker, producer, and consumer KIP-654: Aborted transaction with non-flushed data should throw a non-fatal exception When a Java client producer aborts a transaction with any non-flushed (pending) data, a fatal exception is thrown. But aborting a transaction with pending data is in fact considered a normal situation. The thrown exception should be to notify you that records aren&rsquo;t being sent, not that the application is in an unrecoverable state. KIP-654 introduces a new exception TransactionAbortedException, allowing you to retry if desired. KIP-651: Support PEM format for private keys and SSL certificates and private key Currently, Kafka only supports JKS or PKCS12 file-based key and trust stores when using SSL. While it&rsquo;s no longer a standard for email, the Privacy-Enhanced Mail (PEM) is a standard format for storing and distributing cryptographic keys and certificates. KIP-651 adds support for PEM files for key and trust stores, allowing the use of third party providers relying on the PEM format. KIP-612: Ability to limit connection creation rate on brokers Creating connections adds CPU overhead to the broker. Connection storms can come from seemingly well-behaved clients and can stop the broker from performing other useful work. But now there is now a way of enforcing broker-wide and per-listener connection creation rates. The 2.7.0 release contains the first part of KIP-612, with per-IP connections rate limits expected to come in the 2.8.0 release. KIP-599: Throttle create topic, create partition, and delete topic operations The APIs to create topics, create partitions, and delete topics are operations that have a direct impact on the overall load in the Kafka controller. To prevent a cluster from being overwhelmed due to high concurrent topic and partition creations or topic deletions, there is a new quota limiting these operations. See KIP-599 for more details. KIP-584: Versioning scheme for features Apart from broker-client compatibility (for which Kafka has a strong record to ensure they do remain compatible), there are two main questions when new features become available in Kafka: How do Kafka clients become aware of broker capabilities? How does the broker decide which features to enable? KIP-584 provides a flexible and operationally friendly solution for client discovery, feature gating, and rolling upgrades using a single restart. KIP-554: Add broker-side SCRAM configuration API With KIP-554, SCRAM credentials can be managed via the Kafka protocol and the kafka-configs tool was updated to use the newly introduced protocol APIs. This is another important step towards KIP-500 where ZooKeeper is replaced by a built in quorum. KIP-497: Add inter-broker API to alter ISR Currently, Kafka partition leader and ISR information is stored in ZooKeeper. Either the controller or a partition leader may update this information. Because either can update this state, there needs to be a mechanism for sharing this information, which can cause delays in reflecting ISR changes. The impact of these delays means that metadata requests may receive stale information. In the 2.7.0 release, there is a new AlterIsr API, which gives the controller the exclusive ability to update the state of partition leaders and ISR. The chief benefit of this new API is that metadata requests will always reflect the latest state. The addition of this API is a significant step forward in the process of removing ZooKeeper and the completion of KIP-500. For more information, see KIP-497. KIP-431: Print additional fields from records with the ConsoleConsumer Now you can print the headers on a ConsumerRecord with the ConsoleConsumer. See KIP-431 for more details. Kafka Connect KIP-632: Add DirectoryConfigProvider KIP-632 adds a DirectoryConfigProvider class to support users needing to provide secrets for keys stored in a container filesystem, such as a Kubernetes environment. Kafka Streams KIP-662: Throw exception when source topics of Kafka Streams application is deleted Today, if a user deletes the source topic of a running Kafka Streams application, the embedded consumer clients gracefully shut down. This client shutdown triggers rebalancing until all StreamThreads of the Streams application gracefully exit, leaving the application completely shut down without any chance to respond to the error. With the addition of KIP-662, when a user deletes a source topic from a running Streams application, the app throws a MissingSourceTopicException, allowing for you to react to the error. KIP-648: Renaming getter method for interactive queries KIP-648 changes the getter methods for interactive query objects to follow the Kafka format of not using the get prefix. KIP-617: Allow Kafka Streams state stores to be iterated backwards Currently, when using an iterator over a Kafka Streams state store, you can only traverse elements from oldest to newest. When iterating over a windowed state store and the user desires to return the latest N records, there is no choice but to use the inefficient approach of traversing all the oldest records before getting to the desired newer records. KIP-617 adds support for iteration over a state store in reverse. Iterating in reverse makes a latest N records retrieval much more efficient. KIP-616: Rename implicit SerDes instances in kafka-streams-scala Kafka Streams now how better Scala implicit Serdes support with KIP-616. KIP-613: Add end-to-end latency metrics to Kafka Streams Currently, the actual end-to-end latency of a record flowing through Kafka Streams is difficult to gauge at best. Kafka Streams now exposes end-to-end metrics, which will be a great help for enabling users to make design choices. See KIP-613 for more information. KIP-607: Add metrics to Kafka Streams to report properties of RocksDB The current metrics exposed by Kafka Streams for RocksDB do not include information on memory or disk usage. Now in 2.7.0, Kafka Streams reports properties RocksDB exposes by default. See KIP-607 for more details. KIP-450: Sliding window aggregations in the DSL Kafka Streams implements session windows, tumbling windows, and hopping windows as windowed aggregation methods. While hopping windows with a small advance time can imitate the behavior of a sliding window, this implementation&rsquo;s performance is poor because it results in many overlapping and often redundant windows that require expensive calculations. With the addition of sliding windows via KIP-450, Kafka Streams now provides an efficient way to perform sliding aggregations. Conclusion To learn more about what&rsquo;s new in Apache Kafka 2.7 and to see all the KIPs included in this release, be sure to check out the release notes and the highlights in the release video. To download Apache Kafka 2.7.0, visit the project&#39;s download page. Of course this release would not be possible without a huge effort from the community. A big thank you to everyone involved in this release, including the following 116 people (according to git shortlog) who contributed either code or documentation: A. Sophie Blee-Goldman, Chia-Ping Tsai, John Roesler, David Jacot, Jason Gustafson, Matthias J. Sax, Bruno Cadonna, Ismael Juma, Guozhang Wang, Rajini Sivaram, Luke Chen, Boyang Chen, Tom Bentley, showuon, leah, Bill Bejeck, Chris Egerton, Ron Dagostino, Randall Hauch, Xavier L&eacute;aut&eacute;, Kowshik Prakasam, Konstantine Karantasis, David Arthur, Mickael Maison, Colin Patrick McCabe, huxi, Nikolay, Manikumar Reddy, Jorge Esteban Quilcate Otoya, Vito Jeng, bill, Bob Barrett, vinoth chandar, feyman2016, Jim Galasyn, Greg Harris, khairy, Sanjana Kaundinya, Ning Zhang, Aakash Shah, Andras Katona, Andre Araujo, Andy Coates, Anna Povzner, Badai Aqrandista, Brian Byrne, Dima Reznik, Jeff Kim, John Thomas, Justine Olshan, Lee Dongjin, Leonard Ge, Lucas Bradstreet, Mario Molina, Michael Bingham, Rens Groothuijsen, Stanislav Kozlovski, Yuriy Badalyantc, Levani Kokhreidze, Lucent-Wong, Gokul Srinivas, Mandar Tillu, Gal Margalit, tswstarplanet, Evelyn Bayes, Micah Paul Ramos, vamossagar12, Ego, Navina Ramesh, Nikhil Bhatia, Edoardo Comar, Nikolay Izhikov, Dhruvil Shah, Nitesh Mor, Noa Resare, David Mao, Raman Verma, Cheng Tan, Adam Bellemare, Richard Fussenegger, Rob Meng, Rohan, Can Cecen, Benoit Maggi, Sasaki Toru, Shaik Zakir Hussain, Shailesh Panwar, Sharath Bhat, voffcheg109, Thorsten Hake, Auston, Vikas Singh, Ashish Roy, Arjun Satish, xakassi, Zach Zhang, albert02lowis, Antony Stubbs, Ankit Kumar, gnkoshelev, high.lee, huangyiming, Andrew Egelhofer, jeff kim, jiameixie, Andrew Choi, JoelWee, Jesse Gorzinski, Alex Diachenko, Ivan Yurchenko, manijndl7, Igor Soarez, Gonzalo Mu&ntilde;oz, sbellapu, serjchebotarev, Adem Efe Gencer" />
<meta property="og:description" content="I&#39;m proud to announce the release of Apache Kafka 2.7.0 on behalf of the Apache Kafka&reg; community. The 2.7.0 release contains many new features and improvements. This blog post highlights some of the more prominent ones. Be sure to see the release notes for the full list of changes. You can also watch the release video for a summary of what&rsquo;s new. In this release, we&rsquo;ve continued steady progress toward the task of replacing ZooKeeper in Kafka with KIP-497, which adds a new inter-broker API for altering the in-sync replica (ISR). This release also provides the addition of the Core Raft Implementation as part of KIP-595. Now there is a separate &quot;raft&quot; module containing the core consensus protocol. Until integration with the controller (the broker in the Kafka cluster responsible for managing the state of partitions and replicas) is complete, there is a standalone server that you can use for testing the performance of the Raft implementation. Of course, there are additional efforts underway toward replacing Zookeeper, with seven KIPs in active development to provide support for more partitions per cluster, simpler operation, and tighter security. Tiered Storage work continues and unlocks infinite scaling and faster rebalance times via KIP-405. Kafka broker, producer, and consumer KIP-654: Aborted transaction with non-flushed data should throw a non-fatal exception When a Java client producer aborts a transaction with any non-flushed (pending) data, a fatal exception is thrown. But aborting a transaction with pending data is in fact considered a normal situation. The thrown exception should be to notify you that records aren&rsquo;t being sent, not that the application is in an unrecoverable state. KIP-654 introduces a new exception TransactionAbortedException, allowing you to retry if desired. KIP-651: Support PEM format for private keys and SSL certificates and private key Currently, Kafka only supports JKS or PKCS12 file-based key and trust stores when using SSL. While it&rsquo;s no longer a standard for email, the Privacy-Enhanced Mail (PEM) is a standard format for storing and distributing cryptographic keys and certificates. KIP-651 adds support for PEM files for key and trust stores, allowing the use of third party providers relying on the PEM format. KIP-612: Ability to limit connection creation rate on brokers Creating connections adds CPU overhead to the broker. Connection storms can come from seemingly well-behaved clients and can stop the broker from performing other useful work. But now there is now a way of enforcing broker-wide and per-listener connection creation rates. The 2.7.0 release contains the first part of KIP-612, with per-IP connections rate limits expected to come in the 2.8.0 release. KIP-599: Throttle create topic, create partition, and delete topic operations The APIs to create topics, create partitions, and delete topics are operations that have a direct impact on the overall load in the Kafka controller. To prevent a cluster from being overwhelmed due to high concurrent topic and partition creations or topic deletions, there is a new quota limiting these operations. See KIP-599 for more details. KIP-584: Versioning scheme for features Apart from broker-client compatibility (for which Kafka has a strong record to ensure they do remain compatible), there are two main questions when new features become available in Kafka: How do Kafka clients become aware of broker capabilities? How does the broker decide which features to enable? KIP-584 provides a flexible and operationally friendly solution for client discovery, feature gating, and rolling upgrades using a single restart. KIP-554: Add broker-side SCRAM configuration API With KIP-554, SCRAM credentials can be managed via the Kafka protocol and the kafka-configs tool was updated to use the newly introduced protocol APIs. This is another important step towards KIP-500 where ZooKeeper is replaced by a built in quorum. KIP-497: Add inter-broker API to alter ISR Currently, Kafka partition leader and ISR information is stored in ZooKeeper. Either the controller or a partition leader may update this information. Because either can update this state, there needs to be a mechanism for sharing this information, which can cause delays in reflecting ISR changes. The impact of these delays means that metadata requests may receive stale information. In the 2.7.0 release, there is a new AlterIsr API, which gives the controller the exclusive ability to update the state of partition leaders and ISR. The chief benefit of this new API is that metadata requests will always reflect the latest state. The addition of this API is a significant step forward in the process of removing ZooKeeper and the completion of KIP-500. For more information, see KIP-497. KIP-431: Print additional fields from records with the ConsoleConsumer Now you can print the headers on a ConsumerRecord with the ConsoleConsumer. See KIP-431 for more details. Kafka Connect KIP-632: Add DirectoryConfigProvider KIP-632 adds a DirectoryConfigProvider class to support users needing to provide secrets for keys stored in a container filesystem, such as a Kubernetes environment. Kafka Streams KIP-662: Throw exception when source topics of Kafka Streams application is deleted Today, if a user deletes the source topic of a running Kafka Streams application, the embedded consumer clients gracefully shut down. This client shutdown triggers rebalancing until all StreamThreads of the Streams application gracefully exit, leaving the application completely shut down without any chance to respond to the error. With the addition of KIP-662, when a user deletes a source topic from a running Streams application, the app throws a MissingSourceTopicException, allowing for you to react to the error. KIP-648: Renaming getter method for interactive queries KIP-648 changes the getter methods for interactive query objects to follow the Kafka format of not using the get prefix. KIP-617: Allow Kafka Streams state stores to be iterated backwards Currently, when using an iterator over a Kafka Streams state store, you can only traverse elements from oldest to newest. When iterating over a windowed state store and the user desires to return the latest N records, there is no choice but to use the inefficient approach of traversing all the oldest records before getting to the desired newer records. KIP-617 adds support for iteration over a state store in reverse. Iterating in reverse makes a latest N records retrieval much more efficient. KIP-616: Rename implicit SerDes instances in kafka-streams-scala Kafka Streams now how better Scala implicit Serdes support with KIP-616. KIP-613: Add end-to-end latency metrics to Kafka Streams Currently, the actual end-to-end latency of a record flowing through Kafka Streams is difficult to gauge at best. Kafka Streams now exposes end-to-end metrics, which will be a great help for enabling users to make design choices. See KIP-613 for more information. KIP-607: Add metrics to Kafka Streams to report properties of RocksDB The current metrics exposed by Kafka Streams for RocksDB do not include information on memory or disk usage. Now in 2.7.0, Kafka Streams reports properties RocksDB exposes by default. See KIP-607 for more details. KIP-450: Sliding window aggregations in the DSL Kafka Streams implements session windows, tumbling windows, and hopping windows as windowed aggregation methods. While hopping windows with a small advance time can imitate the behavior of a sliding window, this implementation&rsquo;s performance is poor because it results in many overlapping and often redundant windows that require expensive calculations. With the addition of sliding windows via KIP-450, Kafka Streams now provides an efficient way to perform sliding aggregations. Conclusion To learn more about what&rsquo;s new in Apache Kafka 2.7 and to see all the KIPs included in this release, be sure to check out the release notes and the highlights in the release video. To download Apache Kafka 2.7.0, visit the project&#39;s download page. Of course this release would not be possible without a huge effort from the community. A big thank you to everyone involved in this release, including the following 116 people (according to git shortlog) who contributed either code or documentation: A. Sophie Blee-Goldman, Chia-Ping Tsai, John Roesler, David Jacot, Jason Gustafson, Matthias J. Sax, Bruno Cadonna, Ismael Juma, Guozhang Wang, Rajini Sivaram, Luke Chen, Boyang Chen, Tom Bentley, showuon, leah, Bill Bejeck, Chris Egerton, Ron Dagostino, Randall Hauch, Xavier L&eacute;aut&eacute;, Kowshik Prakasam, Konstantine Karantasis, David Arthur, Mickael Maison, Colin Patrick McCabe, huxi, Nikolay, Manikumar Reddy, Jorge Esteban Quilcate Otoya, Vito Jeng, bill, Bob Barrett, vinoth chandar, feyman2016, Jim Galasyn, Greg Harris, khairy, Sanjana Kaundinya, Ning Zhang, Aakash Shah, Andras Katona, Andre Araujo, Andy Coates, Anna Povzner, Badai Aqrandista, Brian Byrne, Dima Reznik, Jeff Kim, John Thomas, Justine Olshan, Lee Dongjin, Leonard Ge, Lucas Bradstreet, Mario Molina, Michael Bingham, Rens Groothuijsen, Stanislav Kozlovski, Yuriy Badalyantc, Levani Kokhreidze, Lucent-Wong, Gokul Srinivas, Mandar Tillu, Gal Margalit, tswstarplanet, Evelyn Bayes, Micah Paul Ramos, vamossagar12, Ego, Navina Ramesh, Nikhil Bhatia, Edoardo Comar, Nikolay Izhikov, Dhruvil Shah, Nitesh Mor, Noa Resare, David Mao, Raman Verma, Cheng Tan, Adam Bellemare, Richard Fussenegger, Rob Meng, Rohan, Can Cecen, Benoit Maggi, Sasaki Toru, Shaik Zakir Hussain, Shailesh Panwar, Sharath Bhat, voffcheg109, Thorsten Hake, Auston, Vikas Singh, Ashish Roy, Arjun Satish, xakassi, Zach Zhang, albert02lowis, Antony Stubbs, Ankit Kumar, gnkoshelev, high.lee, huangyiming, Andrew Egelhofer, jeff kim, jiameixie, Andrew Choi, JoelWee, Jesse Gorzinski, Alex Diachenko, Ivan Yurchenko, manijndl7, Igor Soarez, Gonzalo Mu&ntilde;oz, sbellapu, serjchebotarev, Adem Efe Gencer" />
<link rel="canonical" href="http://localhost:4000/kafka/entry/what-s-new-in-apache4" />
<meta property="og:url" content="http://localhost:4000/kafka/entry/what-s-new-in-apache4" />
<meta property="og:site_name" content="Blogs Archive" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-21T16:00:29-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="What’s New in Apache Kafka 2.7.0" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2020-12-21T16:00:29-05:00","datePublished":"2020-12-21T16:00:29-05:00","description":"I&#39;m proud to announce the release of Apache Kafka 2.7.0 on behalf of the Apache Kafka&reg; community. The 2.7.0 release contains many new features and improvements. This blog post highlights some of the more prominent ones. Be sure to see the release notes for the full list of changes. You can also watch the release video for a summary of what&rsquo;s new. In this release, we&rsquo;ve continued steady progress toward the task of replacing ZooKeeper in Kafka with KIP-497, which adds a new inter-broker API for altering the in-sync replica (ISR). This release also provides the addition of the Core Raft Implementation as part of KIP-595. Now there is a separate &quot;raft&quot; module containing the core consensus protocol. Until integration with the controller (the broker in the Kafka cluster responsible for managing the state of partitions and replicas) is complete, there is a standalone server that you can use for testing the performance of the Raft implementation. Of course, there are additional efforts underway toward replacing Zookeeper, with seven KIPs in active development to provide support for more partitions per cluster, simpler operation, and tighter security. Tiered Storage work continues and unlocks infinite scaling and faster rebalance times via KIP-405. Kafka broker, producer, and consumer KIP-654: Aborted transaction with non-flushed data should throw a non-fatal exception When a Java client producer aborts a transaction with any non-flushed (pending) data, a fatal exception is thrown. But aborting a transaction with pending data is in fact considered a normal situation. The thrown exception should be to notify you that records aren&rsquo;t being sent, not that the application is in an unrecoverable state. KIP-654 introduces a new exception TransactionAbortedException, allowing you to retry if desired. KIP-651: Support PEM format for private keys and SSL certificates and private key Currently, Kafka only supports JKS or PKCS12 file-based key and trust stores when using SSL. While it&rsquo;s no longer a standard for email, the Privacy-Enhanced Mail (PEM) is a standard format for storing and distributing cryptographic keys and certificates. KIP-651 adds support for PEM files for key and trust stores, allowing the use of third party providers relying on the PEM format. KIP-612: Ability to limit connection creation rate on brokers Creating connections adds CPU overhead to the broker. Connection storms can come from seemingly well-behaved clients and can stop the broker from performing other useful work. But now there is now a way of enforcing broker-wide and per-listener connection creation rates. The 2.7.0 release contains the first part of KIP-612, with per-IP connections rate limits expected to come in the 2.8.0 release. KIP-599: Throttle create topic, create partition, and delete topic operations The APIs to create topics, create partitions, and delete topics are operations that have a direct impact on the overall load in the Kafka controller. To prevent a cluster from being overwhelmed due to high concurrent topic and partition creations or topic deletions, there is a new quota limiting these operations. See KIP-599 for more details. KIP-584: Versioning scheme for features Apart from broker-client compatibility (for which Kafka has a strong record to ensure they do remain compatible), there are two main questions when new features become available in Kafka: How do Kafka clients become aware of broker capabilities? How does the broker decide which features to enable? KIP-584 provides a flexible and operationally friendly solution for client discovery, feature gating, and rolling upgrades using a single restart. KIP-554: Add broker-side SCRAM configuration API With KIP-554, SCRAM credentials can be managed via the Kafka protocol and the kafka-configs tool was updated to use the newly introduced protocol APIs. This is another important step towards KIP-500 where ZooKeeper is replaced by a built in quorum. KIP-497: Add inter-broker API to alter ISR Currently, Kafka partition leader and ISR information is stored in ZooKeeper. Either the controller or a partition leader may update this information. Because either can update this state, there needs to be a mechanism for sharing this information, which can cause delays in reflecting ISR changes. The impact of these delays means that metadata requests may receive stale information. In the 2.7.0 release, there is a new AlterIsr API, which gives the controller the exclusive ability to update the state of partition leaders and ISR. The chief benefit of this new API is that metadata requests will always reflect the latest state. The addition of this API is a significant step forward in the process of removing ZooKeeper and the completion of KIP-500. For more information, see KIP-497. KIP-431: Print additional fields from records with the ConsoleConsumer Now you can print the headers on a ConsumerRecord with the ConsoleConsumer. See KIP-431 for more details. Kafka Connect KIP-632: Add DirectoryConfigProvider KIP-632 adds a DirectoryConfigProvider class to support users needing to provide secrets for keys stored in a container filesystem, such as a Kubernetes environment. Kafka Streams KIP-662: Throw exception when source topics of Kafka Streams application is deleted Today, if a user deletes the source topic of a running Kafka Streams application, the embedded consumer clients gracefully shut down. This client shutdown triggers rebalancing until all StreamThreads of the Streams application gracefully exit, leaving the application completely shut down without any chance to respond to the error. With the addition of KIP-662, when a user deletes a source topic from a running Streams application, the app throws a MissingSourceTopicException, allowing for you to react to the error. KIP-648: Renaming getter method for interactive queries KIP-648 changes the getter methods for interactive query objects to follow the Kafka format of not using the get prefix. KIP-617: Allow Kafka Streams state stores to be iterated backwards Currently, when using an iterator over a Kafka Streams state store, you can only traverse elements from oldest to newest. When iterating over a windowed state store and the user desires to return the latest N records, there is no choice but to use the inefficient approach of traversing all the oldest records before getting to the desired newer records. KIP-617 adds support for iteration over a state store in reverse. Iterating in reverse makes a latest N records retrieval much more efficient. KIP-616: Rename implicit SerDes instances in kafka-streams-scala Kafka Streams now how better Scala implicit Serdes support with KIP-616. KIP-613: Add end-to-end latency metrics to Kafka Streams Currently, the actual end-to-end latency of a record flowing through Kafka Streams is difficult to gauge at best. Kafka Streams now exposes end-to-end metrics, which will be a great help for enabling users to make design choices. See KIP-613 for more information. KIP-607: Add metrics to Kafka Streams to report properties of RocksDB The current metrics exposed by Kafka Streams for RocksDB do not include information on memory or disk usage. Now in 2.7.0, Kafka Streams reports properties RocksDB exposes by default. See KIP-607 for more details. KIP-450: Sliding window aggregations in the DSL Kafka Streams implements session windows, tumbling windows, and hopping windows as windowed aggregation methods. While hopping windows with a small advance time can imitate the behavior of a sliding window, this implementation&rsquo;s performance is poor because it results in many overlapping and often redundant windows that require expensive calculations. With the addition of sliding windows via KIP-450, Kafka Streams now provides an efficient way to perform sliding aggregations. Conclusion To learn more about what&rsquo;s new in Apache Kafka 2.7 and to see all the KIPs included in this release, be sure to check out the release notes and the highlights in the release video. To download Apache Kafka 2.7.0, visit the project&#39;s download page. Of course this release would not be possible without a huge effort from the community. A big thank you to everyone involved in this release, including the following 116 people (according to git shortlog) who contributed either code or documentation: A. Sophie Blee-Goldman, Chia-Ping Tsai, John Roesler, David Jacot, Jason Gustafson, Matthias J. Sax, Bruno Cadonna, Ismael Juma, Guozhang Wang, Rajini Sivaram, Luke Chen, Boyang Chen, Tom Bentley, showuon, leah, Bill Bejeck, Chris Egerton, Ron Dagostino, Randall Hauch, Xavier L&eacute;aut&eacute;, Kowshik Prakasam, Konstantine Karantasis, David Arthur, Mickael Maison, Colin Patrick McCabe, huxi, Nikolay, Manikumar Reddy, Jorge Esteban Quilcate Otoya, Vito Jeng, bill, Bob Barrett, vinoth chandar, feyman2016, Jim Galasyn, Greg Harris, khairy, Sanjana Kaundinya, Ning Zhang, Aakash Shah, Andras Katona, Andre Araujo, Andy Coates, Anna Povzner, Badai Aqrandista, Brian Byrne, Dima Reznik, Jeff Kim, John Thomas, Justine Olshan, Lee Dongjin, Leonard Ge, Lucas Bradstreet, Mario Molina, Michael Bingham, Rens Groothuijsen, Stanislav Kozlovski, Yuriy Badalyantc, Levani Kokhreidze, Lucent-Wong, Gokul Srinivas, Mandar Tillu, Gal Margalit, tswstarplanet, Evelyn Bayes, Micah Paul Ramos, vamossagar12, Ego, Navina Ramesh, Nikhil Bhatia, Edoardo Comar, Nikolay Izhikov, Dhruvil Shah, Nitesh Mor, Noa Resare, David Mao, Raman Verma, Cheng Tan, Adam Bellemare, Richard Fussenegger, Rob Meng, Rohan, Can Cecen, Benoit Maggi, Sasaki Toru, Shaik Zakir Hussain, Shailesh Panwar, Sharath Bhat, voffcheg109, Thorsten Hake, Auston, Vikas Singh, Ashish Roy, Arjun Satish, xakassi, Zach Zhang, albert02lowis, Antony Stubbs, Ankit Kumar, gnkoshelev, high.lee, huangyiming, Andrew Egelhofer, jeff kim, jiameixie, Andrew Choi, JoelWee, Jesse Gorzinski, Alex Diachenko, Ivan Yurchenko, manijndl7, Igor Soarez, Gonzalo Mu&ntilde;oz, sbellapu, serjchebotarev, Adem Efe Gencer","headline":"What’s New in Apache Kafka 2.7.0","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/kafka/entry/what-s-new-in-apache4"},"url":"http://localhost:4000/kafka/entry/what-s-new-in-apache4"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Blogs Archive" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Blogs Archive</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">What&amp;rsquo;s New in Apache Kafka 2.7.0</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-12-21T16:00:29-05:00" itemprop="datePublished">Dec 21, 2020
      </time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">{"display_name"=>"Bill Bejeck", "login"=>"bbejeck", "email"=>"bbejeck@apache.org"}</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>I'm proud to announce the release of Apache Kafka 2.7.0 on behalf of the Apache Kafka<sup>&reg;</sup> community. The 2.7.0 release contains many new features and improvements. This blog post highlights some of the more prominent ones. Be sure to see the <a href="https://dist.apache.org/repos/dist/release/kafka/2.7.0/RELEASE_NOTES.html"><u>release notes</u></a> for the full list of changes. You can also watch the <a href="https://youtu.be/FIHVMWfHtvY"><u>release video</u></a> for a summary of what&rsquo;s new.</p>
<p>In this release, we&rsquo;ve continued steady progress toward the task of <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum">replacing ZooKeeper in Kafka</a> with <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-497%3A+Add+inter-broker+API+to+alter+ISR"><u>KIP-497</u></a>, which adds a new inter-broker API for altering the in-sync replica (ISR). This release also provides the addition of the <a href="https://issues.apache.org/jira/browse/KAFKA-10492">Core Raft Implementation</a> as part of <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum">KIP-595</a>. Now there is a separate "raft" module containing the core consensus protocol. Until integration with the controller (the broker in the Kafka cluster responsible for managing the state of partitions and replicas) is complete, there is a standalone server that you can use for testing the performance of the Raft implementation.</p>
<p>Of course, there are additional efforts underway toward replacing Zookeeper, with seven KIPs in active development to provide support for more partitions per cluster, simpler operation, and tighter security.</p>
<p>Tiered Storage work continues and unlocks infinite scaling and faster rebalance times via <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-405%3A+Kafka+Tiered+Storage"><u>KIP-405</u></a>.</p>
<h2 id="kafka-broker-producer-and-consumer">Kafka broker, producer, and consumer</h2>
<h3 id="kip-654-aborted-transaction-with-non-flushed-data-should-throw-a-non-fatal-exception">KIP-654: Aborted transaction with non-flushed data should throw a non-fatal exception</h3>
<p>When a Java client producer aborts a transaction with any non-flushed (pending) data, a fatal exception is thrown. But aborting a transaction with pending data is in fact considered a normal situation. The thrown exception should be to notify you that records aren&rsquo;t being sent, not that the application is in an unrecoverable state. <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-654%3A+Aborted+transaction+with+non-flushed+data+should+throw+a+non-fatal+exception"><u>KIP-654</u></a> introduces a new exception TransactionAbortedException, allowing you to retry if desired.</p>
<h3 id="kip-651-support-pem-format-for-private-keys-and-ssl-certificates-and-private-key">KIP-651: Support PEM format for private keys and SSL certificates and private key</h3>
<p>Currently, Kafka only supports JKS or PKCS12 file-based key and trust stores when using SSL. While it&rsquo;s no longer a standard for email, the Privacy-Enhanced Mail (PEM) is a standard format for storing and distributing cryptographic keys and certificates. <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-651+-+Support+PEM+format+for+SSL+certificates+and+private+key"><u>KIP-651</u></a> adds support for PEM files for key and trust stores, allowing the use of third party providers relying on the PEM format.</p>
<h3 id="kip-612-ability-to-limit-connection-creation-rate-on-brokers">KIP-612: Ability to limit connection creation rate on brokers</h3>
<p>Creating connections adds CPU overhead to the broker. Connection storms can come from seemingly well-behaved clients and can stop the broker from performing other useful work. But now there is now a way of enforcing broker-wide and per-listener connection creation rates. The 2.7.0 release contains the first part of <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-612%3A+Ability+to+Limit+Connection+Creation+Rate+on+Brokers"><u>KIP-612</u></a>, with per-IP connections rate limits expected to come in the 2.8.0 release.</p>
<h3 id="kip-599-throttle-create-topic-create-partition-and-delete-topic-operations">KIP-599: Throttle create topic, create partition, and delete topic operations</h3>
<p>The APIs to create topics, create partitions, and delete topics are operations that have a direct impact on the overall load in the Kafka controller. To prevent a cluster from being overwhelmed due to high concurrent topic and partition creations or topic deletions, there is a new quota limiting these operations. See <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-599%3A+Throttle+Create+Topic%2C+Create+Partition+and+Delete+Topic+Operations">KIP-599</a> for more details.</p>
<h3 id="kip-584-versioning-scheme-for-features">KIP-584: Versioning scheme for features</h3>
<p>Apart from broker-client compatibility (for which Kafka has a strong record to ensure they do remain compatible), there are two main questions when new features become available in Kafka:</p>
<ol type="1">
<li>
<p>How do Kafka clients become aware of broker capabilities?</p>
</li>
<li>
<p>How does the broker decide which features to enable?</p>
</li>
</ol>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-584%3A+Versioning+scheme+for+features">KIP-584</a> provides a flexible and operationally friendly solution for client discovery, feature gating, and rolling upgrades using a single restart.</p>
<h3 id="kip-554-add-broker-side-scram-configuration-api">KIP-554: Add broker-side SCRAM configuration API</h3>
<p>With <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-554%3A+Add+Broker-side+SCRAM+Config+API"><u>KIP-554</u></a>,  SCRAM credentials can be managed via the Kafka protocol and the kafka-configs tool was updated to use the newly introduced protocol APIs. This is another important step towards KIP-500 where ZooKeeper is replaced by a built in quorum.</p>
<h3 id="kip-497-add-inter-broker-api-to-alter-isr">KIP-497: Add inter-broker API to alter ISR</h3>
<p>Currently, Kafka partition leader and ISR information is stored in ZooKeeper. Either the controller or a partition leader may update this information. Because either can update this state, there needs to be a mechanism for sharing this information, which can cause delays in reflecting ISR changes. The impact of these delays means that metadata requests may receive stale information.</p>
<p>In the 2.7.0 release, there is a new AlterIsr API, which gives the controller the exclusive ability to update the state of partition leaders and ISR. The chief benefit of this new API is that metadata requests will always reflect the latest state.</p>
<p>The addition of this API is a significant step forward in the process of removing ZooKeeper and the completion of KIP-500. For more information, see <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-497%3A+Add+inter-broker+API+to+alter+ISR"><u>KIP-497</u></a>.</p>
<h3 id="kip-431-print-additional-fields-from-records-with-the-consoleconsumer">KIP-431: Print additional fields from records with the ConsoleConsumer</h3>
<p>Now you can print the headers on a ConsumerRecord with the ConsoleConsumer. See <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-431%3A+Support+of+printing+additional+ConsumerRecord+fields+in+DefaultMessageFormatter"><u>KIP-431</u></a> for more details.</p>
<h2 id="kafka-connect">Kafka Connect</h2>
<h3 id="kip-632-add-directoryconfigprovider">KIP-632: Add DirectoryConfigProvider</h3>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-632%3A+Add+DirectoryConfigProvider"><u>KIP-632</u></a> adds a DirectoryConfigProvider class to support users needing to provide secrets for keys stored in a container filesystem, such as a Kubernetes environment.</p>
<h2 id="kafka-streams">Kafka Streams</h2>
<h3 id="kip-662-throw-exception-when-source-topics-of-kafka-streams-application-is-deleted">KIP-662: Throw exception when source topics of Kafka Streams application is deleted</h3>
<p>Today, if a user deletes the source topic of a running Kafka Streams application, the embedded consumer clients gracefully shut down. This client shutdown triggers rebalancing until all StreamThreads of the Streams application gracefully exit, leaving the application completely shut down without any chance to respond to the error. With the addition of <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-662%3A+Throw+Exception+when+Source+Topics+of+a+Streams+App+are+Deleted"><u>KIP-662</u></a>, when a user deletes a source topic from a running Streams application, the app throws a MissingSourceTopicException, allowing for you to react to the error.</p>
<h3 id="kip-648-renaming-getter-method-for-interactive-queries">KIP-648: Renaming getter method for interactive queries</h3>
<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-648%3A+Renaming+getter+method+for+Interactive+Queries">KIP-648</a> changes the getter methods for interactive query objects to follow the Kafka format of not using the get prefix.</p>
<h3 id="kip-617-allow-kafka-streams-state-stores-to-be-iterated-backwards">KIP-617: Allow Kafka Streams state stores to be iterated backwards</h3>
<p>Currently, when using an iterator over a Kafka Streams state store, you can only traverse elements from <em>oldest</em> to <em>newest</em>. When iterating over a windowed state store and the user desires to return the latest N records, there is no choice but to use the inefficient approach of traversing all the oldest records before getting to the desired newer records. <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-617%3A+Allow+Kafka+Streams+State+Stores+to+be+iterated+backwards">KIP-617</a> adds support for iteration over a state store in reverse. Iterating in reverse makes a latest N records retrieval much more efficient.</p>
<h3 id="kip-616-rename-implicit-serdes-instances-in-kafka-streams-scala">KIP-616: Rename implicit SerDes instances in kafka-streams-scala</h3>
<p>Kafka Streams now how better Scala implicit Serdes support with <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-616%3A+Rename+implicit+Serdes+instances+in+kafka-streams-scala">KIP-616</a>.</p>
<h3 id="kip-613-add-end-to-end-latency-metrics-to-kafka-streams">KIP-613: Add end-to-end latency metrics to Kafka Streams</h3>
<p>Currently, the actual end-to-end latency of a record flowing through Kafka Streams is difficult to gauge at best. Kafka Streams now exposes end-to-end metrics, which will be a great help for enabling users to make design choices. See <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-613%3A+Add+end-to-end+latency+metrics+to+Streams">KIP-613</a> for more information.</p>
<h3 id="kip-607-add-metrics-to-kafka-streams-to-report-properties-of-rocksdb">KIP-607: Add metrics to Kafka Streams to report properties of RocksDB</h3>
<p>The current <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-471%3A+Expose+RocksDB+Metrics+in+Kafka+Streams">metrics exposed by Kafka Streams</a> for RocksDB do not include information on memory or disk usage. Now in 2.7.0, Kafka Streams reports properties RocksDB exposes by default. See <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-607%3A+Add+Metrics+to+Kafka+Streams+to+Report+Properties+of+RocksDB">KIP-607</a> for more details.</p>
<h3 id="kip-450-sliding-window-aggregations-in-the-dsl">KIP-450: Sliding window aggregations in the DSL</h3>
<p>Kafka Streams implements session windows, tumbling windows, and hopping windows as windowed aggregation methods. While hopping windows with a small advance time can imitate the behavior of a sliding window, this implementation&rsquo;s performance is poor because it results in many overlapping and often redundant windows that require expensive calculations. With the addition of sliding windows via <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-450%3A+Sliding+Window+Aggregations+in+the+DSL">KIP-450</a>, Kafka Streams now provides an efficient way to perform sliding aggregations.</p>
<h2 id="conclusion">Conclusion</h2>
<p>To learn more about what&rsquo;s new in Apache Kafka 2.7 and to see all the KIPs included in this release, be sure to check out the <a href="https://dist.apache.org/repos/dist/release/kafka/2.7.0/RELEASE_NOTES.html"><u>release notes</u></a> and the highlights in the <a href="https://youtu.be/FIHVMWfHtvY"><u>release video</u></a>.</p>
<p>To download Apache Kafka 2.7.0, visit the <a href="https://kafka.apache.org/downloads">project's download page.</a></p>
<p>Of course this release would not be possible without a huge effort from the community. A big thank you to everyone involved in this release, including the following 116 people (according to git shortlog) who contributed either code or documentation:</p>
<p>A. Sophie Blee-Goldman, Chia-Ping Tsai, John Roesler, David Jacot, Jason Gustafson, Matthias J. Sax, Bruno Cadonna, Ismael Juma, Guozhang Wang, Rajini Sivaram, Luke Chen, Boyang Chen, Tom Bentley, showuon, leah, Bill Bejeck, Chris Egerton, Ron Dagostino, Randall Hauch, Xavier L&eacute;aut&eacute;, Kowshik Prakasam, Konstantine Karantasis, David Arthur, Mickael Maison, Colin Patrick McCabe, huxi, Nikolay, Manikumar Reddy, Jorge Esteban Quilcate Otoya, Vito Jeng, bill, Bob Barrett, vinoth chandar, feyman2016, Jim Galasyn, Greg Harris, khairy, Sanjana Kaundinya, Ning Zhang, Aakash Shah, Andras Katona, Andre Araujo, Andy Coates, Anna Povzner, Badai Aqrandista, Brian Byrne, Dima Reznik, Jeff Kim, John Thomas, Justine Olshan, Lee Dongjin, Leonard Ge, Lucas Bradstreet, Mario Molina, Michael Bingham, Rens Groothuijsen, Stanislav Kozlovski, Yuriy Badalyantc, Levani Kokhreidze, Lucent-Wong, Gokul Srinivas, Mandar Tillu, Gal Margalit, tswstarplanet, Evelyn Bayes, Micah Paul Ramos, vamossagar12, Ego, Navina Ramesh, Nikhil Bhatia, Edoardo Comar, Nikolay Izhikov, Dhruvil Shah, Nitesh Mor, Noa Resare, David Mao, Raman Verma, Cheng Tan, Adam Bellemare, Richard Fussenegger, Rob Meng, Rohan, Can Cecen, Benoit Maggi, Sasaki Toru, Shaik Zakir Hussain, Shailesh Panwar, Sharath Bhat, voffcheg109, Thorsten Hake, Auston, Vikas Singh, Ashish Roy, Arjun Satish, xakassi, Zach Zhang, albert02lowis, Antony Stubbs, Ankit Kumar, gnkoshelev, high.lee, huangyiming, Andrew Egelhofer, jeff kim, jiameixie, Andrew Choi, JoelWee, Jesse Gorzinski, Alex Diachenko, Ivan Yurchenko, manijndl7, Igor Soarez, Gonzalo Mu&ntilde;oz, sbellapu, serjchebotarev, Adem Efe Gencer</p>

  </div><a class="u-url" href="/kafka/entry/what-s-new-in-apache4" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Blogs Archive</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Blogs Archive</li><li><a class="u-email" href="mailto:issues@infra.apache.org">issues@infra.apache.org</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is an archive of the Roller blogs that were previously hosted on blogs.apache.org</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
