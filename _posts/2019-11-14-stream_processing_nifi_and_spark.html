---
layout: post
status: PUBLISHED
published: true
title: 'Stream Processing: NiFi and Spark'
author:
  display_name: markap14@apache.org
  login: markap14
  email: markap14@apache.org
author_login: markap14
author_email: markap14@apache.org
id: fe48b7f6-57c5-47ab-a30e-a5ba7b99be88
date: '2019-11-14 13:30:30 -0500'
categories:
- Dataflow
tags:
- nifi
- spark
- streaming
comments:
- id: 0
  author: Aur&eacute;lien
  author_email: aurelien@dehay.info
  author_url: ''
  date: '2015-11-18 16:09:21 -0500'
  content: "Hello.\r\n\r\nI've managed to use the receiver in Spark scala, but I wonder
    about reliability.\r\n\r\nIf I enable the WAL in Spark, and with the nifi receiver
    data acknowledgment, does the process will be Exactly-one?\r\n\r\nThanks. "
- id: 0
  author: Mark
  author_email: markap14@hotmail.com
  author_url: http://nifi.incubator.apache.org
  date: '2015-05-15 01:30:59 -0400'
  content: 'J.H.: Thanks, you''re right. I fixed the typo. I did some refactoring
    of the API a few times before it was released. Hopefully I caught any other change
    that I may have made :) Sorry it took so long to reply - email got lost in the
    shuffle. Good catch, though! I appreciate the heads-up.'
- id: 0
  author: Atanu
  author_email: atanu.chat@gmail.com
  author_url: ''
  date: '2018-03-22 07:50:06 -0400'
  content: I want to develop similar kind of code in Pyspark, for that which python
    module is need to import ? can you please help me out ?
- id: 1
  author: tech blog online
  author_email: subodhsahu392@gmail.com
  author_url: http://techbloggeronline.blog.fc2.com/
  date: '2018-10-30 06:34:39 -0400'
  content: Hey Mark a great work man, I appreciate your work, Thank you for sharing
    this post.
- id: 3
  author: JHowix
  author_email: jhowix@gmail.com
  author_url: ''
  date: '2015-04-28 22:27:37 -0400'
  content: "Hi Mark,\r\n\r\nThanks for the post.  I think there is a minor syntax
    error in the SiteToSiteClientBuilder piece.  In the 0.0.2 code, the calls are
    url() and portName().  No sets.  Thanks again.\r\n\r\nJ.H."
- id: 7
  author: Mark
  author_email: markap14@hotmail.com
  author_url: http://nifi.apache.org
  date: '2015-11-23 18:54:17 -0500'
  content: "Aur&eacute;lien,\r\n\r\nThis is a fairly common question. However, it
    is important to note that truly having exactly-one delivery is not possible (there
    is a good discussion of why that is true at http://bravenewgeek.com/you-cannot-have-exactly-once-delivery/).\r\n\r\nThe
    \"best\" one can really do from the producer side is to provide at-least-once
    processing. This is what NiFi provides. You are guaranteed to get the data, but
    it's possible that you can receive the data into Spark, commit the transaction
    in Spark, and then lose the connection to NiFi before the acknowledgement is sent
    back. So there is a slim chance that you could get duplicates. If you really need
    to receive the data exactly once, you would have to perform de-duplication on
    the Spark/receiver side. This approach has been discussed in several articles
    online that typically term it as \"exactly-once\" processing. It is not true exactly-once
    delivery but provides the same semantics.\r\n\r\nThanks\r\n-Mark"
- id: 9
  author: Mark
  author_email: markap14@hotmail.com
  author_url: http://nifi.apache.org
  date: '2015-11-23 18:34:41 -0500'
  content: "Aur&eacute;lien,\r\n\r\nThis is a fairly common question. However, it
    is important to note that truly having exactly-one delivery is not possible (there
    is a good discussion of why that is true at http://bravenewgeek.com/you-cannot-have-exactly-once-delivery/).\r\n\r\nThe
    \"best\" one can really do is to provide at-least-once processing. This is what
    NiFi provides. You are guaranteed to get the data exactly once. It is possible,
    however, that you can receive the data into Spark, commit the transaction in Spark,
    and then lose the connection to NiFi or have NiFi shut down before the acknowledgement
    is sent back. So there is a small chance that you could get duplicates. If you
    really need to receive the data exactly once, you would have to perform de-duplication
    on the Spark/receiver side. This approach has been discussed in several articles
    online that typically term it as \"exactly-once\" processing. It is not true exactly-once
    delivery but provides the same semantics. Unfortunately, though, those semantics
    cannot be provided by NiFi alone - the receive must perform the deduplication
    because only the receiver knows for sure what has been received.\r\n\r\nThanks\r\n-Mark"
permalink: nifi/entry/stream_processing_nifi_and_spark
---
<h1>
  Stream Processing: NiFi and Spark<br />
</h1>
<p>
   <span class="author">Mark Payne -&nbsp;</span><br />
   <span class="author"><a href="mailto:markap14@hotmail.com">markap14@hotmail.com</a></span></p>
<hr />
<p>
Without doubt, <a href="https://spark.apache.org/">Apache Spark</a> has become wildly popular for processing large quantities of data. One of the key features that Spark provides<br />
is the ability to process data in either a batch processing mode or a streaming mode with very little change to your code. Batch processing is typically performed by reading data from HDFS.<br />
There have been a few different articles posted about using <a href="http://nifi.incubator.apache.org/">Apache NiFi (incubating)</a> to publish data HDFS.<br />
<a href="http://ingest.tips/2014/12/22/getting-started-with-apache-nifi/">This article</a> does a great job of explaining how to accomplish this.</p>
<p>
In many contexts, though, operating on the data as soon as it is available can provide great benefits. In order to provide the right data as quickly as possible,<br />
NiFi has created a Spark Receiver, available in the <a href="http://nifi.incubator.apache.org/downloads/">0.0.2 release</a> of Apache NiFi.<br />
This post will examine how we can write a simple Spark application to process data from NiFi and how we can configure NiFi to expose the data to Spark.</p>
<p>
Incorporating the Apache NiFi Receiver into your Spark application is pretty easy. First, you'll need to add the Receiver to your application's POM:</p>
<div>
<code></p>
<pre style="background-color: #f1f1f1">
  <dependency>
    <groupId>org.apache.nifi</groupId>
    <artifactId>nifi-spark-receiver</artifactId>
    <version>0.0.2-incubating</version>
  </dependency>
</pre>
<p></code>
</div>
<p>
That's all that is needed in order to be able to use the NiFi Receiver. So now we'll look at how to use the Receiver in your code.</p>
<p>
The NiFi Receiver is a Reliable Java Receiver. This means that if we lose a node after it pulls the data from NiFi, the data will not be lost. Instead, another node will simply pull and process the data. In order to create a NiFi Receiver, we need to first create a configuration that tells the Receiver where to pull the data from. The simplest form is to just tell the config where NiFi is running and which Port to pull the data from:</p>
<div>
<code></p>
<pre style="background-color: #f1f1f1">
SiteToSiteClientConfig config = new SiteToSiteClient.Builder()
  .url("http://localhost:8080/nifi")
  .portName("Data For Spark")
  .buildConfig();
</pre>
<p></code>
</div>
<p>
To briefly explain the terminology here, NiFi refers to its mechanism for transferring data between clusters or instances as Site-to-Site. It exposes options for pushing data or pulling,<br />
so that the most appropriate approach can be used for each situation. Spark doesn't supply a mechanism to have data pushed to it - instead, it wants to pull data from other sources.<br />
In NiFi, this data can be exposed in such a way that a receiver can pull from it by adding an Output Port to the root process group. For Spark, we will use this same mechanism -<br />
we will use this Site-to-Site protocol to pull data from NiFi's Output Ports. In order for this to work, we need two pieces of information: the URL to connect to NiFi and the name of<br />
the Output Port to pull data from.</p>
<p>
If the NiFi instance to connect to is clustered, the URL should be that of the NiFi Cluster Manager. In this case, the Receiver will automatically contact the Cluster<br />
Manager to determine which nodes are in the cluster and will automatically start pulling data from all nodes. The Receiver automatically determines by communicating with the<br />
Cluster Manager which nodes have the most data backed up and will pull from those nodes more heavily than the others. This information is automatically updated periodically<br />
so that as new nodes are added to the cluster or nodes leave the cluster, or if the nodes become more or less bogged down, the Receiver will automatically adjust to handle this.</p>
<p>
Next, we need to instruct the Receiver which Port<br />
to pull data from. Since NiFi can have many different Output Ports, we need to provide either a Port Identifier or a Port Name. If desired, we can also configure communications<br />
timeouts; SSL information for secure data transfer, authentication, and authorization; compression; and preferred batch sizes. See the JavaDocs for the <code>SiteToSiteClient.Builder</code><br />
for more information.</p>
<p>
Once we have constructed this configuration object, we can now create the Receiver:</p>
<div>
<code></p>
<pre style="background-color: #f1f1f1">
 SparkConf sparkConf = new SparkConf().setAppName("NiFi-Spark Streaming example");
 JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, new Duration(1000L));
 
 // Create a JavaReceiverInputDStream using a NiFi receiver so that we can pull data from 
 // specified Port
 JavaReceiverInputDStream packetStream = 
     ssc.receiverStream(new NiFiReceiver(config, StorageLevel.MEMORY_ONLY()));
</pre>
<p></code>
</div>
<p>
This gives us a <code>JavaReceiverInputDStream</code> of type <code>NiFiDataPacket</code>.<br />
The <code>NiFiDataPacket</code> is a simple construct that packages an arbitrary byte array with a map of Key/Value pairs (referred to as attributes) that correspond to the data.<br />
As an example, we can process the data without paying attention to the attributes:</p>
<div>
<code></p>
<pre style="background-color: #f1f1f1">
 // Map the data from NiFi to text, ignoring the attributes
 JavaDStream text = packetStream.map(new Function() {
   public String call(final NiFiDataPacket dataPacket) throws Exception {
     return new String(dataPacket.getContent(), StandardCharsets.UTF_8);
   }
 });
</pre>
<p></code>
</div>
<p>
Or we can make use of the attributes:</p>
<div>
<code></p>
<pre style="background-color: #f1f1f1">
 // Extract the 'uuid' attribute
 JavaDStream text = packetStream.map(new Function() {
   public String call(final NiFiDataPacket dataPacket) throws Exception {
     return dataPacket.getAttributes().get("uuid");
   }
 });
</pre>
<p></code>
</div>
<p>
So now we have our Receiver ready to pull data from NiFi. Let's look at how we can configure NiFi to expose this data.</p>
<p>
First, NiFi has to be configured to allow site-to-site communication. This is accomplished by setting the <code>nifi.remote.input.socket.port</code> property<br />
in the nifi.properties file to the desired port to use for site-to-site (if this value is changed, it will require a restart of NiFi for the changes to take effect).</p>
<p>
Now that NiFi is setup to allow site-to-site, we will build a simple flow to feed data to Spark. We will start by adding two GetFile processors to the flow. One will<br />
pick up from <code>/data/in/notifications</code> and the other will pick up from <code>/data/in/analysis</code>:</p>
<p><img class="screenshot" style="width: 1366px;" src="https://blogs.apache.org/nifi/mediaresource/8d30561d-b6c9-4897-ba2c-ed77ae781b15" alt="GetFile" /></p>
<p>
Next, let's assume that we want to assign different priorities to the data that is picked up from each directory. Let's assign a priority of "1" (the highest priority)<br />
to data coming from the <code>analysis</code> directory and assign a lower priority to the data from the <code>notifications</code> directory. We can use<br />
the UpdateAttribute processor to do this. Drag the Processor onto the graph and configure it. In the Settings tab, we set the name to "Set High Priority". In the<br />
Properties tab, we have no properties available. However, there's a button in the top-right of the dialog that says "New Property." Clicking that button lets us<br />
add a property with the name <code>priority</code> and a value of <code>1</code>. When we click OK, we see it added to the table:</p>
<p><img class="dialog" src="https://blogs.apache.org/nifi/mediaresource/d626b5be-2a4d-4f6b-8312-d4216848b480" alt="Set Priority" /></p>
<p>
Let's click Apply and add another UpdateAttribute processor for the data coming from the <code>notifications</code> directory. Here, we will add a property<br />
with the name <code>priority</code> but give it a value of <code>2</code>. After configuring these processor and connecting the GetFile processors to them,<br />
we end up with a graph that looks like this:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/0fdf59aa-1404-495c-8bce-6cd8dcafc7bc" alt="Connect GetFile and UpdateAttribute" /></p>
<p>
Now, we want to combine all of the data into a single queue so that we can prioritize it before sending the data to Spark. We do this by adding a Funnel to the graph<br />
and connecting both of the UpdateAttribute processors to the Funnel:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/6e31f47a-178c-4eb4-bf47-b1395af43d0c" alt="With Funnel" /></p>
<p>
Now we can add an Output Port that our Spark Streaming application can pull from. We drag an Output Port onto our graph. When prompted for a name, we will name it<br />
<code>Data For Spark</code>, as this is the name that we gave to our Spark Streaming application. Once we connect the Funnel to the Output Port, we have a graph<br />
like this:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/ae7c0df5-f9d6-40f8-9d8c-7a0bdc27b743" alt="Before Running" /></p>
<p>
We haven't actually told NiFi to prioritize the data yet, though. We've simply added an attribute named <code>priority</code>. To prioritize the data based on that, we can<br />
right-click on the connection that feeds the Output Port and choose Configure. From the Settings tab, we can drag the PriorityAttributePrioritizer from the list of Available Prioritizers<br />
to the list of Selected Prioritizers:</p>
<p><img class="dialog" src="https://blogs.apache.org/nifi/mediaresource/35ef6c0f-71cf-46b0-b184-59a50c1c2482" alt="Prioritize" /></p>
<p>
Once we click Apply, we're done. We can start all of the components and we should see the data start flowing to our Spark Streaming application:</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/320ffb30-d5fe-4791-a4ce-e704cfc5e965" alt="Running" /></p>
<p>
Now any data that appears in our <code>/data/in/notifications</code> or <code>/data/in/analysis</code> directory will make its way to our streaming application!</p>
<p>
By using NiFi's Output Port mechanism, we are able to create any number of different named Output Ports, as well. This allows you, as a NiFi user, to choose<br />
exactly which data gets exposed to Spark. Additionally, if NiFi is configured to be secure, each Output Port can be configured to provide the data to only the hosts and users that<br />
are authorized.</p>
<p>
Let's consider, though, that this data has significant value for processing in a streaming fashion as soon as we have the data, but it may also be of value to a batch processing<br />
analytic. This is easy to handle, as well. We can add a MergeContent processor to our graph and configure it to merge data into 64-128 MB TAR files. Then, when we have a<br />
full TAR file, we can push the data to HDFS. We can configure MergeContent to make these bundles like so:</p>
<p><img class="dialog" src="https://blogs.apache.org/nifi/mediaresource/447a0d0a-0723-4102-8d96-bcc23a71f65d" alt="Merge configuration" /></p>
<p>
We can then send the merged files to PutHDFS and auto-terminate the originals. We can feed all the data from our Funnel to this MergeContent processor, and this will<br />
allow us to dual-route all data to both HDFS (bundled into 64-128 MB TAR files) and to Spark Streaming (making the data available as soon as possible with very low latency):</p>
<p><img class="screenshot" src="https://blogs.apache.org/nifi/mediaresource/52d068ce-a8cf-4441-83ed-0bb2ad9c6e80" alt="hdfs and spark streaming" /></p>
<p>
We would love to hear any questions, comments, or feedback that you may have!</p>
<p>
<a href="http://nifi.incubator.apache.org">Learn more about Apache NiFi</a> and feel free to leave comments here or e-mail us at dev@nifi.incubator.apache.org.</p>
