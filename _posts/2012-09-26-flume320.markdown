---
layout: post
title: Apache Flume - FileChannel
date: '2012-09-26T20:43:55+00:00'
categories: flume
---
<p>This blog post is about Apache Flumeâ€™s File Channel.  Apache Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data. It has a simple and flexible architecture based on streaming <a href="http://flume.apache.org/FlumeUserGuide.html#data-flow-model">data flows</a> It is robust and fault tolerant with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible data model that allows for online analytic application.</p> 
  <p>FileChannel is a persistent Flume channel that supports writing to multiple disks in parallel and encryption.</p> <strong>Overview</strong> 
  <p>When using Flume, each flow has a Source, Channel, and Sink. A typical example would be a webserver writing events to a Source via RPC (e.g. <a href="http://flume.apache.org/FlumeUserGuide.html#avro-sink">Avro Source</a>), the sources writing to <a href="http://flume.apache.org/FlumeUserGuide.html#memory-channel">MemoryChannel</a>, and <a href="http://flume.apache.org/FlumeUserGuide.html#hdfs-sink">HDFS Sink</a> consuming the events, writing them to HDFS.</p> <img src="https://blogs.apache.org/mrunit/mediaresource/f8e708c6-4cb1-489d-a7df-e75f1bee4f18" /> 
  <p>MemoryChannel provides high throughput but loses data in the event of a crash or loss of power. As such the development of a persistent Channel was desired. FileChannel was implemented in <a href="https://issues.apache.org/jira/browse/FLUME-1085">FLUME-1085</a>. The goal of FileChannel is to provide a reliable high throughput channel. FileChannel guarantees that when a transaction is committed, no data will be lost due to a subsequent crash or loss of power.</p> 
  <p>It's important to note that FileChannel does not do any replication of data itself. As such, it is only as reliable as the underlying disks. Users who use FileChannel because of its durability should take this into account when purchasing and configuring hardware. The underlying disks should be RAID, SAN, or similar.</p> 
  <p>Many systems trade a small amount of data loss (<a href="http://pubs.opengroup.org/onlinepubs/7908799/xsh/fsync.html">fsync</a>&nbsp;from memory to disk every few seconds for example) for higher throughput. The Flume team decided on a different approach with FileChannel. Flume is a transactional system and multiple events can be either Put or Taken in a single transaction. The batch size can be used to control throughput. Using large batch sizes, Flume can move data through a flow with no data loss and high throughput. The batch size is completely controlled by the client. This is an approach users of RDBMS's will be familiar with.</p> 
  <p>A Flume transaction consists of either Puts or Takes, but not both, and either a commit or a rollback. Each transaction implements both a Put and Take method. Sources do Puts onto the channel and Sinks do Takes from the channel.</p> <strong>Design</strong> 
  <p>FileChannel is based on a write ahead log or <a href="http://en.wikipedia.org/wiki/Write-ahead_logging">WAL</a> in addition to an in-memory queue.&nbsp;Each transaction is written to the WAL based on the transaction type (Take or Put) and the queue is modified accordingly. Each time a transaction is committed, fsync is called on the appropriate file to ensure the data is actually on disk and a pointer to that event is placed on a queue. The queue serves just like any other queue: it manages what is yet to be consumed by the sink. During a take, a pointer is removed from the queue. The event is then read directly from the WAL. Due to the large amount of RAM available today, it's very common for that read to occur from the operating system file cache.</p> 
  <p>After a crash, the WAL can be replayed to place the queue in the same state it was immediately&nbsp;<strong id="internal-source-marker_0.6989466443192214" style="font-family: Times; font-size: medium; font-weight: normal; "><span style="font-size: 13px; font-family: Verdana; background-color: transparent; vertical-align: baseline; white-space: pre-wrap; ">preceding</span></strong>&nbsp;the crash such that no committed transactions are lost. Replaying WALs can be time consuming, so the queue itself is written to disk periodically. Writing the queue to disk is called a checkpoint. After a crash, the queue is loaded from disk and then only committed transactions after the queue was saved to disk are replayed, significantly reducing the amount of WAL, which must be read.&nbsp;</p> 
  <p>For example, a channel that has two events will look like this: </p> <img src="https://blogs.apache.org/mrunit/mediaresource/ed650e01-ca9a-43af-b406-00352c995edc" /><br /> 
  <p>The WAL contains three important items: the transaction id, sequence number, and event data. Each transaction has a unique transaction id, and each event has a unique sequence number. The transaction id is used simply to group events into a transaction while the sequence number is used when replaying logs. In the above example, the transaction id is 1 and the sequence numbers are 1, 2, and 3.</p> 
  <p>When the queue is saved to disk - a checkpoint - the sequence number is incremented and saved as well. At restart, first the queue from disk is loaded and then any WAL entries with a greater sequence number than the queue, are replayed. During the checkpoint operation the channel is locked so that no Put or Take operations can alter it's state. Allowing modification of the queue during the checkpoint would result in an inconsistent snapshot of the queue stored on disk.</p> 
  <p>In the example queue above, a checkpoint occurs after the commit of transaction 1 resulting in the queue being saved to disk with both events (&quot;a&quot; and &quot;b&quot;) and a sequence number of 4.</p> 
  <p>After that point, event a is Taken from the queue in transaction 2:</p> <img src="https://blogs.apache.org/mrunit/mediaresource/b4de6164-fed2-44b9-ab97-cf3d5e35c0d7" /><br /> 
  <p>If a crash occurs, the queue checkpoint is read from disk. Note that since the checkpoint occurred before transaction 2, both events a and b currently exist on the queue. Then the WAL is read and any committed transaction with a sequence number greater than 4 is applied resulting in &quot;a&quot; being removed from the queue.</p> 
  <p>Two items are not covered by the design above. Takes and Puts which are in progress at the time the checkpoint occurs are lost. Assume the checkpoint occurred instead after the take of &quot;a&quot;:</p> <img src="https://blogs.apache.org/mrunit/mediaresource/289d2600-8821-434b-8d80-57865c60d286" /><br /> 
  <p>If a crash occurred at this point, under the design described above, event &quot;b&quot; would be on the queue and on replay any WAL entry with a sequence number greater than 5 would be replayed. The Rollback for transaction 2 would be replayed, but the Take for transaction 2 would not be replayed. As such, &quot;a&quot; would not be placed on the queue resulting in data loss. A similar scenario is played out for Puts. For this reason, when a queue checkpoint occurs, transactions which are still in progress are also written out so that this scenario can be handled appropriately.</p> <strong>Implementation</strong> 
  <p>FileChannel is stored in the flume-file-channel module of the Flume project and it's Java package name is org.apache.flume.channel.file. The queue described above is named <a href="https://github.com/apache/flume/blob/trunk/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/FlumeEventQueue.java">FlumeEventQueue</a> and the WAL is named <a href="https://github.com/apache/flume/blob/trunk/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/Log.java">Log</a>. The queue itself is a circular array and is backed by a <a href="https://github.com/apache/flume/blob/trunk/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/EventQueueBackingStoreFile.java">Memory Mapped File</a> while the WAL is a set of files written and read from using the <a href="https://github.com/apache/flume/blob/trunk/flume-ng-channels/flume-file-channel/src/main/java/org/apache/flume/channel/file/LogFile.java">LogFile</a> class and it's subclasses.</p> 
  <p>Conclusion</p> 
  <p>FileChannel provides Flume users with durability in the face of hardware, software, and environmental failures while perserving high throughput. It is the&nbsp;recommended&nbsp;channel for most topologies where both aspects are important.</p>
